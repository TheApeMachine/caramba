package qpool

import (
	"context"
	"fmt"
	"log"
	"sync"
	"time"
)

// Q is our hybrid worker pool/message queue
type Q struct {
	ctx        context.Context
	workers    chan chan Job
	jobs       chan Job
	space      *QuantumSpace
	scaler     *Scaler
	metrics    *Metrics
	breakers   map[string]*CircuitBreaker
	quit       chan struct{}
	workerMu   sync.Mutex
	workerList []*Worker
	breakersMu sync.RWMutex
	config     *Config
	cancel     context.CancelFunc
	wg         sync.WaitGroup
}

// Config struct
type Config struct {
	SchedulingTimeout time.Duration
}

// NewQ creates a new quantum pool
func NewQ(ctx context.Context, minWorkers, maxWorkers int, config *Config) *Q {
	ctx, cancel := context.WithCancel(ctx)
	q := &Q{
		ctx:        ctx,
		cancel:     cancel,
		breakers:   make(map[string]*CircuitBreaker),
		workerList: make([]*Worker, 0),
		quit:       make(chan struct{}),
		jobs:       make(chan Job, maxWorkers*10),
		workers:    make(chan chan Job, maxWorkers),
		space:      newQuantumSpace(),
		metrics:    newMetrics(),
		config:     config,
	}

	// Start initial workers
	for i := 0; i < minWorkers; i++ {
		q.startWorker()
	}

	// Start the manager goroutine
	q.wg.Add(1)
	go func() {
		defer q.wg.Done()
		q.manage()
	}()

	// Start metrics collection
	q.wg.Add(1)
	go func() {
		defer q.wg.Done()
		q.collectMetrics()
	}()

	// Start scaler with appropriate configuration
	scalerConfig := &ScalerConfig{
		TargetLoad:         2.0,                    // Reasonable target load
		ScaleUpThreshold:   4.0,                    // Scale up when load is high
		ScaleDownThreshold: 1.0,                    // Scale down when load is low
		Cooldown:           time.Millisecond * 500, // Reasonable cooldown
	}
	q.scaler = NewScaler(q, minWorkers, maxWorkers, scalerConfig)

	return q
}

// Pool management
func (q *Q) manage() {
	for {
		select {
		case <-q.ctx.Done():
			return
		case job := <-q.jobs:
			// Wait for a worker with timeout
			select {
			case <-q.ctx.Done():
				return
			case workerChan := <-q.workers:
				// Send job to worker
				select {
				case workerChan <- job:
					// Job successfully sent to worker
				case <-q.ctx.Done():
					return
				}
			case <-time.After(q.getSchedulingTimeout()):
				log.Printf("No available workers for job: %s, timeout occurred", job.ID)
				// Store error result since we couldn't process the job
				q.space.Store(job.ID, nil, fmt.Errorf("no available workers"), job.TTL)
			}
		}
	}
}

func (q *Q) collectMetrics() {
	ticker := time.NewTicker(500 * time.Millisecond)
	defer ticker.Stop()

	for {
		select {
		case <-q.ctx.Done():
			return
		case <-ticker.C:
			q.metrics.mu.Lock()
			q.metrics.JobQueueSize = len(q.jobs)
			q.metrics.ActiveWorkers = len(q.workers)
			q.metrics.mu.Unlock()
		}
	}
}

// Public API methods
func (q *Q) Schedule(id string, fn func() (any, error), opts ...JobOption) chan QuantumValue {
	// Create context with configured timeout
	ctx, cancel := context.WithTimeout(q.ctx, q.getSchedulingTimeout())
	defer cancel()

	startTime := time.Now()

	job := Job{
		ID: id,
		Fn: fn,
		RetryPolicy: &RetryPolicy{
			MaxAttempts: 3,
			Strategy:    &ExponentialBackoff{Initial: time.Second},
		},
		StartTime: startTime,
	}

	// Apply options
	for _, opt := range opts {
		opt(&job)
	}

	// Check circuit breaker if configured
	if job.CircuitID != "" {
		breaker := q.getCircuitBreaker(job)
		if breaker != nil && !breaker.Allow() {
			ch := make(chan QuantumValue, 1)
			ch <- QuantumValue{
				Error:     fmt.Errorf("circuit breaker %s is open", job.CircuitID),
				CreatedAt: time.Now(),
			}
			close(ch)
			return ch
		}
	}

	// Try to schedule job with context timeout
	select {
	case q.jobs <- job:
		return q.space.Await(id)
	case <-ctx.Done():
		ch := make(chan QuantumValue, 1)
		ch <- QuantumValue{
			Error:     fmt.Errorf("job scheduling timeout: %w", ctx.Err()),
			CreatedAt: time.Now(),
		}
		close(ch)

		// Update metrics for scheduling failure
		q.metrics.mu.Lock()
		q.metrics.SchedulingFailures++
		q.metrics.mu.Unlock()

		return ch
	}
}

func (q *Q) CreateBroadcastGroup(id string, ttl time.Duration) *BroadcastGroup {
	return q.space.CreateBroadcastGroup(id, ttl)
}

func (q *Q) Subscribe(groupID string) chan QuantumValue {
	return q.space.Subscribe(groupID)
}

// Helper functions
func (q *Q) startWorker() {
	worker := &Worker{
		pool:   q,
		jobs:   make(chan Job),
		cancel: nil,
	}
	q.workerMu.Lock()
	q.workerList = append(q.workerList, worker)
	q.workerMu.Unlock()

	q.metrics.mu.Lock()
	q.metrics.WorkerCount++
	q.metrics.mu.Unlock()

	q.wg.Add(1)
	go func() {
		defer q.wg.Done()
		worker.run()
	}()
	log.Printf("Started worker, total workers: %d", q.metrics.WorkerCount)
}

// WithTTL configures TTL for a job
func WithTTL(ttl time.Duration) JobOption {
	return func(j *Job) {
		j.TTL = ttl
	}
}

// Example demonstrates usage of the quantum pool
func Example() {
	ctx := context.Background()
	q := NewQ(ctx, 5, 20, nil)

	// Create a broadcast group
	group := q.CreateBroadcastGroup("sensors", time.Minute)
	subscriber1 := q.Subscribe("sensors")
	subscriber2 := q.Subscribe("sensors")

	// Use the subscribers to avoid unused variable warnings
	go func() {
		for value := range subscriber1 {
			// Handle sensor data from subscriber 1
			fmt.Printf("Subscriber 1 received: %v\n", value.Value)
		}
	}()

	go func() {
		for value := range subscriber2 {
			// Handle sensor data from subscriber 2
			fmt.Printf("Subscriber 2 received: %v\n", value.Value)
		}
	}()

	// Simulate broadcasting data
	go func() {
		for {
			group.Send(QuantumValue{Value: "sensor data", CreatedAt: time.Now()})
			time.Sleep(time.Second)
		}
	}()

	// Schedule a job with retry and circuit breaker
	result := q.Schedule("sensor-read", func() (any, error) {
		// Simulate sensor reading
		return "sensor reading", nil
	},
		WithRetry(3, &ExponentialBackoff{Initial: time.Second}),
		WithCircuitBreaker("sensors", 5, time.Minute),
		WithTTL(time.Minute),
	)

	// Process results
	go func() {
		for value := range result {
			if value.Error != nil {
				fmt.Printf("Error: %v\n", value.Error)
				continue
			}
			// Process the sensor data
			fmt.Printf("Processed data: %v\n", value.Value)
		}
	}()

	// Close subscribers after some time (for demonstration purposes)
	time.AfterFunc(10*time.Second, func() {
		close(subscriber1)
		close(subscriber2)
	})
}

// Helper functions
func (q *Q) getCircuitBreaker(job Job) *CircuitBreaker {
	if job.CircuitID == "" || job.CircuitConfig == nil {
		return nil
	}

	q.breakersMu.Lock()
	defer q.breakersMu.Unlock()

	breaker, exists := q.breakers[job.CircuitID]
	if !exists {
		breaker = &CircuitBreaker{
			maxFailures:  job.CircuitConfig.MaxFailures,
			resetTimeout: job.CircuitConfig.ResetTimeout,
			halfOpenMax:  job.CircuitConfig.HalfOpenMax,
			state:        CircuitClosed,
		}
		q.breakers[job.CircuitID] = breaker
	}

	return breaker
}

// Add method to get scheduling timeout from config or use default
func (q *Q) getSchedulingTimeout() time.Duration {
	if q.config != nil && q.config.SchedulingTimeout > 0 {
		return q.config.SchedulingTimeout
	}
	return 5 * time.Second // Default timeout
}

// Update Close method to handle channel closing safely
func (q *Q) Close() {
	if q == nil {
		return
	}

	log.Println("Closing Quantum Pool")

	// Cancel context first to stop all operations
	if q.cancel != nil {
		log.Println("Cancelling context")
		q.cancel()
	}

	// Wait for all goroutines to finish before closing channels
	q.wg.Wait()

	// Now it's safe to close channels as no goroutines are using them
	q.workerMu.Lock()
	for _, worker := range q.workerList {
		close(worker.jobs)
	}
	q.workerList = nil
	q.workerMu.Unlock()

	close(q.quit)
	close(q.jobs)
	close(q.workers)

	log.Println("Quantum Pool closed")
}


package qpool

import (
	"context"
	"fmt"
	"log"
	"time"
)

// Worker processes jobs
type Worker struct {
	pool       *Q
	jobs       chan Job
	cancel     context.CancelFunc
	currentJob *Job // Added Field to Track Current Job
}

// run starts the worker's job processing loop
func (w *Worker) run() {
	jobChan := w.jobs // Store the job channel locally for clarity

	for {
		// First check if we should exit
		select {
		case <-w.pool.ctx.Done():
			log.Printf("Worker exiting due to context cancellation")
			return
		default:
		}

		// Register ourselves as available
		log.Printf("Worker registering as available")
		w.pool.workers <- jobChan

		// Wait for a job
		select {
		case <-w.pool.ctx.Done():
			log.Printf("Worker exiting while waiting for job")
			return
		case job, ok := <-jobChan:
			if !ok {
				log.Printf("Worker job channel closed")
				return
			}

			log.Printf("Worker received job: %s", job.ID)
			w.currentJob = &job
			result, err := w.processJobWithTimeout(w.pool.ctx, job)
			w.currentJob = nil
			log.Printf("Worker completed job: %s, err: %v", job.ID, err)

			// Handle result
			if err != nil {
				w.pool.metrics.RecordJobFailure()
				log.Printf("Job %s failed: %v", job.ID, err)
			} else {
				w.pool.metrics.RecordJobSuccess(time.Since(job.StartTime))
				log.Printf("Job %s succeeded", job.ID)
			}

			// Always store the result, even if there was an error
			w.pool.space.Store(job.ID, result, err, job.TTL)
			log.Printf("Stored result for job: %s", job.ID)

			// Notify dependents
			if len(job.Dependencies) > 0 {
				for _, depID := range job.Dependencies {
					if children := w.pool.space.GetChildren(depID); len(children) > 0 {
						for _, childID := range children {
							log.Printf("Notifying dependent job %s", childID)
						}
					}
				}
			}
		}
	}
}

// processJobWithTimeout processes a job with a timeout
func (w *Worker) processJobWithTimeout(ctx context.Context, job Job) (any, error) {
	startTime := time.Now()
	done := make(chan struct{})
	var result any
	var err error

	go func() {
		defer close(done)
		result, err = job.Fn()
		// Store result immediately after job completion
		w.pool.space.Store(job.ID, result, err, job.TTL)
		log.Printf("Job %s completed and result stored", job.ID)
	}()

	select {
	case <-ctx.Done():
		w.handleJobTimeout(job)
		return nil, fmt.Errorf("job %s timed out", job.ID)
	case <-done:
		// Check dependencies after job execution
		for _, depID := range job.Dependencies {
			if err := w.checkSingleDependency(depID, job.DependencyRetryPolicy); err != nil {
				w.pool.metrics.recordJobExecution(startTime, false)
				if job.CircuitID != "" {
					w.recordFailure(job.CircuitID)
				}
				return nil, err
			}
		}
		w.pool.metrics.recordJobExecution(startTime, err == nil)
		return result, err
	}
}

// checkSingleDependency checks a single job dependency with retries
func (w *Worker) checkSingleDependency(depID string, retryPolicy *RetryPolicy) error {
	maxAttempts := 1
	var strategy RetryStrategy = &ExponentialBackoff{Initial: time.Second}

	if retryPolicy != nil {
		maxAttempts = retryPolicy.MaxAttempts
		strategy = retryPolicy.Strategy
	}

	circuitID := ""
	if w.currentJob != nil {
		circuitID = w.currentJob.CircuitID
	}

	for attempt := 0; attempt < maxAttempts; attempt++ {
		ch := w.pool.space.Await(depID)
		if result := <-ch; result.Error == nil {
			return nil
		} else if attempt < maxAttempts-1 {
			time.Sleep(strategy.NextDelay(attempt + 1))
			continue
		}
	}

	w.pool.breakersMu.RLock()
	breaker, exists := w.pool.breakers[circuitID]
	w.pool.breakersMu.RUnlock()

	if exists {
		breaker.RecordFailure()
	}

	w.pool.space.mu.Lock()
	if w.pool.space.children == nil {
		w.pool.space.children = make(map[string][]string)
	}
	if w.currentJob != nil {
		w.pool.space.children[depID] = append(w.pool.space.children[depID], w.currentJob.ID)
	}
	w.pool.space.mu.Unlock()

	return fmt.Errorf("dependency %s failed after %d attempts", depID, maxAttempts)
}

// recordFailure records a failure for a specific circuit breaker
func (w *Worker) recordFailure(circuitID string) {
	if circuitID == "" {
		return
	}

	w.pool.breakersMu.RLock()
	breaker, exists := w.pool.breakers[circuitID]
	w.pool.breakersMu.RUnlock()

	if exists {
		breaker.RecordFailure()
	}
}

// Add this method to the Worker struct
func (w *Worker) handleJobTimeout(job Job) {
	w.pool.metrics.RecordJobFailure()
	err := fmt.Errorf("job %s timed out", job.ID)
	w.pool.space.Store(job.ID, nil, err, job.TTL)
	log.Printf("Job %s timed out", job.ID)
}


package qpool

import (
	"log"
	"sync"
	"time"
)

// BroadcastGroup handles pub/sub
type BroadcastGroup struct {
	ID       string
	channels []chan QuantumValue
	TTL      time.Duration
	LastUsed time.Time
}

// QuantumValue wraps a value with metadata
type QuantumValue struct {
	Value     any
	Error     error
	CreatedAt time.Time
	TTL       time.Duration
}

// QuantumSpace handles value storage and messaging
type QuantumSpace struct {
	mu       sync.RWMutex
	values   map[string]QuantumValue
	waiting  map[string][]chan QuantumValue
	groups   map[string]*BroadcastGroup
	errors   map[string]error
	children map[string][]string
	wg       sync.WaitGroup
}

func newQuantumSpace() *QuantumSpace {
	qs := &QuantumSpace{
		values:  make(map[string]QuantumValue),
		waiting: make(map[string][]chan QuantumValue),
		groups:  make(map[string]*BroadcastGroup),
		mu:      sync.RWMutex{},
	}

	// Start cleanup goroutine
	qs.wg.Add(1)
	go func() {
		defer qs.wg.Done()
		qs.cleanup()
	}()

	return qs
}

// Store stores a value with its metadata
func (qs *QuantumSpace) Store(id string, value any, err error, ttl time.Duration) {
	qs.mu.Lock()
	defer qs.mu.Unlock()

	qv := QuantumValue{
		Value:     value,
		Error:     err,
		CreatedAt: time.Now(),
		TTL:       ttl,
	}
	qs.values[id] = qv
	log.Printf("Storing value for job %s: value=%v, err=%v", id, value, err)

	// Notify any waiting channels
	if channels, ok := qs.waiting[id]; ok {
		log.Printf("Found %d waiting channels for job %s", len(channels), id)
		for i, ch := range channels {
			select {
			case ch <- qv:
				log.Printf("Successfully sent result to channel %d for job %s", i, id)
				close(ch)
			default:
				log.Printf("Failed to send result to channel %d for job %s (channel full or closed)", i, id)
			}
		}
		delete(qs.waiting, id)
		log.Printf("Cleared waiting channels for job %s", id)
	} else {
		log.Printf("No waiting channels found for job %s", id)
	}
}

// Await returns a channel that will receive the value when it's available
func (qs *QuantumSpace) Await(id string) chan QuantumValue {
	qs.mu.Lock()
	defer qs.mu.Unlock()

	ch := make(chan QuantumValue, 1)
	log.Printf("Created await channel for job %s", id)

	// Check if value already exists
	if qv, ok := qs.values[id]; ok {
		log.Printf("Value already exists for job %s, sending immediately", id)
		ch <- qv
		close(ch)
		return ch
	}

	// Add to waiting list
	if qs.waiting == nil {
		qs.waiting = make(map[string][]chan QuantumValue)
	}
	qs.waiting[id] = append(qs.waiting[id], ch)
	log.Printf("Added channel to waiting list for job %s (total waiting: %d)",
		id, len(qs.waiting[id]))

	return ch
}

func (qs *QuantumSpace) cleanup() {
	ticker := time.NewTicker(time.Minute)
	defer ticker.Stop()

	for range ticker.C {
		qs.mu.Lock()
		qs.cleanupExpiredValues()
		qs.cleanupExpiredGroups()
		qs.mu.Unlock()
	}
}

func (qs *QuantumSpace) cleanupExpiredValues() {
	now := time.Now()
	for id, qv := range qs.values {
		if qv.TTL > 0 && now.Sub(qv.CreatedAt) > qv.TTL {
			delete(qs.values, id)
		}
	}
}

func (qs *QuantumSpace) cleanupExpiredGroups() {
	now := time.Now()
	for id, group := range qs.groups {
		if group.TTL > 0 && now.Sub(group.LastUsed) > group.TTL {
			for _, ch := range group.channels {
				close(ch)
			}
			delete(qs.groups, id)
		}
	}
}

func (qs *QuantumSpace) CreateBroadcastGroup(id string, ttl time.Duration) *BroadcastGroup {
	qs.mu.Lock()
	defer qs.mu.Unlock()

	group := &BroadcastGroup{
		ID:       id,
		channels: make([]chan QuantumValue, 0),
		TTL:      ttl,
		LastUsed: time.Now(),
	}
	qs.groups[id] = group
	return group
}

func (bg *BroadcastGroup) Send(qv QuantumValue) {
	bg.LastUsed = time.Now()
	for _, ch := range bg.channels {
		ch <- qv
	}
}

func (qs *QuantumSpace) Subscribe(groupID string) chan QuantumValue {
	qs.mu.Lock()
	defer qs.mu.Unlock()

	ch := make(chan QuantumValue, 10)
	if group, ok := qs.groups[groupID]; ok {
		group.channels = append(group.channels, ch)
	}
	return ch
}

func (qs *QuantumSpace) Close() {
	qs.CleanUp()
	// Add any additional cleanup if necessary
}

func (qs *QuantumSpace) CleanUp() {
	qs.mu.Lock()
	defer qs.mu.Unlock()
	now := time.Now()
	for id, qv := range qs.values {
		if now.Sub(qv.CreatedAt) > qv.TTL {
			delete(qs.values, id)
		}
	}
}

func (qs *QuantumSpace) StoreError(id string, err error) {
	qs.mu.Lock()
	defer qs.mu.Unlock()
	if qs.errors == nil {
		qs.errors = make(map[string]error)
	}
	qs.errors[id] = err
}

func (qs *QuantumSpace) AddChild(parentID, childID string) {
	qs.mu.Lock()
	defer qs.mu.Unlock()
	if qs.children == nil {
		qs.children = make(map[string][]string)
	}
	qs.children[parentID] = append(qs.children[parentID], childID)
}

func (qs *QuantumSpace) GetChildren(parentID string) []string {
	qs.mu.RLock()
	defer qs.mu.RUnlock()
	return qs.children[parentID]
}


package qpool

import (
	"log"
	"math"
	"time"
)

// Scaler manages pool size based on current load
type Scaler struct {
	pool               *Q
	minWorkers         int
	maxWorkers         int
	targetLoad         float64
	scaleUpThreshold   float64
	scaleDownThreshold float64
	cooldown           time.Duration
	lastScale          time.Time
}

// ScalerConfig defines configuration for the Scaler
type ScalerConfig struct {
	TargetLoad         float64
	ScaleUpThreshold   float64
	ScaleDownThreshold float64
	Cooldown           time.Duration
}

// evaluate assesses the current load and scales the worker pool accordingly
func (s *Scaler) evaluate() {
	s.pool.metrics.mu.Lock()
	defer s.pool.metrics.mu.Unlock()

	if time.Since(s.lastScale) < s.cooldown {
		return
	}

	// Ensure at least one worker for load calculation
	if s.pool.metrics.WorkerCount == 0 {
		s.pool.metrics.WorkerCount = 1
	}

	currentLoad := float64(s.pool.metrics.JobQueueSize) / float64(s.pool.metrics.WorkerCount)
	log.Printf("Current load: %.2f, Workers: %d, Queue: %d",
		currentLoad, s.pool.metrics.WorkerCount, s.pool.metrics.JobQueueSize)

	switch {
	case currentLoad > s.scaleUpThreshold && s.pool.metrics.WorkerCount < s.maxWorkers:
		needed := int(math.Ceil(float64(s.pool.metrics.JobQueueSize) / s.targetLoad))
		toAdd := Min(s.maxWorkers-s.pool.metrics.WorkerCount, needed)
		if toAdd > 0 {
			s.scaleUp(toAdd)
			s.lastScale = time.Now()
		}

	case currentLoad < s.scaleDownThreshold && s.pool.metrics.WorkerCount > s.minWorkers:
		needed := Max(int(math.Ceil(float64(s.pool.metrics.JobQueueSize)/s.targetLoad)), s.minWorkers)
		toRemove := Min(s.pool.metrics.WorkerCount-s.minWorkers, Max(1, (s.pool.metrics.WorkerCount-needed)/2))
		if toRemove > 0 {
			s.scaleDown(toRemove)
			s.lastScale = time.Now()
		}
	}
}

// scaleUp adds 'count' number of workers to the pool
func (s *Scaler) scaleUp(count int) {
	toAdd := Min(s.maxWorkers-s.pool.metrics.WorkerCount, Max(1, count))

	for i := 0; i < toAdd; i++ {
		s.pool.startWorker()
	}
	log.Printf("Scaled up by %d workers, total workers: %d", toAdd, s.pool.metrics.WorkerCount)
}

// scaleDown removes 'count' number of workers from the pool
func (s *Scaler) scaleDown(count int) {
	for i := 0; i < count; i++ {
		if len(s.pool.workerList) == 0 {
			break
		}

		// Remove the last worker from the list
		w := s.pool.workerList[len(s.pool.workerList)-1]
		s.pool.workerList = s.pool.workerList[:len(s.pool.workerList)-1]

		// Cancel the worker's context
		if w.cancel != nil {
			w.cancel()
		}

		s.pool.metrics.WorkerCount--

		log.Printf("Scaled down worker, total workers: %d", s.pool.metrics.WorkerCount)

		// Add a small delay between worker removals
		time.Sleep(time.Millisecond * 50)
	}
}

// run starts the scaler's evaluation loop
func (s *Scaler) run() {
	ticker := time.NewTicker(time.Second * 1)
	defer ticker.Stop()

	for {
		select {
		case <-s.pool.ctx.Done():
			return
		case <-ticker.C:
			s.evaluate()
		}
	}
}

// NewScaler initializes and starts a new Scaler
func NewScaler(q *Q, minWorkers, maxWorkers int, config *ScalerConfig) *Scaler {
	scaler := &Scaler{
		pool:               q,
		minWorkers:         minWorkers,
		maxWorkers:         maxWorkers,
		targetLoad:         config.TargetLoad,
		scaleUpThreshold:   config.ScaleUpThreshold,
		scaleDownThreshold: config.ScaleDownThreshold,
		cooldown:           config.Cooldown,
		lastScale:          time.Now(),
	}

	q.wg.Add(1)
	go func() {
		defer q.wg.Done()
		scaler.run()
	}()

	return scaler
}


package qpool

import (
	"log"
	"math"
	"time"
)

// Scaler manages pool size based on current load
type Scaler struct {
	pool               *Q
	minWorkers         int
	maxWorkers         int
	targetLoad         float64
	scaleUpThreshold   float64
	scaleDownThreshold float64
	cooldown           time.Duration
	lastScale          time.Time
}

// ScalerConfig defines configuration for the Scaler
type ScalerConfig struct {
	TargetLoad         float64
	ScaleUpThreshold   float64
	ScaleDownThreshold float64
	Cooldown           time.Duration
}

// evaluate assesses the current load and scales the worker pool accordingly
func (s *Scaler) evaluate() {
	s.pool.metrics.mu.Lock()
	defer s.pool.metrics.mu.Unlock()

	if time.Since(s.lastScale) < s.cooldown {
		return
	}

	// Ensure at least one worker for load calculation
	if s.pool.metrics.WorkerCount == 0 {
		s.pool.metrics.WorkerCount = 1
	}

	currentLoad := float64(s.pool.metrics.JobQueueSize) / float64(s.pool.metrics.WorkerCount)
	log.Printf("Current load: %.2f, Workers: %d, Queue: %d",
		currentLoad, s.pool.metrics.WorkerCount, s.pool.metrics.JobQueueSize)

	switch {
	case currentLoad > s.scaleUpThreshold && s.pool.metrics.WorkerCount < s.maxWorkers:
		needed := int(math.Ceil(float64(s.pool.metrics.JobQueueSize) / s.targetLoad))
		toAdd := Min(s.maxWorkers-s.pool.metrics.WorkerCount, needed)
		if toAdd > 0 {
			s.scaleUp(toAdd)
			s.lastScale = time.Now()
		}

	case currentLoad < s.scaleDownThreshold && s.pool.metrics.WorkerCount > s.minWorkers:
		needed := Max(int(math.Ceil(float64(s.pool.metrics.JobQueueSize)/s.targetLoad)), s.minWorkers)
		toRemove := Min(s.pool.metrics.WorkerCount-s.minWorkers, Max(1, (s.pool.metrics.WorkerCount-needed)/2))
		if toRemove > 0 {
			s.scaleDown(toRemove)
			s.lastScale = time.Now()
		}
	}
}

// scaleUp adds 'count' number of workers to the pool
func (s *Scaler) scaleUp(count int) {
	toAdd := Min(s.maxWorkers-s.pool.metrics.WorkerCount, Max(1, count))

	for i := 0; i < toAdd; i++ {
		s.pool.startWorker()
	}
	log.Printf("Scaled up by %d workers, total workers: %d", toAdd, s.pool.metrics.WorkerCount)
}

// scaleDown removes 'count' number of workers from the pool
func (s *Scaler) scaleDown(count int) {
	for i := 0; i < count; i++ {
		if len(s.pool.workerList) == 0 {
			break
		}

		// Remove the last worker from the list
		w := s.pool.workerList[len(s.pool.workerList)-1]
		s.pool.workerList = s.pool.workerList[:len(s.pool.workerList)-1]

		// Cancel the worker's context
		if w.cancel != nil {
			w.cancel()
		}

		s.pool.metrics.WorkerCount--

		log.Printf("Scaled down worker, total workers: %d", s.pool.metrics.WorkerCount)

		// Add a small delay between worker removals
		time.Sleep(time.Millisecond * 50)
	}
}

// run starts the scaler's evaluation loop
func (s *Scaler) run() {
	ticker := time.NewTicker(time.Second * 1)
	defer ticker.Stop()

	for {
		select {
		case <-s.pool.ctx.Done():
			return
		case <-ticker.C:
			s.evaluate()
		}
	}
}

// NewScaler initializes and starts a new Scaler
func NewScaler(q *Q, minWorkers, maxWorkers int, config *ScalerConfig) *Scaler {
	scaler := &Scaler{
		pool:               q,
		minWorkers:         minWorkers,
		maxWorkers:         maxWorkers,
		targetLoad:         config.TargetLoad,
		scaleUpThreshold:   config.ScaleUpThreshold,
		scaleDownThreshold: config.ScaleDownThreshold,
		cooldown:           config.Cooldown,
		lastScale:          time.Now(),
	}

	q.wg.Add(1)
	go func() {
		defer q.wg.Done()
		scaler.run()
	}()

	return scaler
}


package qpool

import (
	"math"
	"sort"
	"sync"
	"time"
)

// tDigestCentroid represents a centroid in the t-digest
type tDigestCentroid struct {
	mean  float64
	count int64
}

type Metrics struct {
	mu                   sync.RWMutex
	WorkerCount          int
	JobQueueSize         int
	ActiveWorkers        int
	LastScale            time.Time
	ErrorRates           map[string]float64
	TotalJobTime         time.Duration
	JobCount             int64
	CircuitBreakerStates map[string]CircuitState

	// Additional suggested metrics
	AverageJobLatency   time.Duration
	P95JobLatency       time.Duration
	P99JobLatency       time.Duration
	JobSuccessRate      float64
	QueueWaitTime       time.Duration
	ResourceUtilization float64

	// Rate limiting metrics
	RateLimitHits int64
	ThrottledJobs int64

	// t-digest fields for percentile calculation
	centroids    []tDigestCentroid
	compression  float64
	totalWeight  int64
	maxCentroids int

	// SchedulingFailures field to track scheduling timeouts
	SchedulingFailures int64

	// Additional metrics
	FailureCount int64
}

func newMetrics() *Metrics {
	return &Metrics{
		ErrorRates:           make(map[string]float64),
		CircuitBreakerStates: make(map[string]CircuitState),
		SchedulingFailures:   0,
		compression:          100,
		maxCentroids:         100,
		centroids:            make([]tDigestCentroid, 0, 100),
		totalWeight:          0,
		JobSuccessRate:       1.0,
	}
}

// Add prometheus-style metrics collection
func (m *Metrics) recordJobExecution(startTime time.Time, success bool) {
	m.mu.RLock()
	oldTime := m.TotalJobTime
	m.mu.RUnlock()

	duration := time.Since(startTime)

	m.mu.Lock()
	m.TotalJobTime = oldTime + duration
	m.JobCount++
	if success {
		m.JobSuccessRate = float64(m.JobCount-m.FailureCount) / float64(m.JobCount)
	}
	m.mu.Unlock()

	// Update latency percentiles in a separate lock to reduce contention
	m.updateLatencyPercentiles(duration)
}

// Add updateLatencyPercentiles method
func (m *Metrics) updateLatencyPercentiles(duration time.Duration) {
	m.mu.Lock()
	defer m.mu.Unlock()

	// Update average using existing calculation
	m.AverageJobLatency = (m.AverageJobLatency*time.Duration(m.JobCount-1) + duration) / time.Duration(m.JobCount)

	// Convert duration to float64 milliseconds for t-digest
	value := float64(duration.Milliseconds())

	// Find the closest centroid or create a new one
	inserted := false
	m.totalWeight++

	if len(m.centroids) == 0 {
		m.centroids = append(m.centroids, tDigestCentroid{mean: value, count: 1})
		return
	}

	// Find insertion point
	idx := sort.Search(len(m.centroids), func(i int) bool {
		return m.centroids[i].mean >= value
	})

	// Calculate maximum weight for this point
	q := m.calculateQuantile(value)
	maxWeight := int64(4 * m.compression * math.Min(q, 1-q))

	// Try to merge with existing centroid
	if idx < len(m.centroids) && m.centroids[idx].count < maxWeight {
		c := &m.centroids[idx]
		c.mean = (c.mean*float64(c.count) + value) / float64(c.count+1)
		c.count++
		inserted = true
	} else if idx > 0 && m.centroids[idx-1].count < maxWeight {
		c := &m.centroids[idx-1]
		c.mean = (c.mean*float64(c.count) + value) / float64(c.count+1)
		c.count++
		inserted = true
	}

	// If we couldn't merge, insert new centroid
	if !inserted {
		newCentroid := tDigestCentroid{mean: value, count: 1}
		m.centroids = append(m.centroids, tDigestCentroid{})
		copy(m.centroids[idx+1:], m.centroids[idx:])
		m.centroids[idx] = newCentroid
	}

	// Compress if we have too many centroids
	if len(m.centroids) > m.maxCentroids {
		m.compress()
	}

	// Update P95 and P99
	m.P95JobLatency = time.Duration(m.estimatePercentile(0.95)) * time.Millisecond
	m.P99JobLatency = time.Duration(m.estimatePercentile(0.99)) * time.Millisecond
}

func (m *Metrics) calculateQuantile(value float64) float64 {
	rank := 0.0
	for _, c := range m.centroids {
		if c.mean < value {
			rank += float64(c.count)
		}
	}
	return rank / float64(m.totalWeight)
}

func (m *Metrics) estimatePercentile(p float64) float64 {
	if len(m.centroids) == 0 {
		return 0
	}

	targetRank := p * float64(m.totalWeight)
	cumulative := 0.0

	for i, c := range m.centroids {
		cumulative += float64(c.count)
		if cumulative >= targetRank {
			// Linear interpolation between centroids
			if i > 0 {
				prev := m.centroids[i-1]
				prevCumulative := cumulative - float64(c.count)
				t := (targetRank - prevCumulative) / float64(c.count)
				return prev.mean + t*(c.mean-prev.mean)
			}
			return c.mean
		}
	}
	return m.centroids[len(m.centroids)-1].mean
}

func (m *Metrics) compress() {
	if len(m.centroids) <= 1 {
		return
	}

	// Sort centroids by mean if needed
	sort.Slice(m.centroids, func(i, j int) bool {
		return m.centroids[i].mean < m.centroids[j].mean
	})

	// Merge adjacent centroids while respecting size constraints
	newCentroids := make([]tDigestCentroid, 0, m.maxCentroids)
	current := m.centroids[0]

	for i := 1; i < len(m.centroids); i++ {
		if current.count+m.centroids[i].count <= int64(m.compression) {
			// Merge centroids
			totalCount := current.count + m.centroids[i].count
			current.mean = (current.mean*float64(current.count) +
				m.centroids[i].mean*float64(m.centroids[i].count)) /
				float64(totalCount)
			current.count = totalCount
		} else {
			newCentroids = append(newCentroids, current)
			current = m.centroids[i]
		}
	}
	newCentroids = append(newCentroids, current)
	m.centroids = newCentroids
}

// Add metrics export functionality
func (m *Metrics) ExportMetrics() map[string]interface{} {
	m.mu.RLock()
	defer m.mu.RUnlock()

	return map[string]interface{}{
		"worker_count":         m.WorkerCount,
		"queue_size":           m.JobQueueSize,
		"success_rate":         m.JobSuccessRate,
		"avg_latency":          m.AverageJobLatency.Milliseconds(),
		"p95_latency":          m.P95JobLatency.Milliseconds(),
		"p99_latency":          m.P99JobLatency.Milliseconds(),
		"resource_utilization": m.ResourceUtilization,
	}
}

func (m *Metrics) RecordJobSuccess(latency time.Duration) {
	m.mu.Lock()
	defer m.mu.Unlock()
	m.JobCount++
	m.TotalJobTime += latency
	m.AverageJobLatency = time.Duration(int64(m.TotalJobTime) / m.JobCount)
	// Update t-digest for percentiles
	m.updateLatencyMetrics(latency)
	m.JobSuccessRate = float64(m.JobCount-m.FailureCount) / float64(m.JobCount)
}

// RecordJobFailure records the failure of a job and updates metrics
func (m *Metrics) RecordJobFailure() {
	m.mu.Lock()
	defer m.mu.Unlock()
	m.FailureCount++
	m.JobSuccessRate = float64(m.JobCount-m.FailureCount) / float64(m.JobCount)
}

// updateLatencyMetrics updates latency percentiles
func (m *Metrics) updateLatencyMetrics(latency time.Duration) {
	// Simple implementation: update P95 and P99 if current latency exceeds them
	if latency > m.P99JobLatency {
		m.P99JobLatency = latency
	} else if latency > m.P95JobLatency {
		m.P95JobLatency = latency
	}
}

package qpool

import "time"

// Job represents work to be done
type Job struct {
	ID                    string
	Fn                    func() (any, error)
	RetryPolicy           *RetryPolicy
	CircuitID             string
	CircuitConfig         *CircuitBreakerConfig
	Dependencies          []string
	TTL                   time.Duration
	Attempt               int
	LastError             error
	DependencyRetryPolicy *RetryPolicy
	StartTime             time.Time
}

// JobOption is a function type for configuring jobs
type JobOption func(*Job)

// CircuitBreakerConfig defines configuration for a circuit breaker
type CircuitBreakerConfig struct {
	MaxFailures  int
	ResetTimeout time.Duration
	HalfOpenMax  int
}

// WithDependencyRetry configures retry behavior for dependencies
func WithDependencyRetry(attempts int, strategy RetryStrategy) JobOption {
	return func(j *Job) {
		j.DependencyRetryPolicy = &RetryPolicy{
			MaxAttempts: attempts,
			Strategy:    strategy,
		}
	}
}



package qpool

import (
	"log"
	"sync"
	"time"
)

// CircuitState represents the state of the circuit breaker
type CircuitState int

const (
	CircuitClosed CircuitState = iota
	CircuitOpen
	CircuitHalfOpen
)

// CircuitBreaker implements the circuit breaker pattern
type CircuitBreaker struct {
	mu               sync.RWMutex
	maxFailures      int
	resetTimeout     time.Duration
	halfOpenMax      int
	failureCount     int
	state            CircuitState
	openTime         time.Time
	halfOpenAttempts int
}

// RecordFailure records a failure and updates the circuit state
func (cb *CircuitBreaker) RecordFailure() {
	cb.mu.Lock()
	defer cb.mu.Unlock()

	cb.failureCount++
	if cb.failureCount >= cb.maxFailures {
		if cb.state == CircuitHalfOpen {
			// If we fail in half-open state, go back to open
			cb.state = CircuitOpen
			cb.openTime = time.Now()
			log.Printf("Circuit breaker reopened from half-open state")
		} else if cb.state == CircuitClosed {
			// Only open the circuit if we were closed
			cb.state = CircuitOpen
			cb.openTime = time.Now()
			log.Printf("Circuit breaker opened")
		}
	}
}

// RecordSuccess records a successful attempt and updates the circuit state
func (cb *CircuitBreaker) RecordSuccess() {
	cb.mu.Lock()
	defer cb.mu.Unlock()

	if cb.state == CircuitHalfOpen {
		cb.halfOpenAttempts++
		if cb.halfOpenAttempts >= cb.halfOpenMax {
			cb.state = CircuitClosed
			cb.failureCount = 0
			cb.halfOpenAttempts = 0
			log.Printf("Circuit breaker closed from half-open")
		}
	}
}

// Allow determines if a request is allowed based on the circuit state
func (cb *CircuitBreaker) Allow() bool {
	cb.mu.RLock()
	defer cb.mu.RUnlock()

	switch cb.state {
	case CircuitClosed:
		return true
	case CircuitOpen:
		if time.Since(cb.openTime) > cb.resetTimeout {
			cb.mu.RUnlock()
			cb.mu.Lock()
			cb.state = CircuitHalfOpen
			cb.halfOpenAttempts = 0
			cb.mu.Unlock()
			cb.mu.RLock()
			return true
		}
		return false
	case CircuitHalfOpen:
		return cb.halfOpenAttempts < cb.halfOpenMax
	default:
		return false
	}
}
