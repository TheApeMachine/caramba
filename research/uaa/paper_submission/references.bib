% Core transformer
@inproceedings{vaswani2017attention,
  title        = {Attention Is All You Need},
  author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle    = {Advances in Neural Information Processing Systems},
  volume       = {30},
  year         = {2017}
}

% Attention interpretability / faithfulness
@inproceedings{jain2019attention,
  title        = {Attention is not Explanation},
  author       = {Jain, Sarthak and Wallace, Byron C.},
  booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)},
  year         = {2019}
}

@inproceedings{serrano2019attention,
  title        = {Is Attention Interpretable?},
  author       = {Serrano, Sofia and Smith, Noah A.},
  booktitle    = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year         = {2019}
}

% Counterfactual / attribution baselines (optional comparison points)
@inproceedings{sundararajan2017ig,
  title        = {Axiomatic Attribution for Deep Networks},
  author       = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  booktitle    = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
  year         = {2017}
}

@inproceedings{li2016erasure,
  title        = {Understanding Neural Networks through Representation Erasure},
  author       = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
  booktitle    = {arXiv preprint arXiv:1612.08220},
  year         = {2016}
}

% Placeholder for UAA-specific citations (fill as needed)
% @article{uaa2026,
%   title   = {Utility-Aligned Attention},
%   author  = {...},
%   year    = {2026}
% }

