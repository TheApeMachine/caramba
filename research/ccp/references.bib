% State-space models
@article{gu2023mamba,
  title        = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author       = {Gu, Albert and Dao, Tri},
  journal      = {arXiv preprint arXiv:2312.00752},
  year         = {2023}
}

@article{peng2023rwkv,
  title        = {{RWKV}: Reinventing {RNNs} for the Transformer Era},
  author       = {Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Biderman, Stella and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and others},
  journal      = {arXiv preprint arXiv:2305.13048},
  year         = {2023}
}

@misc{de2024griffin,
  title         = {Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models},
  author        = {De, Soham and Smith, Samuel L. and Fernando, Anushan and Botev, Aleksandar and Cristian-Muraru, George and Gu, Albert and Haroun, Ruba and Berrada, Leonard and Chen, Yutian and Srinivasan, Srivatsan and Desjardins, Guillaume and Doucet, Arnaud and Budden, David and Teh, Yee Whye and Pascanu, Razvan and De Freitas, Nando and Gulcehre, Caglar},
  year          = {2024},
  eprint        = {2402.19427},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG}
}

% Memory-augmented neural networks
@article{graves2014ntm,
  title        = {Neural Turing Machines},
  author       = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal      = {arXiv preprint arXiv:1410.5401},
  year         = {2014}
}

@article{graves2016dnc,
  title        = {Hybrid computing using a neural network with dynamic external memory},
  author       = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'n}ska, Agnieszka and Colmenarejo, Sergio G{\'o}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and others},
  journal      = {Nature},
  volume       = {538},
  number       = {7626},
  pages        = {471--476},
  year         = {2016},
  publisher    = {Nature Publishing Group}
}

@inproceedings{rae2016sparse,
  title        = {Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes},
  author       = {Rae, Jack W and Hunt, Jonathan J and Danihelka, Ivo and Harley, Timothy and Senior, Andrew W and Wayne, Gregory and Graves, Alex and Lillicrap, Timothy P},
  booktitle    = {Advances in Neural Information Processing Systems},
  volume       = {29},
  year         = {2016}
}

% N-gram cache models
@article{kuhn1990cache,
  title        = {A Cache-Based Natural Language Model for Speech Recognition},
  author       = {Kuhn, Roland and De Mori, Renato},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume       = {12},
  number       = {6},
  pages        = {570--583},
  year         = {1990},
  publisher    = {IEEE}
}

@article{grave2016cache,
  title        = {Improving Neural Language Models with a Continuous Cache},
  author       = {Grave, Edouard and Joulin, Armand and Usunier, Nicolas},
  journal      = {arXiv preprint arXiv:1612.04426},
  year         = {2016}
}

@article{liu2024infinigram,
  title        = {Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens},
  author       = {Liu, Jiacheng and Min, Sewon and Zettlemoyer, Luke and Choi, Yejin and Hajishirzi, Hannaneh},
  journal      = {arXiv preprint arXiv:2401.17377},
  year         = {2024}
}

% Nearest-neighbor / hash-based memory
@article{jegou2011pq,
  title        = {Product Quantization for Nearest Neighbor Search},
  author       = {Jegou, Herve and Douze, Matthijs and Schmid, Cordelia},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume       = {33},
  number       = {1},
  pages        = {117--128},
  year         = {2011},
  publisher    = {IEEE}
}

% Attention mechanisms
@inproceedings{vaswani2017attention,
  title        = {Attention Is All You Need},
  author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle    = {Advances in Neural Information Processing Systems},
  volume       = {30},
  year         = {2017}
}

% Efficient attention
@inproceedings{wang2020linformer,
  title        = {Linformer: Self-Attention with Linear Complexity},
  author       = {Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  booktitle    = {arXiv preprint arXiv:2006.04768},
  year         = {2020}
}

@inproceedings{choromanski2021performer,
  title        = {Rethinking Attention with Performers},
  author       = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
  booktitle    = {International Conference on Learning Representations},
  year         = {2021}
}

@inproceedings{kitaev2020reformer,
  title        = {Reformer: The Efficient Transformer},
  author       = {Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  booktitle    = {International Conference on Learning Representations},
  year         = {2020}
}

@article{beltagy2020longformer,
  title        = {Longformer: The Long-Document Transformer},
  author       = {Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal      = {arXiv preprint arXiv:2004.05150},
  year         = {2020}
}

@inproceedings{zaheer2020bigbird,
  title        = {{BigBird}: Transformers for Longer Sequences},
  author       = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  booktitle    = {Advances in Neural Information Processing Systems},
  volume       = {33},
  year         = {2020}
}

% KV-cache optimization
@article{shazeer2019mqa,
  title        = {Fast Transformer Decoding: One Write-Head is All You Need},
  author       = {Shazeer, Noam},
  journal      = {arXiv preprint arXiv:1911.02150},
  year         = {2019}
}

@inproceedings{ainslie2023gqa,
  title        = {{GQA}: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author       = {Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  year         = {2023}
}

@misc{deepseek2024v2,
  title        = {{DeepSeek-V2}: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
  author       = {{DeepSeek-AI}},
  year         = {2024},
  eprint       = {2405.04434},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL}
}

@inproceedings{hooper2024kvquant,
  title        = {{KVQuant}: Towards 10 Million Context Length {LLM} Inference with {KV} Cache Quantization},
  author       = {Hooper, Coleman and Kim, Sehoon and Mohber, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
  booktitle    = {arXiv preprint arXiv:2401.18079},
  year         = {2024}
}

@misc{li2025commvq,
  title        = {{CommVQ}: Reducing Communication Overhead for KV Cache Compression via Importance-Aware Vector Quantization},
  author       = {Li, Shiquan and others},
  year         = {2025},
  note         = {arXiv preprint}
}

% Compressive / infinite context memory
@misc{munkhdalai2024infiniattention,
  title        = {Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention},
  author       = {Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
  year         = {2024},
  eprint       = {2404.07143},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL}
}

@inproceedings{wu2022memorizing,
  title        = {Memorizing Transformers},
  author       = {Wu, Yuhuai and Rabe, Markus N and Hutchins, DeLesley and Szegedy, Christian},
  booktitle    = {International Conference on Learning Representations},
  year         = {2022}
}

@article{borgeaud2022retro,
  title        = {Improving Language Models by Retrieving from Trillions of Tokens},
  author       = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and van den Driessche, George and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  journal      = {International Conference on Machine Learning},
  year         = {2022}
}

% Memory layers / product keys
@inproceedings{lample2019large,
  title        = {Large Memory Layers with Product Keys},
  author       = {Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'e}gou, Herv{\'e}},
  booktitle    = {Advances in Neural Information Processing Systems},
  volume       = {32},
  year         = {2019}
}

% Vector symbolic architectures
@article{kanerva2009hyperdimensional,
  title        = {Hyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors},
  author       = {Kanerva, Pentti},
  journal      = {Cognitive Computation},
  volume       = {1},
  number       = {2},
  pages        = {139--159},
  year         = {2009},
  publisher    = {Springer}
}

% Hopfield networks / modern associative memory
@inproceedings{ramsauer2021hopfield,
  title        = {Hopfield Networks is All You Need},
  author       = {Ramsauer, Hubert and Sch{\"a}fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlovi{\'c}, Milena and Sandve, Geir Kjetil and others},
  booktitle    = {International Conference on Learning Representations},
  year         = {2021}
}

% Memory operating systems
@misc{li2025memos,
  title        = {{MemOS}: A Memory Operating System for {AI} Systems},
  author       = {Li, Zhiyu and Song, Shichao and Xi, Chenyang and Wang, Hanyu and Tang, Chen and Niu, Simin and Chen, Ding and Yang, Jiawei and Li, Chunyu and Yu, Qingchen and others},
  year         = {2025},
  eprint       = {2507.03724},
  archivePrefix= {arXiv},
  primaryClass = {cs.AI}
}

@misc{packer2024memgpt,
  title        = {{MemGPT}: Towards {LLMs} as Operating Systems},
  author       = {Packer, Charles and Wooders, Sarah and Lin, Kevin and Fang, Vivian and Patil, Shishir G and Stoica, Ion and Gonzalez, Joseph E},
  year         = {2024},
  eprint       = {2310.08560},
  archivePrefix= {arXiv},
  primaryClass = {cs.AI}
}

% Wormhole connections for discrete addressing
@article{gulcehre2018wormhole,
  title        = {Memory Augmented Neural Networks with Wormhole Connections},
  author       = {Gulcehre, Caglar and Chandar, Sarath and Bengio, Yoshua},
  journal      = {arXiv preprint arXiv:1701.08718},
  year         = {2018}
}
