version: 2
name: dba_paper_local_ablations
artifacts_dir: research/dba/benchmark_ablations
notes: >
  Local Mac ablation suite for DBA paper checkpoints.
  Compares baseline, bottleneck, decoupled, GQA, and several DBA design ablations
  (no RoPE, tie QK, null token, gate) using a consistent benchmark suite.

defaults:
  data:
    tokenizer: tiktoken:gpt2
    val_frac: 0.0
  logging:
    instrument: rich
    wandb: false
    wandb_project: ''
    wandb_entity: ''
    wandb_mode: disabled
    eval_iters: 0
  runtime:
    save_every: 1

vars:
  # --- Checkpoints ---
  baseline_ckpt: research/dba/10k_runs/mac_fw1b_l12_baseline_s1337/train_standard_final.pt
  bottleneck_ckpt: research/dba/10k_runs/mac_fw1b_l12_bottleneck_s1337/train_standard_final.pt
  decoupled_ckpt: research/dba/10k_runs/mac_fw1b_l12_decoupled_s1337/train_standard_final.pt
  gqa_ckpt: research/dba/10k_runs/mac_fw1b_l12_gqa_s1337/train_standard_final.pt
  baseline_lr2e4_ckpt: research/dba/10k_runs/mac_fw1b_l12_baseline_lr2e4_s1337/train_standard_final.pt
  baseline_lr4e4_ckpt: research/dba/10k_runs/mac_fw1b_l12_baseline_lr4e4_s1337/train_standard_final.pt
  decoupled_norope_ckpt: research/dba/10k_runs/mac_fw1b_l12_decoupled_norope_s1337/train_standard_final.pt
  decoupled_tiedqk_ckpt: research/dba/10k_runs/mac_fw1b_l12_decoupled_tieqk_s1337/train_standard_final.pt
  decoupled_null_ckpt: research/dba/10k_runs/mac_fw1b_l12_decoupled_null_s1337/train_standard_final.pt
  decoupled_gate_ckpt: research/dba/10k_runs/mac_fw1b_l12_decoupled_gate_s1337/train_standard_final.pt

  # --- Dataset ---
  dataset_path: artifacts/datasets/HuggingFaceFW/fineweb/fineweb_20b.npy
  block_size: 2048

  # --- Model dims (these checkpoints are 12-layer) ---
  d_model: 2048
  n_layers: 12
  n_heads: 32
  # 8:1 ratio (32 Q heads / 4 KV heads) used by the GQA ablation checkpoints.
  n_kv_heads_gqa: 4
  d_ff: 5632
  vocab_size: 50304
  rope_base: 10000.0

  # --- DBA dims for sem8/geo32/v40 (selected config) ---
  sem8_sem_dim: 256
  sem8_geo_dim: 1024
  sem8_attn_dim: 1280

  # --- Runtime ---
  device: mps
  dtype: float16

  # --- Llama 3.2 1B surgery benchmark (HF baseline vs DBA init-only) ---
  llama_teacher_ckpt: hf://meta-llama/Llama-3.2-1B
  llama_dataset_path: artifacts/datasets/fineweb_llama/train.npy
  llama_block_size: 2048
  llama_device: mps
  llama_dtype: float16
  llama_dba_init: fresh
  llama_d_model: 2048
  llama_n_layers: 16
  llama_n_heads: 32
  llama_n_kv_heads: 8
  llama_d_ff: 8192
  llama_vocab_size: 128256
  llama_rope_base: 500000.0
  llama_sem_dim: 128
  llama_geo_dim: 256

# =============================================================================
# YAML ANCHORS - Shared model definitions
# =============================================================================

x-final-layers: &final_layers
  # Match checkpoint topology: final layers are wrapped in a SequentialTopology.
  type: SequentialTopology
  layers:
    - type: RMSNormLayer
      d_model: ${d_model}
      eps: 1e-5
    - type: LinearLayer
      d_in: ${d_model}
      d_out: ${vocab_size}
      bias: false

x-baseline-model: &baseline_model
  type: TransformerModel
  tied_embeddings: false
  embedder:
    type: token
    vocab_size: ${vocab_size}
    d_model: ${d_model}
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                mode: standard
                rope_enabled: true
                rope_base: ${rope_base}
                is_causal: true
                dropout_p: 0.0
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: false
      - *final_layers

x-decoupled-sem8-model: &decoupled_sem8_model
  type: TransformerModel
  tied_embeddings: false
  embedder:
    type: token
    vocab_size: ${vocab_size}
    d_model: ${d_model}
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                mode: decoupled
                dba_train_backend: sdpa
                attn_dim: ${sem8_attn_dim}
                sem_dim: ${sem8_sem_dim}
                geo_dim: ${sem8_geo_dim}
                rope_enabled: true
                rope_base: ${rope_base}
                rope_semantic: false
                tie_qk: false
                null_attn: false
                decoupled_gate: false
                is_causal: true
                dropout_p: 0.0
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: false
      - *final_layers

x-dummy-run: &dummy_run
  - id: eval
    mode: train
    exp: benchmark_only
    # Match the training seed of the checkpoints (paper-local ablations use 1337 most commonly).
    seed: 1337
    steps: 1
    expected: {phase: standard}
    train:
      phase: standard
      batch_size: 1
      block_size: ${block_size}
      lr: 1.0e-9
      device: ${device}
      dtype: ${dtype}

# =============================================================================
# ARCHITECTURE ANCHORS - Match the various checkpoint architectures
# =============================================================================

x-bottleneck-model: &bottleneck_model
  type: TransformerModel
  tied_embeddings: false
  embedder:
    type: token
    vocab_size: ${vocab_size}
    d_model: ${d_model}
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                mode: standard
                # BOTTLENECK: reduced attention dimension (single low-rank bottleneck).
                attn_dim: ${sem8_attn_dim}
                rope_enabled: true
                rope_base: ${rope_base}
                is_causal: true
                dropout_p: 0.0
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: false
      - *final_layers

x-decoupled-model: &decoupled_model
  type: TransformerModel
  tied_embeddings: false
  embedder:
    type: token
    vocab_size: ${vocab_size}
    d_model: ${d_model}
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                mode: decoupled
                dba_train_backend: sdpa
                attn_dim: ${sem8_attn_dim}
                sem_dim: ${sem8_sem_dim}
                geo_dim: ${sem8_geo_dim}
                rope_enabled: true
                rope_base: ${rope_base}
                rope_semantic: false
                tie_qk: false
                null_attn: false
                decoupled_gate: false
                is_causal: true
                dropout_p: 0.0
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: false
      - *final_layers

x-gqa-model: &gqa_model
  type: TransformerModel
  tied_embeddings: false
  embedder:
    type: token
    vocab_size: ${vocab_size}
    d_model: ${d_model}
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                # GQA: fewer KV heads (32Q / 4KV by default).
                n_kv_heads: ${n_kv_heads_gqa}
                mode: standard
                rope_enabled: true
                rope_base: ${rope_base}
                is_causal: true
                dropout_p: 0.0
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: false
      - *final_layers

x-decoupled-norope-model: &decoupled_norope_model
  <<: *decoupled_model
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                mode: decoupled
                dba_train_backend: sdpa
                attn_dim: ${sem8_attn_dim}
                sem_dim: ${sem8_sem_dim}
                geo_dim: ${sem8_geo_dim}
                rope_enabled: false
                tie_qk: false
                null_attn: false
                decoupled_gate: false
                is_causal: true
                dropout_p: 0.0
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: false
      - *final_layers

x-decoupled-tiedqk-model: &decoupled_tiedqk_model
  <<: *decoupled_model
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                mode: decoupled
                dba_train_backend: sdpa
                attn_dim: ${sem8_attn_dim}
                sem_dim: ${sem8_sem_dim}
                geo_dim: ${sem8_geo_dim}
                rope_enabled: true
                rope_base: ${rope_base}
                rope_semantic: false
                tie_qk: true
                null_attn: false
                decoupled_gate: false
                is_causal: true
                dropout_p: 0.0
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: false
      - *final_layers

x-decoupled-null-model: &decoupled_null_model
  <<: *decoupled_model
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                mode: decoupled
                dba_train_backend: sdpa
                attn_dim: ${sem8_attn_dim}
                sem_dim: ${sem8_sem_dim}
                geo_dim: ${sem8_geo_dim}
                rope_enabled: true
                rope_base: ${rope_base}
                rope_semantic: false
                tie_qk: false
                null_attn: true
                decoupled_gate: false
                is_causal: true
                dropout_p: 0.0
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: false
      - *final_layers

x-decoupled-gate-model: &decoupled_gate_model
  <<: *decoupled_model
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                mode: decoupled
                dba_train_backend: sdpa
                attn_dim: ${sem8_attn_dim}
                sem_dim: ${sem8_sem_dim}
                geo_dim: ${sem8_geo_dim}
                rope_enabled: true
                rope_base: ${rope_base}
                rope_semantic: false
                tie_qk: false
                null_attn: false
                decoupled_gate: true
                is_causal: true
                dropout_p: 0.0
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: false
      - *final_layers

# =============================================================================
# Multi-checkpoint benchmark suite for N-way comparison
# =============================================================================
x-multi-benchmark-suite: &multi_benchmark_suite
  - id: ppl_fineweb
    config:
      type: perplexity
      dataset: ${dataset_path}
      block_size: ${block_size}
      batch_size: 1
      num_batches: 200
      # GPT-2 tokenizer vocab is 50257; Caramba models often pad to 50304.
      # Slice logits to this size for fair perplexity.
      valid_vocab_size: 50257
    models: [baseline, bottleneck, decoupled, gqa, baseline_lr2e4, baseline_lr4e4, decoupled_norope, decoupled_tiedqk, decoupled_null, decoupled_gate]

  - id: latency_cached
    config:
      type: latency
      seed: 1337
      prompt_lengths: [128, 512, 1024, 2048, 4096]
      generation_lengths: [128]
      batch_sizes: [1]
      warmup_runs: 2
      timed_runs: 5
      use_cache: true
      cache_kind: fp16
      # Keep token generation in the real tokenizer vocab for comparable inputs.
      valid_vocab_size: 50257
    models: [baseline, bottleneck, decoupled, gqa, baseline_lr2e4, baseline_lr4e4, decoupled_norope, decoupled_tiedqk, decoupled_null, decoupled_gate]

  - id: latency_long_context
    config:
      type: latency
      seed: 1337
      prompt_lengths: [2048, 4096, 8192]
      generation_lengths: [64]
      batch_sizes: [1]
      warmup_runs: 1
      timed_runs: 3
      use_cache: true
      cache_kind: fp16
      valid_vocab_size: 50257
    models: [baseline, bottleneck, decoupled, gqa, baseline_lr2e4, baseline_lr4e4, decoupled_norope, decoupled_tiedqk, decoupled_null, decoupled_gate]

  - id: memory_kv
    config:
      type: memory
      seed: 1337
      sequence_lengths: [512, 1024, 2048, 4096, 8192, 16384]
      batch_sizes: [1]
      measure_peak: true
      measure_kvcache: true
      quantization_modes: [fp16]
      valid_vocab_size: 50257
    models: [baseline, bottleneck, decoupled, gqa, baseline_lr2e4, baseline_lr4e4, decoupled_norope, decoupled_tiedqk, decoupled_null, decoupled_gate]

  - id: behavior_sanity
    config:
      type: behavior
      tokenizer:
        type: tiktoken
        encoding: gpt2
      seed: 1337
      suite_file: benchmark/behavior/cases.yml
      max_new_tokens: 32
      context_window: 2048
      dump_attention: true
      dump_attention_case_ids:
        - copy_simple_3
        - copy_simple_numbers
        - copy_medium_sequence
        - copy_long_sequence_15
        - copy_mixed_alphanumeric
        - copy_special_chars
        - copy_with_spaces_preserved
        - fewshot_simple_transform
        - fewshot_simple_prefix
        - fewshot_simple_suffix
        - fewshot_reverse_words
        - fewshot_first_last_swap
        - fewshot_double_letter
        - fewshot_increment
        - fewshot_double
        - fewshot_square
        - distractor_numbers_explicit
        - distractor_words_explicit
        - distractor_implicit_colors
        - distractor_implicit_numbers
        - distractor_typo_numbers
        - distractor_typo_words
        - distractor_semantic_animals
        - distractor_semantic_numbers
        - reason_compare_simple
        - reason_compare_equal
        - reason_transitive_simple
        - reason_transitive_chain
        - reason_negation_simple
        - reason_double_negation
        - reason_conditional_modus_ponens
        - reason_conditional_modus_tollens
        - reason_set_membership
        - reason_set_exclusion
        - math_add_single
        - math_sub_single
        - math_mul_single
        - math_div_single
        - math_add_two_digit
        - math_sub_two_digit
        - math_mul_two_digit
        - math_add_fewshot
        - math_pattern_fewshot
        - math_negative_result
        - math_negative_input
        - seq_arithmetic_add2
        - seq_arithmetic_add5
        - seq_arithmetic_sub3
        - seq_geometric_x2
        - seq_geometric_x3
        - seq_letters_simple
        - seq_letters_skip
        - seq_alternating
        - seq_fibonacci_start
        - fact_capital_france
        - fact_capital_japan
        - fact_water_formula
        - fact_planets_count
        - fact_days_week
        - fact_months_year
        - semantic_synonym_happy
        - semantic_synonym_big
        - semantic_antonym_hot
        - semantic_antonym_up
        - semantic_category_fruit
        - semantic_category_animal
        - semantic_analogy_animal
        - semantic_analogy_size
        - format_json_simple
        - format_json_nested
        - format_table_extract
        - format_csv_parse
        - format_list_first
        - format_list_last
        - format_bracket_complete
        - format_nested_brackets
        - context_early_retrieval
        - context_late_retrieval
        - context_update_value
        - context_multiple_facts
        - context_instruction_first
        - context_instruction_count
        - robust_rephrase_capital
        - robust_rephrase_capital_alt
        - robust_case_upper
        - robust_case_lower
        - robust_whitespace_normal
        - robust_whitespace_extra
        - edge_single_char
        - edge_single_digit
        - edge_zero_result
        - edge_multiply_zero
        - edge_large_add
        - edge_large_compare
        - edge_repeated_copy
        - edge_all_same
        - attention_fine_discrimination
        - attention_subtle_diff
        - attention_focus_noise
        - attention_recent_vs_distant
        - attention_similar_items
        - attention_binding
        - attention_crossref
        - attention_order_first
        - attention_order_last
        - attention_count_interference
        - instruct_return_first
        - instruct_return_last
        - instruct_count_words
        - instruct_uppercase
        - instruct_reverse
        - consist_add_equals
        - consist_add_question
        - consist_add_words
        - consist_copy_arrow
        - consist_copy_colon
        - consist_copy_echo
      dump_attention_max_tokens: 96
      dump_attention_max_heads: 4
      dump_attention_anchor: ""
      dump_attention_paper_dir: research/dba/figs/attention
      dump_attention_paper_tag: dba_10k
    models: [baseline, bottleneck, decoupled, gqa, baseline_lr2e4, baseline_lr4e4, decoupled_norope, decoupled_tiedqk, decoupled_null, decoupled_gate]

  - id: downstream_accuracy
    config:
      type: accuracy
      tasks: [winogrande, arc_easy, boolq, hellaswag]
      tokenizer:
        type: tiktoken
        encoding: gpt2
      context_window: 2048
      stream_live: true
      stream_every: 10
    models: [baseline, bottleneck, decoupled, gqa, baseline_lr2e4, baseline_lr4e4, decoupled_norope, decoupled_tiedqk, decoupled_null, decoupled_gate]

  - id: context_sweep
    config:
      type: context
      dataset: ${dataset_path}
      context_lengths: [2048, 4096, 8192, 16384, 32768, 65536, 98304, 131072]
      chunk_size: 1024
      max_mask_elems: 32000000
      batch_size: 1
      decode_len: 128
      decode_warmup: 8
      cache_kind: fp16
      # Slice logits / validate dataset ids against tokenizer vocab.
      valid_vocab_size: 50257
    models: [baseline, bottleneck, decoupled, gqa, baseline_lr2e4, baseline_lr4e4, decoupled_norope, decoupled_tiedqk, decoupled_null, decoupled_gate]
  
# =============================================================================
# LLAMA 3.2 1B: HF baseline vs DBA surgery (no training)
# =============================================================================
#
# Run:
#   python3.12 -m caramba research/dba/benchmark_ablations.yml --target target:llama32_1b_surgery_compare
#
x-llama32-baseline-model: &llama32_baseline_model
  type: TransformerModel
  tied_embeddings: false
  embedder:
    type: token
    vocab_size: ${llama_vocab_size}
    d_model: ${llama_d_model}
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${llama_n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${llama_d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${llama_d_model}
                n_heads: ${llama_n_heads}
                n_kv_heads: ${llama_n_kv_heads}
                mode: standard
                rope_enabled: true
                rope_base: ${llama_rope_base}
                is_causal: true
                dropout_p: 0.0
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${llama_d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${llama_d_model}
                d_ff: ${llama_d_ff}
                bias: false
      - type: RMSNormLayer
        d_model: ${llama_d_model}
        eps: 1e-5
      - type: LinearLayer
        d_in: ${llama_d_model}
        d_out: ${llama_vocab_size}
        bias: false

x-llama32-student-model: &llama32_student_model
  type: TransformerModel
  tied_embeddings: false
  embedder:
    type: token
    vocab_size: ${llama_vocab_size}
    d_model: ${llama_d_model}
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${llama_n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${llama_d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${llama_d_model}
                n_heads: ${llama_n_heads}
                n_kv_heads: ${llama_n_kv_heads}
                mode: decoupled
                sem_dim: ${llama_sem_dim}
                geo_dim: ${llama_geo_dim}
                rope_enabled: true
                rope_base: ${llama_rope_base}
                is_causal: true
                dropout_p: 0.0
                decoupled_gate: true
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${llama_d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${llama_d_model}
                d_ff: ${llama_d_ff}
                bias: false
      - type: RMSNormLayer
        d_model: ${llama_d_model}
        eps: 1e-5
      - type: LinearLayer
        d_in: ${llama_d_model}
        d_out: ${llama_vocab_size}
        bias: false

x-llama32-benchmark-suite: &llama32_benchmark_suite
  - id: ppl_llama
    config:
      type: perplexity
      dataset: ${llama_dataset_path}
      block_size: ${llama_block_size}
      batch_size: 1
      num_batches: 200
    models: [baseline, student]

  - id: latency_cached_llama
    config:
      type: latency
      seed: 1337
      prompt_lengths: [128, 512, 1024, 2048, 4096]
      generation_lengths: [128]
      batch_sizes: [1]
      warmup_runs: 2
      timed_runs: 5
      use_cache: true
      cache_kind: fp16
    models: [baseline, student]

  - id: memory_kv_llama
    config:
      type: memory
      seed: 1337
      sequence_lengths: [512, 1024, 2048, 4096, 8192, 16384]
      batch_sizes: [1]
      measure_peak: true
      measure_kvcache: true
      quantization_modes: [fp16]
    models: [baseline, student]

  - id: behavior_sanity_llama
    config:
      type: behavior
      tokenizer:
        type: llama
        model_id: meta-llama/Llama-3.2-1B
      seed: 42
      suite_file: benchmark/behavior/cases.yml
      max_new_tokens: 32
      context_window: 2048
      dump_attention: true
      dump_attention_max_tokens: 96
      dump_attention_max_heads: 4
      dump_attention_anchor: ""
      dump_attention_paper_dir: research/dba/figs/attention
      dump_attention_paper_tag: llama32_surgery
    models: [baseline, student]

  - id: downstream_accuracy_llama
    config:
      type: accuracy
      tasks: [winogrande, arc_easy]
      tokenizer:
        type: llama
        model_id: meta-llama/Llama-3.2-1B
      context_window: 2048
      stream_live: true
      stream_every: 10
    models: [baseline, student]
# =============================================================================
# TARGETS
# =============================================================================

targets:
  # ===========================================================================
  # PRIMARY: Multi-checkpoint N-way comparison (baseline vs sem16 vs sem8)
  # ===========================================================================
  - type: experiment
    name: multi_checkpoint_compare
    description: "Compare all 8 checkpoints: baseline vs bottleneck vs decoupled vs gqa vs decoupled_norope vs decoupled_tiedqk vs decoupled_null vs decoupled_gate."
    backend: torch
    task: task.language_modeling
    data:
      ref: dataset.tokens
      config:
        path: ${dataset_path}
        block_size: ${block_size}
    system:
      ref: system.language_model
      config:
        model: *baseline_model
    objective: objective.next_token_ce
    trainer:
      ref: trainer.multi_checkpoint_compare
      config:
        checkpoints:
          - name: baseline
            checkpoint: ${baseline_ckpt}
            model_config: *baseline_model
            is_baseline: true
          - name: bottleneck
            checkpoint: ${bottleneck_ckpt}
            model_config: *bottleneck_model
            is_baseline: false
          - name: decoupled
            checkpoint: ${decoupled_ckpt}
            model_config: *decoupled_model
            is_baseline: false
          - name: gqa
            checkpoint: ${gqa_ckpt}
            model_config: *gqa_model
            is_baseline: false
          - name: baseline_lr2e4
            checkpoint: ${baseline_lr2e4_ckpt}
            model_config: *baseline_model
            is_baseline: false
          - name: baseline_lr4e4
            checkpoint: ${baseline_lr4e4_ckpt}
            model_config: *baseline_model
            is_baseline: false
          - name: decoupled_norope
            checkpoint: ${decoupled_norope_ckpt}
            model_config: *decoupled_norope_model
            is_baseline: false
          - name: decoupled_tiedqk
            checkpoint: ${decoupled_tiedqk_ckpt}
            model_config: *decoupled_tiedqk_model
            is_baseline: false
          - name: decoupled_null
            checkpoint: ${decoupled_null_ckpt}
            model_config: *decoupled_null_model
            is_baseline: false
          - name: decoupled_gate
            checkpoint: ${decoupled_gate_ckpt}
            model_config: *decoupled_gate_model
        device: ${device}
        dtype: ${dtype}
        strict: false
        unsafe_pickle_load: false
    runs: *dummy_run
    benchmarks: *multi_benchmark_suite

  - type: experiment
    name: llama32_1b_surgery_compare
    description: "HF Llama-3.2-1B baseline vs DBA surgery (init-only) using the paper benchmark harness."
    backend: torch
    task: task.language_modeling
    data:
      ref: dataset.tokens
      config:
        path: ${llama_dataset_path}
        block_size: ${llama_block_size}
    system:
      ref: system.language_model
      config:
        model: *llama32_student_model
    objective: objective.next_token_ce
    trainer:
      ref: trainer.surgery_compare
      config:
        teacher_ckpt: ${llama_teacher_ckpt}
        teacher_model: *llama32_baseline_model
        student_model: *llama32_student_model
        dba_init: ${llama_dba_init}
        device: ${llama_device}
        dtype: ${llama_dtype}
    runs: *dummy_run
    benchmarks: *llama32_benchmark_suite

entrypoints:
  default: "target:multi_checkpoint_compare"
