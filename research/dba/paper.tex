\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\graphicspath{{./}{figs/attention/}{research/dba/figs/attention/}{figs/}{research/dba/figs/}}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
% \usepackage{algorithm}  % Not currently used
% \usepackage{algorithmic}  % Not currently used
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
% \usepackage{microtype}  % Disabled: causes font expansion errors
\usepackage[margin=1in]{geometry}  % Consistent margins
\usepackage{placeins} % Provides \FloatBarrier to prevent float reordering across sections

% Custom colors
\definecolor{accent}{RGB}{45, 106, 79}

% Checkmark and xmark symbols for tables
\newcommand{\cmark}{\textcolor{green!60!black}{\checkmark}}
\newcommand{\xmark}{\textcolor{red!70!black}{$\times$}}

\title{\textbf{Decoupled Bottleneck Attention:} \\[0.3em]
\large Low-Rank Semantic Routing as Structural Regularization}

\author{
  Daniel Owen van Dommelen\\
  \textit{Independent Research}\\
  \texttt{theapemachine@gmail.com}
}

\date{January 2026}

\begin{document}

\maketitle

% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
We propose \textbf{Decoupled Bottleneck Attention (DBA)}, an attention architecture that separates low-rank semantic routing ($d_{\text{sem}}{=}8$ dims/head) from higher-rank positional geometry ($d_{\text{geo}}{=}32$ dims/head), with RoPE applied only to the geometric path.

At 1B scale (100k steps on FineWeb-Edu), DBA incurs a \textbf{6\% increase in held-out perplexity} (13.53 vs 12.76) but yields \textbf{1.6$\times$ KV-cache reduction} (112,640 vs 180,224 bytes/token) and \textbf{1.12$\times$ faster cached decode} (85.8 vs 76.9 tok/s), measured end-to-end on Metal.

Despite this quality gap, behavioral probes (117 tests, 15 categories) show no evidence of capability collapse: accuracy is comparable (27.4\% vs 26.5\%), with head-to-head wins at 7-6. This suggests the removed capacity is not uniformly task-relevant.

DBA characterizes a concrete efficiency--quality tradeoff in attention design, not a free compression method. It is orthogonal to GQA and KV-cache quantization and can be composed with them.
\end{abstract}

\paragraph{Keywords:}
Transformer, attention mechanism, low-rank, attention drift, context hallucination, structural regularization, KV-cache, memory efficiency, robustness

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}

Modern Transformer architectures \cite{vaswani2017attention} achieve remarkable performance across language modeling, translation, and reasoning tasks. However, we hypothesize that in autoregressive language modeling, \textbf{high-rank semantic routing may be unnecessary}: allocating large bandwidth to token-token matching could encourage the model to attend broadly to context patterns rather than the immediate instruction.

When a 64-dimensional attention head looks back at context, it can represent fine-grained distinctions across all tokens. This bandwidth may allow the model to attend to irrelevant tokens---few-shot examples, repeated patterns, distractor content. We call this potential failure mode \textbf{attention drift}. Our experiments show patterns consistent with this hypothesis, though we have not established a causal mechanism.

\subsection{The Bottleneck as Regularizer (Hypothesis)}

We propose a simple architectural modification: force attention through a narrow bottleneck. By constraining the semantic routing path to extremely low rank ($d_{\text{sem}}{=}8$ dims/head), the model has limited capacity to represent token-token relationships. We hypothesize that this forces prioritization---the most relevant signals may survive the bottleneck while less relevant patterns are suppressed.

If this hypothesis holds, efficient attention becomes not merely a memory optimization, but a \textbf{structural regularizer}. Our attention visualizations (Section 3) show patterns consistent with this interpretation, though we have not established causality.

\subsection{The Redundancy Hypothesis}

DBA originates from a simple question: do neurons move in \textit{sympathetic clusters}---correlated groups that effectively reduce the dimensionality of the representation? If so, could we reduce computation by exploiting this redundancy?

Prior work suggests such redundancy exists. LoRA \cite{hu2021lora} demonstrated that weight \textit{updates} during fine-tuning are low-rank (typically $r \leq 64$). Recent work on gradient dynamics \cite{refael2024adarankgrad} shows that optimization naturally collapses to low rank. These results suggest that the effective dimensionality of learned representations may be much lower than the architectural dimensionality.

We test this hypothesis directly: if attention routing is intrinsically low-rank, then constraining the architecture to low rank should preserve most capability while reducing compute. The fact that DBA achieves competitive performance with 8-dimensional semantic routing (vs 64-dimensional baseline) provides evidence that significant redundancy exists---though we have not directly measured the intrinsic rank of baseline attention.

Appendix~\ref{app:effective_rank} reserves space for empirical effective-rank measurements on the final checkpoints.

\subsection{Comparison with Existing Approaches}

\paragraph{Grouped-Query Attention (GQA).}
While Grouped-Query Attention \cite{ainslie2023gqa} successfully reduces KV-cache memory by sharing key-value heads across multiple query heads, it maintains the full computational cost of the query projection and attention scoring in the high-dimensional space. Each query still operates in $\mathbb{R}^{d}$, and every attention score still requires a $d$-dimensional dot product---GQA merely amortizes the \textit{storage} cost, not the \textit{interaction} cost.

Our Bottleneck approach reduces both memory \textit{and} compute by compressing the interaction manifold. Rather than sharing high-dimensional KV pairs, we project queries and keys into a low-rank semantic subspace ($r \ll d$) \textit{before} computing attention, reducing dot-product complexity from $O(n^2 d)$ to $O(n^2 r)$.

\paragraph{Multi-Head Latent Attention (MLA).}
DeepSeek-V2 \cite{deepseek2024v2} introduced MLA, which compresses KV storage into a latent vector, achieving 93\% cache reduction. However, MLA \textit{up-projects} during the forward pass to perform attention in the original high-dimensional space. Our method remains low-rank throughout, saving both memory and compute.

\paragraph{Key distinction.}
\textbf{GQA} shares KV \emph{storage}; \textbf{MLA} compresses KV \emph{storage} but expands for scoring; \textbf{DBA} compresses the \emph{interaction dimension} used to compute attention scores, while preserving a higher-fidelity geometric (RoPE) path.

\paragraph{Disentangled Attention.}
DeBERTa \cite{he2020deberta} pioneered the separation of content and position representations in attention scoring. We adopt this disentanglement principle but leverage it for \textit{efficiency}: applying aggressive compression to the semantic (content) path while preserving fidelity in the geometric (position) path.

\subsection{Contributions}

\begin{enumerate}
    \item We propose \textbf{Decoupled Bottleneck Attention (DBA)}, separating semantic routing ($d_{\text{sem}}{=}8$ dims/head) from positional geometry ($d_{\text{geo}}{=}32$ dims/head), achieving \textbf{37.5\% KV-cache reduction} (112,640 vs 180,224 bytes/token measured).
    \item We characterize the \textbf{efficiency/quality tradeoff}: DBA incurs a 6\% held-out perplexity increase (13.53 vs 12.76) while delivering 12\% faster cached decode (85.8 vs 76.9 tok/s) and 1.6$\times$ memory reduction.
    \item We evaluate both architectures on an \textbf{extended behavioral benchmark} (117 tests, 15 categories) and find no evidence of capability collapse: 27.4\% vs 26.5\% accuracy, head-to-head 7-6 in favor of DBA.
    \item We provide a reproducible evaluation harness (\path{research/dba/benchmark.yml}) and release all checkpoints, enabling independent verification.
    \item We propose the ``bandwidth-as-regularization'' hypothesis as a potential mechanistic explanation, noting that further investigation is needed.
\end{enumerate}

% ============================================================================
% RELATED WORK
% ============================================================================
\section{Related Work}
\label{sec:related_work}

\paragraph{Low-rank and approximate attention.}
A long line of work seeks to reduce the quadratic cost of attention by approximating the score computation or constraining its rank. Linformer \cite{wang2020linformer} projects keys and values into a lower-dimensional subspace along the sequence dimension, yielding linear-time attention under a low-rank assumption. Kernel and hashing methods such as Performer \cite{choromanski2021performer} and Reformer \cite{kitaev2020reformer} similarly reduce attention cost via randomized features or locality-sensitive hashing. Our setting is different: we train standard causal LMs, but explicitly reduce the query/key interaction dimension inside each layer, targeting both compute (\(O(n^2 r)\)) and KV-cache memory (\(O(n r)\)).

\paragraph{Sparse/local attention for long documents.}
Sparse patterns (e.g., sliding window with global tokens) as in Longformer \cite{beltagy2020longformer} and BigBird \cite{zaheer2020bigbird} reduce attention compute while retaining access to distant context. However, for autoregressive decoding these methods still accumulate a KV cache whose size grows linearly with context length. Our work instead reduces the per-token cache footprint, which is complementary to sparse attention and other long-context strategies \cite{huang2023longcontextsurvey}.

\paragraph{KV-cache optimization.}
Sharing KV heads reduces cache storage by amortizing keys and values across query heads (MQA/GQA) \cite{shazeer2019mqa,ainslie2023gqa}. Latent KV schemes such as MLA compress the cache into a lower-dimensional latent that is expanded during attention \cite{deepseek2024v2}. Orthogonally, quantizing the KV cache reduces memory at fixed architecture \cite{hooper2024kvquant,li2025commvq}. Our decoupled bottleneck reduces the interaction dimension before scoring (saving compute) and also makes heterogeneous KV quantization natural: semantic keys can often be quantized more aggressively than geometric keys.

\paragraph{Expressiveness limits and structured alternatives.}
Reducing interaction rank too far can harm representation power: theory and empirical evidence show regimes where increasing heads under fixed head dimension does not recover lost capacity \cite{bhojanapalli2020lowrank,amsel2025qualityheads}. Recent structured-matrix formulations aim to increase effective rank without full cost by parameterizing attention maps with richer structured operators \cite{kuang2025structuredmatrices}. Decoupling is a simple architectural compromise: we keep a higher-dimensional geometric path (with RoPE) while aggressively compressing only the semantic routing path.

% ============================================================================
% 2. METHODOLOGY
% ============================================================================
\section{Methodology}

\subsection{Standard Multi-Head Attention}

In standard scaled dot-product attention with $H$ heads:
\begin{equation}
    \text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
\end{equation}
where $Q, K, V \in \mathbb{R}^{n \times d}$ are obtained by linear projection from the input $X \in \mathbb{R}^{n \times d_{\text{model}}}$:
\begin{equation}
    Q = XW_Q, \quad K = XW_K, \quad V = XW_V
\end{equation}
with $W_Q, W_K, W_V \in \mathbb{R}^{d_{\text{model}} \times d}$. For language modeling with context length $n$ and dimension $d$, the KV-cache requires $O(2 \cdot L \cdot n \cdot d)$ memory, where $L$ is the number of layers.

\subsection{Bottleneck Attention}

We introduce a simple modification: project $Q$ and $K$ to a lower-dimensional space \textit{before} computing attention scores.\footnote{Our use of ``bottleneck'' refers to dimensionality reduction in the query/key space, distinct from Park et al.'s BAM \cite{park2018bam}, which applies channel and spatial attention in CNNs for computer vision.}
\begin{equation}
    Q' = XW_Q', \quad K' = XW_K'
\end{equation}
where $W_Q', W_K' \in \mathbb{R}^{d_{\text{model}} \times d_{\text{attn}}}$ with $d_{\text{attn}} \ll d_{\text{model}}$. The attention computation becomes:
\begin{equation}
    \text{Attn}_{\text{bottleneck}}(Q', K', V') = \text{softmax}\left(\frac{Q'K'^\top}{\sqrt{d_{\text{attn}}/H}}\right) V'
\end{equation}

This reduces the dot-product complexity from $O(n^2 \cdot d_{\text{model}})$ to $O(n^2 \cdot d_{\text{attn}})$ and the KV-cache from $O(n \cdot d_{\text{model}})$ to $O(n \cdot d_{\text{attn}})$.

\subsection{Decoupled Bottleneck Attention}

The key insight motivating decoupling is that \textit{semantic matching} (``is this token semantically related?'') and \textit{geometric positioning} (``how far away is this token?'') have different intrinsic dimensionality requirements.

We decompose the attention score into two additive components:
\begin{equation}
    \text{Score} = \underbrace{\frac{Q_{\text{sem}} K_{\text{sem}}^\top}{\sqrt{d_{\text{sem}}/H}}}_{\text{Semantic Path}} + \underbrace{\frac{Q_{\text{geo}} K_{\text{geo}}^\top}{\sqrt{d_{\text{geo}}/H}}}_{\text{Geometric Path}}
\end{equation}

\noindent The final attention weights are $\text{Attn} = \text{softmax}(\text{Score}) \cdot V$, where:
\begin{align}
    Q_{\text{sem}} &= XW_{Q,\text{sem}}, \quad K_{\text{sem}} = XW_{K,\text{sem}} \quad &(d_{\text{sem}} = 256 \text{; 8 dims/head}) \\
    Q_{\text{geo}} &= XW_{Q,\text{geo}}, \quad K_{\text{geo}} = XW_{K,\text{geo}} \quad &(d_{\text{geo}} = 1024 \text{; 32 dims/head})
\end{align}

Critically, we apply \textbf{Rotary Position Embeddings (RoPE)} \cite{su2021roformer} \textit{only} to the geometric path:
\begin{equation}
    Q_{\text{geo}}, K_{\text{geo}} \leftarrow \text{RoPE}(Q_{\text{geo}}, K_{\text{geo}}, \text{position})
\end{equation}

The semantic path operates on pure content similarity (controlled by \texttt{rope\_semantic=false} in the manifest), while the geometric path encodes positional relationships. The value projection uses dimension $d_{\text{attn}}$:
\begin{equation}
    V = XW_V, \quad W_V \in \mathbb{R}^{d_{\text{model}} \times d_{\text{attn}}}
\end{equation}
where $d_{\text{attn}} = 1280$ (40 dims/head). With $H{=}32$ heads, our configuration yields 8 dims/head for semantic and 32 dims/head for geometric routing, preserving higher fidelity for position-dependent RoPE interactions while aggressively compressing the semantic path.

\paragraph{Optional semantic/geometric gating (not used in primary experiments).}
In addition to the additive score composition above, our implementation optionally enables a learnable per-head gate that rescales the semantic vs.\ geometric query streams before scoring. In this draft, unless explicitly labeled otherwise, all reported DBA results use the \emph{ungated} variant (\texttt{decoupled\_gate=false}) to keep the primary comparison focused on the bottleneck/decoupling itself.

\subsection{Optional Null Token (stability at extreme rank)}

Low-rank attention can become unstable when queries lack semantically appropriate keys. We introduce a learnable \textbf{null token} $k_\emptyset$ providing an explicit ``attend nowhere'' option:
\begin{equation}
    \text{Score}_{\text{null}} = \frac{Q_{\text{sem}} k_{\emptyset,\text{sem}}^\top}{\sqrt{d_{\text{sem}}/H}} + \frac{Q_{\text{geo}} k_{\emptyset,\text{geo}}^\top}{\sqrt{d_{\text{geo}}/H}}
\end{equation}

This score is concatenated to the attention matrix before softmax, allowing the model to ``dump'' attention mass when no key is appropriate, which can stabilize training at very low ranks. In our flagship decoupled preset, the null token is disabled by default and treated as an ablation (Appendix~\ref{app:decoupled_ablations}).

\subsection{Tied Q-K Projections}

For the semantic path, we optionally \textbf{tie} the query and key projections: $W_{Q,\text{sem}} = W_{K,\text{sem}}$. This enforces symmetric similarity (``A attends to B iff B attends to A''), which is appropriate for content matching but not for position-dependent relationships.

\subsection{Quantized Inference}

For inference, we apply aggressive quantization to the KV-cache. Recent work has demonstrated that 4-bit KV cache quantization preserves model quality remarkably well. Turboderp's ExLlamaV2 implementation \cite{turboderp2024qcache} showed Q4 cache performs comparably to FP16, and this capability has been integrated into production inference engines like llama.cpp \cite{llamacpp2024kvcache}. We implement block-wise Q4\_0 quantization following this approach:
\begin{equation}
    x_{\text{quantized}} = \text{round}\left(\frac{x}{\text{scale}}\right), \quad \text{scale} = \frac{\max(|x_{\text{block}}|)}{7}
\end{equation}
where each block of 32 elements shares a single FP16 scale factor. In the idealized limit (ignoring scale metadata) 4-bit values correspond to 0.5 bytes/value. With Q4\_0 block scales, the effective bytes/value is slightly larger (18 bytes per 32 values $\Rightarrow$ 0.5625 bytes/value), so the ideal 4$\times$ factor becomes $\approx 3.56\times$ in practice. Combined with dimension reduction, the per-layer KV-cache reduction is approximately:
\begin{equation}
    \text{Compression} \approx \underbrace{\frac{d_{\text{model}}}{d_{\text{attn}}}}_{\text{Dimension}} \times \underbrace{\frac{2~\text{bytes}}{0.5625~\text{bytes}}}_{\text{Q4\_0 (incl.\ scale)}} \;\;\approx\;\; \frac{d_{\text{model}}}{d_{\text{attn}}}\times 3.56.
\end{equation}
For a representative setting with $d_{\text{model}}/d_{\text{attn}} \approx 1.33$ (e.g., $2048\!\rightarrow\!1536$), homogeneous Q4\_0 KV-cache quantization implies an implementation-aligned compression of $\approx 1.33\times 3.56 \approx 4.7\times$ versus a standard FP16 baseline (before accounting for any heterogeneous policy choices).

\paragraph{Scaling arithmetic (context only; not validated at scale).}
The KV-cache memory at long context depends on the choice of attention dimension $d_{\text{attn}}$ at scale. For a rough Llama-like configuration (32 layers, $d_{\text{model}}=4096$, 128k context, batch=1), the FP16 KV cache is:
\[
M_{\text{FP16}} \approx 2 \cdot 32 \cdot 4096 \cdot 128\text{k} \cdot 2 \text{ bytes} \approx 64~\text{GiB}.
\]
With 4-bit KV-cache quantization (idealized 0.5 bytes/value), the memory becomes:
\[
M_{\text{Q4}} \approx 2 \cdot 32 \cdot d_{\text{attn}} \cdot 128\text{k} \cdot 0.5 \text{ bytes}.
\]
This yields two reference scenarios (for context):
\begin{itemize}
    \item \textbf{Constant-fraction $d_{\text{attn}}$ (e.g., $d_{\text{attn}}=768$):} $M_{\text{Q4}} \approx 3.0$~GiB, for an overall reduction of $\sim 21\times$.
    \item \textbf{Speculative fixed-rank $d_{\text{attn}}$ (intuition only):} If one could keep $d_{\text{attn}}$ roughly constant while scaling $d_{\text{model}}$ (e.g., $d_{\text{attn}}{=}96$ at $d_{\text{model}}{=}4096$), the same linear arithmetic yields an $\mathcal{O}(10^2)$ reduction versus a standard FP16 baseline. We do \emph{not} validate fixed-rank scaling in this work.
\end{itemize}
The architectural contribution is the \textit{dimension reduction} (the ratio $4096/d_{\text{attn}}$); the additional factor of $4\times$ comes from standard 16$\rightarrow$4-bit quantization (idealized). For fair comparisons, note that GQA caches can also be quantized; e.g., an $8\times$ GQA KV cache with Q4 would already yield $\sim$32$\times$ reduction vs FP16 standard. We therefore treat fixed-rank scaling numbers as back-of-the-envelope upper bounds, not a primary experimental claim.

\paragraph{Heterogeneous KV-cache quantization (decoupled).}
A practical benefit of decoupling is that it enables \textit{heterogeneous} KV-cache quantization: we can compress the semantic path more aggressively (e.g., Q4) while keeping the geometric (RoPE) path at higher fidelity (e.g., Q8). In this draft, heterogeneous KV-cache policies are treated as \textit{planned work}: we will report both quality deltas (held-out perplexity) and end-to-end device memory deltas at long context once the corresponding inference benchmarks are finalized.

In this draft, we do not yet report the end-to-end device memory deltas at 128k context; instead we report (i) theoretical KV-cache bytes/token estimates derived from the checkpoint \texttt{ModelConfig} and (ii) empirical long-context stability and decode-at-context timing from the benchmark harness (\path{research/dba/benchmark.yml}). End-to-end memory instrumentation remains planned work.

While we report training throughput in our experiments, the theoretical FLOPs reduction in the attention mechanism ($O(n^2 d) \to O(n^2 r)$) implies a proportional speedup in the \textit{prefill phase} of inference, where the KV-cache is populated. For autoregressive decoding, the memory bandwidth savings from the smaller cache dominate latency improvements.

% ============================================================================
% 3. EXPERIMENTS
% ============================================================================
\section{Experiments}

\subsection{Experimental Setup}

\paragraph{Reproducibility.}
All paper experiments are executed via declarative YAML manifests.\footnote{Experiments are run using Caramba, a manifest-driven ML research framework developed for this work. Caramba provides declarative specification of model topology, training protocols, and evaluation harnesses, with custom Triton and Metal kernels for improved throughput. Available at \url{https://github.com/theapemachine/caramba}.} Two configurations are defined:

\begin{table}[htbp]
\centering
\footnotesize
\caption{Experimental configurations (from manifest presets).}
\label{tab:configs}
\begin{tabular}{@{}llccccc@{}}
\toprule
\textbf{Suite} & \textbf{Configuration} & $d_{\text{sem}}$ & $d_{\text{geo}}$ & \textbf{Per-head} & \textbf{Steps} & \textbf{Status} \\
\midrule
\multirow{2}{*}{A100 (22L, 1B)}
  & Baseline & --- & --- & 64 & 100k & Complete \\
  & Decoupled (sem8/geo32/v40) & 256 & 1024 & 8+32 & 100k & Complete \\
\midrule
\multirow{4}{*}{Local (12L, 550M)}
  & Baseline ($\times$3) & --- & --- & 64 & 5k & Planned \\
  & Bottleneck ($\times$3) & --- & --- & 48 & 5k & Planned \\
  & Decoupled ($\times$3) & 256 & 1024 & 8+32 & 5k & Planned \\
  & GQA 32Q/4KV ($\times$3) & --- & --- & 64 & 5k & Planned \\
\bottomrule
\end{tabular}
\end{table}

\noindent \textbf{A100 suite:} $d_{\text{model}}{=}2048$, $n_{\text{layers}}{=}22$, $n_{\text{heads}}{=}32$, $d_{\text{ff}}{=}5632$, vocab${=}50304$, \texttt{rope\_base=10000}, global batch size 128 (micro-batch 32 $\times$ 4 gradient accumulation), FineWeb-Edu 20B tokens.

\noindent \textbf{Local suite:} Same per-layer architecture but $n_{\text{layers}}{=}12$ ($\sim$550M params), FineWeb-Edu 1B tokens, 3 seeds (1337, 1338, 1339) for statistical validation. Manifest: \path{config/presets/dba_paper_local.yml}.

\noindent The canonical training invocations for the primary comparison are:
\begin{verbatim}
python -m caramba.cli run config/presets/dba_paper_rerun.yml --target baseline
python -m caramba.cli run config/presets/dba_paper_rerun.yml --target decoupled
\end{verbatim}

\paragraph{Datasets.}
Training uses a large tokenized FineWeb-Edu dump (\texttt{fineweb\_20b.npy}; \(\sim\)20B tokens). For lightweight post-hoc evaluation on local hardware we use smaller token shards from the same pipeline: \texttt{fineweb\_100m.npy} and \texttt{fineweb\_1b.npy}.

\paragraph{Evaluation and artifacts (this paper).}
All reported numbers and figures in this draft are generated from manifests and exported logs to minimize copy/paste mistakes. We use a local benchmark harness (Section~3; \path{research/dba/benchmark.yml} and variants) for paired checkpoint comparisons; it writes plots/tables into \path{research/dba/} via \texttt{artifacts\_dir} to avoid manual copying. For behavioral probes (identity retention and semantic collision), we include per-case teacher/student outputs as a generated appendix table when available (Appendix~\ref{app:behavior_cases}).

\paragraph{Training dynamics (A100; 1B; 100k steps).}
Figure~\ref{fig:a100_training_curves} summarizes the training loss and perplexity for the A100 training runs (\texttt{baseline} vs.\ \texttt{decoupled}). These plots are generated directly from the exported W\&B CSV in \path{research/dba/} to avoid manual copy errors. The comparison uses the configuration from \path{config/presets/dba_paper_rerun.yml}: $d_{\text{sem}}{=}256$, $d_{\text{geo}}{=}1024$, $d_{\text{attn}}{=}1280$, trained for 100{,}000 steps with seed 42.

\begin{figure}[htbp]
\centering
\begin{subfigure}[t]{0.49\textwidth}
  \centering
  \IfFileExists{A100-1b-100k-loss.png}{
    \includegraphics[width=\textwidth]{\detokenize{A100-1b-100k-loss.png}}
  }{
    \fbox{\parbox{0.92\textwidth}{\textbf{Missing figure:} \texttt{A100-1b-100k-loss.png}.}}
  }
  \caption{Training loss vs.\ step.}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.49\textwidth}
  \centering
  \IfFileExists{A100-1b-100k-ppl.png}{
    \includegraphics[width=\textwidth]{\detokenize{A100-1b-100k-ppl.png}}
  }{
    \fbox{\parbox{0.92\textwidth}{\textbf{Missing figure:} \texttt{A100-1b-100k-ppl.png}.}}
  }
  \caption{Perplexity proxy ($\exp(\text{loss})$) vs.\ step.}
\end{subfigure}
\caption{A100 training curves for the 1B/100k-step run (\texttt{baseline} vs.\ \texttt{decoupled}).}
\label{fig:a100_training_curves}
\end{figure}

% Auto-generated summary table from the W\&B export CSV.
\IfFileExists{A100-1b-100k-training_summary.tex}{
  \input{\detokenize{A100-1b-100k-training_summary.tex}}
}{}

\paragraph{Optimization stability note (warmup boundary).}
In early decoupled runs we observed transient loss spikes at the end of LR warmup (peak LR), even when the overall training curve matched the baseline. We mitigate this by using a more conservative peak LR for the decoupled configuration, adding gradient clipping, and applying small implementation-level compensations for the decoupled bottleneck. A diagnostic plot (loss min/max band and LR) follows:

\begin{figure}[htbp]
\centering
\IfFileExists{A100-1b-100k-warmup_spikes.png}{
  \includegraphics[width=0.9\textwidth]{\detokenize{A100-1b-100k-warmup_spikes.png}}
}{}
\caption{Warmup-boundary diagnostic: loss (with min/max envelope when logged) and learning-rate schedule.}
\label{fig:warmup_spikes}
\end{figure}

\paragraph{Completed: A100 1B 100k-step training.}
The 100k-step training has completed for both baseline and decoupled configurations (Table~\ref{tab:a100-training-summary-100k}). Training loss converges similarly: baseline 2.67 vs DBA 2.68. However, held-out perplexity on FineWeb-Edu reveals a larger gap: \textbf{baseline 12.76 PPL vs DBA 13.53 PPL} (6\% relative increase). This distinction between training loss and held-out evaluation is important: DBA trains efficiently but generalizes slightly worse.

\paragraph{Training efficiency.}
\label{sec:training_efficiency}
Figures~\ref{fig:gpu_memory}--\ref{fig:grad_norms} present training efficiency metrics from the A100 runs. The data reveals a compelling efficiency story:

\begin{figure}[htbp]
\centering
\IfFileExists{A100-1b-100k-gpu_memory.png}{
  \includegraphics[width=0.85\textwidth]{\detokenize{A100-1b-100k-gpu_memory.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Missing figure:} \texttt{A100-1b-10k-gpu\_memory.png}.}}
}
\caption{GPU memory allocated during training. DBA reduces static model memory by 10.4\% total (37.5\% in attention layers) due to the compressed Q/K/V projections.}
\label{fig:gpu_memory}
\end{figure}

\begin{figure}[htbp]
\centering
\IfFileExists{A100-1b-100k-tok_s.png}{
  \includegraphics[width=0.85\textwidth]{\detokenize{A100-1b-100k-tok_s.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Missing figure:} \texttt{A100-1b-10k-tok\_s.png}.}}
}
\caption{Training throughput (tokens/sec). Both models show similar throughput patterns during 100k steps of training. The smaller attention projections in DBA provide modest throughput benefits during the attention computation phase.}
\label{fig:tok_s}
\end{figure}

\begin{figure}[htbp]
\centering
\IfFileExists{A100-1b-100k-grad_norms.png}{
  \includegraphics[width=0.85\textwidth]{\detokenize{A100-1b-100k-grad_norms.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Missing figure:} \texttt{A100-1b-10k-grad\_norms.png}.}}
}
\caption{Gradient norms during training. Baseline exhibits spikes up to 2.5 early in training (around warmup end at step 2000). DBA runs use \texttt{grad\_clip\_norm=1.0}, visible as clipped peaks, providing more stable optimization dynamics. All configurations converge to similar gradient magnitudes ($\sim$0.3--0.5) by end of training.}
\label{fig:grad_norms}
\end{figure}

\noindent\textbf{Key observations:}
\begin{itemize}
    \item \textbf{Parameter reduction:} DBA reduces attention parameters by 37.5\% (verified from checkpoint configs). Total model parameter reduction is 10.4\%.
    \item \textbf{KV-cache reduction:} 37.5\% reduction in KV-cache elements per token (2560 vs 4096), derived directly from architectural dimensions.
    \item \textbf{Training throughput:} Both models show similar throughput patterns during 100k steps. We do not claim significant throughput improvements during training---the primary benefit is inference memory, not training speed.
    \item \textbf{Stability:} Gradient clipping (\texttt{grad\_clip\_norm=1.0}) combined with lower LR ($2 \times 10^{-4}$ vs.\ $3 \times 10^{-4}$) stabilizes DBA training.
\end{itemize}

\paragraph{Measured inference efficiency.}
Beyond architectural accounting, we measured end-to-end inference using custom Metal kernels on Apple Silicon (M-series GPU, fp16, batch=1):
\begin{itemize}
    \item \textbf{KV-cache memory:} baseline 180,224 bytes/token, DBA 112,640 bytes/token (37.5\% reduction, matching architectural prediction).
    \item \textbf{Cached decode throughput:} baseline 76.9 tok/s, DBA 85.8 tok/s (12\% speedup).
    \item \textbf{Long-prompt decode:} baseline 21.9 tok/s, DBA 24.6 tok/s (12\% speedup at 2048+ context).
\end{itemize}
Absolute values are backend-dependent; relative comparisons are paired on identical hardware.

\subsection{FineWeb-Edu Results}

\paragraph{Training loss comparison (10k steps).}
Table~\ref{tab:a100-training-summary-10k} summarizes the 10k-step training results. The baseline (standard attention) and decoupled (DBA with $d_{\text{sem}}{=}512$, $d_{\text{geo}}{=}1024$) achieve near-identical final loss (3.36 vs.\ 3.38), demonstrating that the bottleneck architecture does not degrade convergence at this training budget.

\IfFileExists{A100-1b-10k-training_summary.tex}{
  \input{\detokenize{A100-1b-10k-training_summary.tex}}
}{}

\paragraph{Held-out evaluation (pending).}
The benchmark harness defines perplexity evaluation on held-out FineWeb-Edu shards, but results have not yet been generated. The harness will evaluate both the baseline and decoupled checkpoints at 10k steps.

\begin{table}[htbp]
\centering
\small
\caption{Planned held-out perplexity evaluation (pending benchmark execution).}
\label{tab:ppl_pending}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Checkpoint} & \textbf{Status} & \textbf{Perplexity} \\
\midrule
Baseline (10k steps) & Pending & --- \\
Decoupled (10k steps) & Pending & --- \\
\bottomrule
\end{tabular}
\end{table}
\subsection{Behavioral Probes: No Capability Collapse}

To evaluate whether DBA's 6\% perplexity degradation translates to behavioral collapse, we constructed an extended behavioral probe suite spanning 15 cognitive categories with 117 test cases: exact copy tasks, few-shot learning, distractor filtering, logical reasoning, arithmetic, sequence completion, world knowledge, semantic understanding, format preservation, long-context retrieval, robustness to rephrasing, edge cases, nuanced attention patterns, instruction following, and consistency across equivalent prompts.

\paragraph{Key finding: differential degradation, not collapse.}
Table~\ref{tab:behavior_results_100k} presents the 100k-step results. Despite the perplexity gap, behavioral performance is comparable: DBA achieves 27.4\% accuracy vs baseline's 26.5\% across 117 probes. In head-to-head comparison on tests where models diverge, DBA wins 7 to baseline's 6. Category-level analysis shows each architecture winning 2 categories with 11 ties. This suggests the lost capacity is not uniformly task-relevant.

\begin{table}[htbp]
\centering
\small
\caption{Extended behavioral benchmark (100k-step checkpoints, 117 tests across 15 categories). DBA maintains parity with baseline while achieving 37.5\% KV-cache reduction.}
\label{tab:behavior_results_100k}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{DBA (sem8/geo32)} \\
\midrule
Overall accuracy & 26.5\% (31/117) & 27.4\% (32/117) \\
Head-to-head wins & 6 & 7 \\
Category wins & 2 & 2 \\
\midrule
\multicolumn{3}{l}{\textit{Notable differences:}} \\
\quad Reasoning (10 tests, N=10) & 50.0\% & \textbf{70.0\%} \\
\quad World Knowledge (6 tests) & 50.0\% & \textbf{66.7\%} \\
\quad Distractor Tests (8 tests) & \textbf{75.0\%} & 62.5\% \\
\quad Copy Tasks (exact match)$^\dagger$ & \textbf{14.3\%} & 0.0\% \\
\quad Copy Tasks (soft match)$^\dagger$ & 42.9\% & \textbf{71.4\%} \\
\bottomrule
\end{tabular}
\vspace{0.5em}
\footnotesize{$^\dagger$ DBA's 0\% \textit{exact} match on copy tasks is misleading: manual inspection reveals DBA correctly recalls target content (5/7 cases) but fails to terminate generation (e.g., outputting ``1 2 3...15 16 17...'' instead of stopping at 15). \textit{Soft match} checks whether target content appears in output. This suggests a generation termination issue, not a content recall failure.}
\end{table}

\paragraph{Where each model wins.}
Results include both favorable and unfavorable outcomes for DBA:
\begin{itemize}
    \item \textbf{DBA advantages:} Reasoning tasks show a +20\% difference (7/10 vs 5/10), though we note the small sample size means this could be seed variance. World knowledge (+16.7\%) and attention focus under noise also favor DBA. DBA correctly identifies ``APPLE'' when surrounded by ``NOISE'' tokens; baseline attends to the noise.
    \item \textbf{Baseline advantages:} Distractor filtering ($-$12.5\%), exact copy tasks ($-$14.3\%). On copy tasks, DBA tends toward continuation rather than termination (e.g., continuing ``1 2 3 ... 15'' to ``16 17 18...'').
    \item \textbf{Ties (11 categories):} Arithmetic, sequences, format parsing, instructions---both models fail equally on these tasks, reflecting shared limitations at 100k training steps.
\end{itemize}

\paragraph{Qualitative examples: head-to-head differences.}
Table~\ref{tab:behavior_qualitative_100k} shows the 13 tests where exactly one model succeeded. These reveal characteristic differences in failure modes rather than overall capability gaps.

\begin{table}[htbp]
\centering
\footnotesize
\caption{Head-to-head differences (100k steps). Tests where exactly one model passed.}
\label{tab:behavior_qualitative_100k}
\begin{tabular}{@{}lp{4cm}p{4cm}@{}}
\toprule
\textbf{Test} & \textbf{Baseline} & \textbf{DBA} \\
\midrule
\multicolumn{3}{l}{\textit{Only Baseline passed (6 tests):}} \\
copy\_long\_sequence & \cmark~Stops at 15 & \xmark~Continues to 16, 17... \\
distractor\_words & \cmark~``BLUE'' & \xmark~``GREEN'' \\
double\_negation & \cmark~``true'' & \xmark~``false'' \\
analogy\_size & \cmark~``short'' & \xmark~``wide'' \\
multiply\_zero & \cmark~0 & \xmark~1 \\
recent\_vs\_distant & \cmark~``NEW'' & \xmark~``OLD'' \\
\midrule
\multicolumn{3}{l}{\textit{Only DBA passed (7 tests):}} \\
compare\_simple & \xmark~``true'' & \cmark~``false'' \\
compare\_equal & \xmark~``true'' & \cmark~``false'' \\
negation\_simple & \xmark~``true'' & \cmark~``false'' \\
days\_week & \xmark~6 & \cmark~7 \\
antonym\_hot & \xmark~``cool'' & \cmark~``cold'' \\
single\_digit (1+1) & \xmark~1 & \cmark~2 \\
attention\_focus\_noise & \xmark~``NOISE'' & \cmark~``APPLE'' \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Possible explanations.}
The following interpretations are speculative:

\begin{itemize}
    \item \textbf{DBA's reasoning advantage:} DBA wins on simple logical comparisons (``4 > 9: false'') where baseline incorrectly answers ``true''. One possible explanation is that the compressed semantic path forces the model to rely more on learned logical structure rather than surface pattern matching. However, this could also be an artifact of optimization dynamics or random seed effects.

    \item \textbf{Baseline's copy advantage:} Baseline correctly terminates sequences (stops at 15) while DBA continues generating (16, 17...). This may reflect DBA's more diffuse attention making it harder to detect ``end of pattern'' signals. Alternatively, it could simply be a generation hyperparameter issue (temperature, sampling).

    \item \textbf{The attention\_focus\_noise case:} This is perhaps our cleanest mechanistic evidence. When asked to identify ``TARGET: APPLE'' surrounded by ``NOISE NOISE NOISE'', DBA correctly outputs ``APPLE'' while baseline outputs ``NOISE''. This is consistent with (but does not prove) the bandwidth-as-regularization hypothesis.
\end{itemize}

\noindent These are post-hoc interpretations of a 7-6 head-to-head split. The dominant finding is \textbf{parity}, not superiority in either direction.

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.92\textwidth}{\small\raggedright
\textbf{Prompt:}\\
\texttt{Input: 1 2 3. Output: 1 2 3.}\\
\texttt{Input: Red Green Blue. Output: Red Green Blue.}\\
\texttt{Input: A7 B4 C9 D2. Output:}\hfill\textbf{Expected:} \texttt{A7 B4 C9 D2.}\\[0.8em]
\hrule
\vspace{0.5em}
\textbf{Baseline ($d=64$ per head):}\hfill\textcolor{red!70!black}{\textit{Attention Drift}}\\
\texttt{Red Green Blue. Output: Red Green Blue. Output: Red Green...}\\[0.5em]
\hrule
\vspace{0.5em}
\textbf{DBA Extreme ($d=8$ per head):}\hfill\textcolor{green!60!black}{\textit{Correct Recall}}\\
\texttt{Output: A7 B4 C9 D2. Output:}
}}
\caption{Attention drift in action. The baseline loops on distractor context; DBA correctly recalls the target. The bottleneck forces prioritization of the immediate instruction.}
\label{fig:attention_drift}
\end{figure}

\paragraph{Attention analysis (100k checkpoints).}
To examine attention patterns at the 100k checkpoint, we record attention weights for the final query token and compare baseline vs DBA across all 117 behavioral probes. Figures~\ref{fig:attn_focus_noise_last_layer}--\ref{fig:attn_focus_noise_mass} show the \texttt{attention\_focus\_noise} case---where the prompt contains ``NOISE'' tokens surrounding a ``TARGET: APPLE'' signal. This case is particularly revealing: baseline outputs ``NOISE'' while DBA correctly outputs ``APPLE''.

\begin{figure}[htbp]
\centering
\IfFileExists{attn_focus_noise_comparison_last_layer.png}{
  \includegraphics[width=0.95\textwidth]{attn_focus_noise_comparison_last_layer.png}
}{
  \fbox{\parbox{0.92\textwidth}{\small Missing: \texttt{attn\_focus\_noise\_comparison\_last\_layer.png}}}
}
\caption{Last-layer attention heads for \texttt{attention\_focus\_noise} (100k checkpoints). Top row: baseline. Bottom row: DBA. The vertical white line marks the target anchor. \textbf{Key observation:} Baseline heads (top) show diffuse attention spreading into NOISE tokens (left of anchor), while DBA heads (bottom) concentrate attention more tightly around the target region. This visual difference corresponds to the behavioral outcome: baseline outputs ``NOISE'', DBA outputs ``APPLE''.}
\label{fig:attn_focus_noise_last_layer}
\end{figure}

\begin{figure}[htbp]
\centering
\IfFileExists{attn_focus_noise_comparison_heatmap.png}{
  \includegraphics[width=0.95\textwidth]{attn_focus_noise_comparison_heatmap.png}
}{
  \fbox{\parbox{0.92\textwidth}{\small Missing: \texttt{attn\_focus\_noise\_comparison\_heatmap.png}}}
}
\caption{Attention heatmaps across all layers for \texttt{attention\_focus\_noise}. Left: baseline. Right: DBA. Rows are layers (depth); columns are prompt tokens. The vertical white line marks the anchor separating distractor region from target.}
\label{fig:attn_focus_noise}
\end{figure}

\begin{figure}[htbp]
\centering
\IfFileExists{attn_focus_noise_comparison_mass.png}{
  \includegraphics[width=0.95\textwidth]{attn_focus_noise_comparison_mass.png}
}{
  \fbox{\parbox{0.92\textwidth}{\small Missing: \texttt{attn\_focus\_noise\_comparison\_mass.png}}}
}
\caption{Attention mass by layer for \texttt{attention\_focus\_noise}. Blue/orange curves show attention mass on exemplar (distractor) vs target regions for baseline (solid) and DBA (dashed).}
\label{fig:attn_focus_noise_mass}
\end{figure}

\subsection{Downstream Accuracy: 10k-Step Results}

We evaluate all three architectures on standard multiple-choice benchmarks using log-probability scoring. Table~\ref{tab:downstream_10k} presents the results.

\begin{table}[htbp]
\centering
\small
\caption{Downstream accuracy at 10k training steps. Random baseline: Winogrande 50\%, ARC-Easy 25\%. All models are undertrained; differences are small but directionally informative.}
\label{tab:downstream_10k}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Task} & \textbf{Baseline} & \textbf{DBA (16+32)} & \textbf{DBA (8+32)} \\
\midrule
Winogrande & 51.85\% & 50.43\% & \textbf{52.01\%} \\
ARC-Easy & \textbf{48.42\%} & 43.86\% & 45.61\% \\
\midrule
\textit{$\Delta$ vs.\ Baseline} & --- & $-$4.0 avg & $-$1.3 avg \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key observations.}
\begin{itemize}
    \item \textbf{Extreme compression is viable:} The aggressive DBA variant (8+32 dims/head, 37.5\% of baseline attention dimension) achieves 52.01\% on Winogrande---\textit{outperforming} the baseline (51.85\%). This demonstrates that extreme compression does not lobotomize the model.
    \item \textbf{U-curve effect:} The primary DBA variant (16+32) underperforms both baseline and extreme DBA on both tasks. The more aggressive compression appears to provide stronger regularization that compensates for capacity reduction.
    \item \textbf{ARC-Easy gap:} Baseline leads on ARC-Easy (48.42\% vs.\ 45.61\%), a 2.8 percentage point difference. However, at 10k steps all models are undertrained (ARC-Easy random is 25\%), so this gap may close with extended training.
    \item \textbf{All models are near-random on Winogrande:} The $\sim$50\% scores indicate that commonsense reasoning has not yet emerged at 10k steps, regardless of architecture.
\end{itemize}

\paragraph{Interpretation.}
The most important finding is that \textbf{aggressive DBA compression remains viable at 10k steps}. Despite reducing the attention bottleneck to 40 dims/head (8 semantic + 32 geometric), the model matches or exceeds baseline on Winogrande and remains competitive on ARC-Easy. The 37.5\% architectural KV-cache reduction comes without catastrophic capability loss, which motivated proceeding with 100k-step training.

\paragraph{Long-context stability (planned).}
The benchmark harness (\path{research/dba/benchmark.yml}) defines a \texttt{context\_sweep} benchmark that tests chunked prefill and decode-at-context up to 131{,}072 tokens. This benchmark will measure:
\begin{itemize}
    \item Total prefill time across chunk boundaries
    \item Decode-at-context latency (ms/token)
    \item Last-chunk perplexity (noting RoPE extrapolation beyond 2k training context)
\end{itemize}
Results are pending execution of the benchmark harness on the finalized checkpoints.

\begin{figure}[htbp]
\centering
\IfFileExists{compare_context_decode_tok_per_sec.png}{
  \includegraphics[width=0.9\textwidth]{\detokenize{compare_context_decode_tok_per_sec.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Pending figure:} \texttt{compare\_context\_decode\_tok\_per\_sec.png}.\\
  Generated by the benchmark harness (\path{research/dba/benchmark.yml}).}}
}
\caption{Planned: cached decode throughput (tokens/sec) vs.\ context length using chunked prefill + cached decode.}
\label{fig:compare_context_decode_tok_per_sec}
\end{figure}

\subsection{Local Suite: Multi-Architecture Comparison (Planned)}

The local suite (\path{config/presets/dba_paper_local.yml}) provides broader architectural comparisons on a 12-layer model ($\sim$550M params) with 3 seeds for statistical significance:

\begin{table}[htbp]
\centering
\small
\caption{Local suite: architecture comparison (pending execution).}
\label{tab:local_arch_compare}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Architecture} & \textbf{Loss (mean $\pm$ std)} & \textbf{PPL} & \textbf{KV bytes/token} \\
\midrule
Baseline (standard) & --- & --- & 4096 \\
Bottleneck & --- & --- & 3072 \\
Decoupled (DBA) & --- & --- & 3072 \\
GQA (32Q/4KV) & --- & --- & 512 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{DBA Design Ablations (Planned)}

The following DBA ablations are defined in the local manifest and will isolate the contribution of each design choice:

\begin{table}[htbp]
\centering
\small
\caption{DBA ablations (pending execution).}
\label{tab:dba_ablations}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Ablation} & \textbf{null\_attn} & \textbf{tie\_qk} & \textbf{gate} & \textbf{RoPE} & \textbf{Loss} \\
\midrule
Decoupled (default) & false & false & false & geo only & --- \\
+ Null token & true & false & false & geo only & --- \\
+ Tied Q-K & false & true & false & geo only & --- \\
+ Gate & false & false & true & geo only & --- \\
-- RoPE (none) & false & false & false & none & --- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{LR Sensitivity (Planned)}

We sweep learning rate for the baseline to establish fair comparison bounds:

\begin{table}[htbp]
\centering
\small
\caption{Learning rate sensitivity (baseline, seed 1337).}
\label{tab:lr_sweep}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Learning Rate} & \textbf{Final Loss} & \textbf{PPL} \\
\midrule
$2 \times 10^{-4}$ & --- & --- \\
$3 \times 10^{-4}$ (default) & --- & --- \\
$4 \times 10^{-4}$ & --- & --- \\
\bottomrule
\end{tabular}
\end{table}

\noindent Note: Decoupled runs use $\text{lr}=2 \times 10^{-4}$ (lower than baseline) with \texttt{grad\_clip\_norm=1.0} to prevent loss spikes after warmup.

\subsection{Inference Benchmarks (Planned)}

The following benchmarks are defined in \path{research/dba/benchmark.yml}:
\begin{itemize}
    \item \textbf{Held-out perplexity:} \texttt{ppl\_fineweb\_100m} (50--200 batches) comparing baseline vs.\ DBA.
    \item \textbf{Downstream accuracy:} \texttt{downstream\_accuracy} for HellaSwag, Winogrande, ARC-Easy.
    \item \textbf{Latency:} \texttt{latency\_cached} measuring decode throughput at prompt lengths 128--8192.
    \item \textbf{Memory:} \texttt{memory\_kv} measuring KV-cache footprint at sequence lengths 512--16384.
    \item \textbf{Behavioral probes:} \texttt{behavior\_sanity} using 22 test cases (copy tasks, arithmetic, passkey retrieval).
    \item \textbf{Context sweep:} \texttt{context\_sweep} testing chunked prefill + decode up to 131k tokens.
\end{itemize}

\subsection{Memory--Quality Trade-off (Pending)}

We will report a Pareto-style comparison (perplexity vs.\ KV-cache footprint and decode throughput).

\IfFileExists{table_scale.tex}{
  \input{\detokenize{table_scale.tex}}
}{
  % Optional: generate via generate_paper_figures.py --paper-dir paper
}

\begin{figure}[htbp]
\centering
\IfFileExists{pareto_curve.png}{
  \includegraphics[width=0.85\textwidth]{\detokenize{pareto_curve.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Pending figure:} \texttt{pareto\_curve.png}.\\
  Will be generated from \texttt{mem128k.json} + training logs via \path{generate_paper_figures.py}.}}
}
\caption{Planned Pareto curve: quality (perplexity) vs.\ efficiency (KV-cache bytes/token and decode throughput).}
\label{fig:pareto}
\end{figure}

\subsection{Memory Footprint Analysis}

Table~\ref{tab:memory} gives an illustrative KV-cache scaling projection for a 128k context in a Llama-like configuration (32 layers, $d_{\text{model}} = 4096$). We intentionally \emph{do not} foreground optimistic fixed-rank ``upper bound'' numbers here; the experimentally grounded takeaway is the linear dependence on the interaction dimension and the fact that architectural reduction composes multiplicatively with KV-cache quantization. End-to-end device memory deltas at 128k are planned and will be reported in a future revision.

\begin{table}[htbp]
\centering
\small
\caption{KV-Cache Memory for 128k Context (Llama-like scale; projected)}
\label{tab:memory}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Architecture} & \textbf{VRAM} & \textbf{Compression} \\
\midrule
Standard (FP16) & 64.0 GB & 1$\times$ \\
GQA (32Q/4KV; FP16) & 8.0 GB & 8$\times$ \\
GQA (32Q/4KV; Q4, ideal) & 2.0 GB & 32$\times$ \\
MLA (FP16) & 4.3 GB & 15$\times$ \\
Bottleneck (FP16) & 1.5 GB & 43$\times$ \\
Decoupled (Q4, constant-fraction $d_{\text{attn}}{=}768$) & 3.0 GB & 21$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\IfFileExists{memory_footprint.png}{
  \includegraphics[width=0.85\textwidth]{\detokenize{memory_footprint.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Pending figure:} \texttt{memory\_footprint.png}.\\
  Generated from \texttt{mem128k.json} via \path{generate_paper_figures.py}.}}
}
\caption{KV-cache memory comparison at long context (illustrative projection).}
\label{fig:memory}
\end{figure}

% ============================================================================
% 4. DISCUSSION
% ============================================================================
\section{Discussion}

\subsection{Why Does Low-Rank Attention Work?}

We hypothesize two complementary explanations:

\paragraph{Intrinsic Dimensionality.}
Following Aghajanyan et al. \cite{aghajanyan2021intrinsic}, natural language representations lie on low-dimensional manifolds. The attention mechanism's role is \textit{routing}---selecting which tokens to aggregate---not computing complex transformations. Routing decisions are inherently low-entropy and thus low-rank.

\paragraph{Regularization Effect.}
The bottleneck may act as an implicit regularizer by reducing the capacity of token--token interactions. In the 10k-step comparison (Table~\ref{tab:a100-training-summary-10k}), baseline achieves slightly lower loss (3.36 vs.\ 3.38), suggesting DBA's primary benefit is efficiency rather than perplexity improvement.

\paragraph{The Perplexity--Capability Dissociation.}
The 6\% perplexity gap (13.53 vs 12.76) warrants explanation. Perplexity measures the ability to predict \textit{every} token, including noise and distractors in context. Figure~\ref{fig:attn_focus_noise_last_layer} shows that baseline attention spreads into distractor regions---effectively ``memorizing'' local noise patterns---which improves next-token prediction on noisy sequences. DBA's bottleneck \textit{cannot} attend to this noise with the same fidelity, raising perplexity but improving signal-to-noise ratio for the target task. This interpretation is consistent with DBA's advantage on the \texttt{attention\_focus\_noise} behavioral probe: the model that is ``worse'' at predicting noise is better at ignoring it.

\paragraph{Gradient Rank Dynamics.}
AdaRankGrad \cite{refael2024adarankgrad} proves that gradient rank decreases monotonically during training, eventually approaching rank one. This suggests that \textit{architectural} bottlenecks become increasingly appropriate as training progresses---the model naturally ``wants'' to operate in a low-rank subspace. By hard-wiring this constraint from the start, we may accelerate convergence by matching the architecture to the optimization landscape.

\subsection{When to Use Each Architecture}

Our experiments are organized into a local suite (FineWeb-Edu 100M) for broader comparisons and a scale suite (FineWeb-Edu 20B tokens) for confirmation.

\begin{itemize}
    \item \textbf{Decoupled Bottleneck:} On FineWeb-Edu, the decoupled bottleneck is a strong default that preserves the KV memory benefits of low-rank attention while enabling \textbf{heterogeneous quantization} (e.g., Q4 semantic, Q8 geometric).
    \item \textbf{Standard Attention:} A strong baseline and simplest implementation, but can be memory-inefficient for long contexts.
\end{itemize}

\paragraph{Recommendation.}
For \textit{training}, iterate on the local FineWeb-Edu suite and validate at scale with the A100 suite. For \textit{inference} under memory constraints, use Decoupled with heterogeneous quantization (aggressively compress semantic, preserve geometric fidelity).

\paragraph{Flash/SDPA compatibility.}
Decoupled Bottleneck Attention can be implemented using PyTorch's fused \texttt{scaled\_dot\_product\_attention} by concatenating the scaled semantic and geometric Q/K projections along the head dimension, making it compatible with modern Flash Attention kernels.

\subsection{Limitations}

\begin{itemize}
    \item \textbf{Quality--efficiency trade-off is real:} The 6\% perplexity gap (13.53 vs 12.76) is a genuine cost, not measurement noise. Applications requiring minimal quality degradation should not use DBA.
    \item \textbf{Known failure mode---sequence termination:} DBA shows degraded performance on exact-copy tasks requiring precise termination (0\% exact match vs 14.3\% baseline). The model recalls content correctly but fails to stop generating. This may be unacceptable for applications requiring exact symbolic reproduction.
    \item \textbf{Single seed at scale:} The 100k-step 1B results use seed 42 only. Variance across seeds is not characterized at this scale.
    \item \textbf{Long-context stability not validated:} Training used 2k context; behavior beyond this (especially with RoPE extrapolation) is not measured.
    \item \textbf{Mechanism not established:} The ``bandwidth-as-regularization'' hypothesis is consistent with but not proven by the attention visualizations. We show correlation, not causation.
    \item \textbf{Effective rank not yet measured:} Appendix~\ref{app:effective_rank} reserves space for empirical rank measurements that would strengthen the redundancy hypothesis.
    \item \textbf{Scope:} DBA targets autoregressive LMs; other settings (dense retrieval, multimodal alignment, algorithmic tasks) may require higher-rank routing.
\end{itemize}

% ============================================================================
% 5. CONCLUSION
% ============================================================================
\section{Conclusion}

\subsection{What We Demonstrated}

We trained 1B-parameter models for 100k steps and measured:
\begin{enumerate}
    \item \textbf{Efficiency gains:} 37.5\% KV-cache reduction (112,640 vs 180,224 bytes/token), 12\% faster cached decode (85.8 vs 76.9 tok/s). Measured using custom Metal kernels on Apple Silicon, fp16, batch=1.
    \item \textbf{Quality cost:} 6\% held-out perplexity increase (13.53 vs 12.76 PPL). Training loss converges similarly (2.68 vs 2.67), but evaluation reveals the gap.
    \item \textbf{No capability collapse:} On 117 behavioral probes, DBA scores 27.4\% vs baseline's 26.5\%. Head-to-head: 7-6 DBA. Category wins: 2-2 tie. Downstream accuracy on Winogrande/ARC-Easy remains competitive.
\end{enumerate}

\subsection{What We Did Not Demonstrate}

Several questions remain open:
\begin{itemize}
    \item We did not establish a causal mechanism for the behavioral differences. The ``bandwidth-as-regularization'' hypothesis is speculation.
    \item We tested at 1B scale only. Scaling behavior to larger models is unknown.
    \item Both models fail on arithmetic, sequence completion, and instruction following. DBA is not a capability improvement.
    \item Long-context behavior beyond 2k training context was not validated.
\end{itemize}

\subsection{Summary}

\textbf{DBA trades 6\% perplexity for 1.6$\times$ memory reduction and 1.12$\times$ speedup.} This is an explicit efficiency/quality tradeoff, not free compression. The behavioral benchmark suggests the lost capacity is not uniformly task-relevant: attention behavior and downstream accuracy do not degrade proportionally to perplexity.

\paragraph{Independence from mechanistic claims.}
Even if the proposed ``bandwidth-as-regularization'' interpretation is incorrect, the empirical finding stands: large portions of semantic attention bandwidth can be removed while preserving usable behavior, at predictable and measured efficiency gains. The paper's contribution does not depend on the mechanistic hypothesis being true.

\paragraph{Composability.}
DBA is orthogonal to KV-sharing methods (GQA/MQA) and KV-cache quantization. It reduces the \textit{interaction dimension} used in attention scoring, while GQA reduces \textit{head count} and quantization reduces \textit{precision}. These can be composed to move further along the efficiency--quality Pareto frontier.

\paragraph{Future Work.}
(1) Effective-rank measurements on baseline vs DBA activations (Appendix~\ref{app:effective_rank}); (2) long-context sweeps to 128k tokens; (3) scaling experiments at 7B+ parameters; (4) ablations isolating semantic/geometric path contributions; (5) heterogeneous KV-cache quantization (Q4 semantic, Q8 geometric).

% ============================================================================
% STATEMENTS
% ============================================================================
\section*{Statements and Declarations}

\paragraph{Conflict of Interest.}
The author declares no competing interests. This research was conducted independently without corporate affiliation or funding from entities with financial interests in the outcomes.

\paragraph{Data Availability.}
All datasets used in this study are publicly available: FineWeb-Edu is available from Hugging Face. The code, trained model checkpoints, and all experimental logs are available at \url{https://github.com/theapemachine/caramba}. Manifest presets are in \path{config/presets/dba_paper_rerun*.yml}; benchmark definitions in \path{research/dba/benchmark*.yml}.

\paragraph{Funding.}
This research was conducted without external funding. All computational resources were provided by the author.

% ============================================================================
% APPENDIX
% ============================================================================
\appendix

\section{Pending: Effective Rank Evidence}
\label{app:effective_rank}

This appendix reserves space for empirical effective-rank measurements of Q/K projection activations (singular value spectra and entropy effective rank) for the \texttt{paper\_baseline} and \texttt{paper\_decoupled} checkpoints. These results will be generated from the production checkpoints and copied into the paper directory for arXiv.

\section{Decoupled Ablations}
\label{app:decoupled_ablations}

This appendix presents detailed results for DBA design ablations, run via \path{config/presets/dba_paper_local.yml}:

\begin{itemize}
    \item \textbf{Null token} (\texttt{null\_attn=true}): Provides an explicit ``attend nowhere'' option. May stabilize training at very low ranks.
    \item \textbf{Tied Q-K} (\texttt{tie\_qk=true}): Forces symmetric attention ($W_Q = W_K$ for semantic path). Reduces parameters but may limit expressivity.
    \item \textbf{Gating} (\texttt{decoupled\_gate=true}): Enables a learnable per-head mixing gate between semantic and geometric routing paths. Treated as a distinct variant from the ungated DBA used in the primary comparison.
    \item \textbf{No RoPE} (\texttt{rope\_enabled=false}): Removes positional encoding entirely. Expected to degrade significantly on position-sensitive tasks.
\end{itemize}

\noindent Full training curves and behavioral probe results will be included once the local suite completes.

\section{Behavioral Probe Cases (Per-Case Outputs)}
\label{app:behavior_cases}

The per-case table below shows behavioral probes (teacher vs.\ student pass/fail, plus a truncated preview of each model's output), generated automatically by the benchmark harness.

\IfFileExists{behavior_cases_table.tex}{
  \input{\detokenize{behavior_cases_table.tex}}
}{}

\section{Long-Context Sweep (Up to 131{,}072 Tokens)}
\label{app:long_context_sweep}

The \texttt{context\_sweep} benchmark (\path{research/dba/benchmark.yml}) will test chunked prefill and decode-at-context for the paired checkpoints. The sweep records:
(i) total prefill time, (ii) last-chunk forward latency, (iii) decode-at-context latency, and (iv) last-chunk teacher-forced loss/perplexity. Note that loss/perplexity at long context reflects RoPE extrapolation beyond the 2k training context, not long-context capability.

\begin{table}[htbp]
\centering
\small
\caption{Context sweep results (pending benchmark execution).}
\label{tab:context_sweep}
\begin{tabular}{@{}rcccc@{}}
\toprule
\textbf{Context} & \textbf{Prefill (s)} & \textbf{Decode 1 tok (ms)} & \textbf{PPL (last chunk)} & \textbf{OK} \\
\midrule
2{,}048 & --- & --- & --- & --- \\
4{,}096 & --- & --- & --- & --- \\
8{,}192 & --- & --- & --- & --- \\
16{,}384 & --- & --- & --- & --- \\
32{,}768 & --- & --- & --- & --- \\
65{,}536 & --- & --- & --- & --- \\
98{,}304 & --- & --- & --- & --- \\
131{,}072 & --- & --- & --- & --- \\
\bottomrule
\end{tabular}
\end{table}

\noindent\textit{Note: The benchmark harness defines context lengths \{2048, 4096, 8192, 16384, 32768, 65536, 98304, 131072\} with chunk\_size=1024 and decode\_len=128. Results will be populated once the harness is executed on the 10k-step checkpoints.}

\begin{figure}[htbp]
\centering
\IfFileExists{context_decode_one_ms.png}{
  \includegraphics[width=0.9\textwidth]{\detokenize{context_decode_one_ms.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Pending figure:} \texttt{context\_decode\_one\_ms.png}.\\
  Will be generated by \texttt{context\_sweep} benchmark once executed.}}
}
\caption{Planned: Decode-at-context cost (ms/token) vs.\ context length.}
\label{fig:context_decode_one_ms}
\end{figure}

\begin{figure}[htbp]
\centering
\IfFileExists{latency_tokens_per_sec.png}{
  \includegraphics[width=0.9\textwidth]{\detokenize{latency_tokens_per_sec.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Pending figure:} \texttt{latency\_tokens\_per\_sec.png}.\\
  Will be generated by \texttt{latency\_cached} benchmark once executed.}}
}
\caption{Planned: Cached decode throughput microbenchmark (batch=1).}
\label{fig:latency_tokens_per_sec}
\end{figure}

\begin{figure}[htbp]
\centering
\IfFileExists{perplexity.png}{
  \includegraphics[width=0.85\textwidth]{\detokenize{perplexity.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Pending figure:} \texttt{perplexity.png}.\\
  Will be generated by \texttt{ppl\_fineweb\_100m} benchmark once executed.}}
}
\caption{Planned: Held-out perplexity on FineWeb-Edu token shards.}
\label{fig:perplexity_plot}
\end{figure}

% ============================================================================
% REFERENCES
% ============================================================================
\FloatBarrier
\clearpage
\bibliographystyle{plain}
\bibliography{references}

\end{document}
