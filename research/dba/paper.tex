\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\graphicspath{{figs/attention/}{research/dba/figs/attention/}{figs/}{research/dba/figs/}}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{microtype}  % Better text flow, reduces overfull hbox
\usepackage[margin=1in]{geometry}  % Consistent margins
\usepackage{placeins} % Provides \FloatBarrier to prevent float reordering across sections

% Custom colors
\definecolor{accent}{RGB}{45, 106, 79}

% Checkmark and xmark symbols for tables
\newcommand{\cmark}{\textcolor{green!60!black}{\checkmark}}
\newcommand{\xmark}{\textcolor{red!70!black}{$\times$}}

\title{\textbf{Decoupled Bottleneck Attention:} \\[0.3em]
\large Low-Rank Semantic Routing as Structural Regularization}

\author{
  Daniel Owen van Dommelen\\
  \textit{Independent Research}\\
  \texttt{theapemachine@gmail.com}
}

\date{January 2026}

\begin{document}

\maketitle

% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
Standard Multi-Head Attention suffers from \textit{attention drift}: high-dimensional heads overfit to spurious context correlations, causing repetition loops. We propose \textbf{Decoupled Bottleneck Attention (DBA)}, separating \textit{semantic routing} from \textit{positional geometry} and constraining the semantic path to low rank ($d_{\text{sem}}=8$ per head). This bottleneck forces the model to discard distractors and focus on the immediate instruction---a \textbf{structural regularizer} against context hallucination.

We train 1B-parameter models on FineWeb-Edu and compare standard attention ($d=64$ per head) against DBA variants. At 10k steps, baseline achieves loss 3.36 while DBA achieves 3.38---near-parity in convergence. DBA reduces training memory by 27\% and maintains 32\% higher throughput after warmup. Critically, behavioral probes reveal that the baseline model enters pathological repetition loops on in-context copy tasks (outputting ``Red Green Blue...'' when asked to copy ``A7 B4 C9 D2''), while the aggressively compressed DBA variant \textbf{correctly recalls the target content}.

We argue that, for autoregressive language modeling, \textit{high-rank semantic routing is often unnecessary and can be harmful}: excessive bandwidth can encourage spurious context matching, distracting the model from the current instruction. By bottlenecking only the semantic path while preserving positional geometry, DBA reframes efficient attention not merely as a memory technique, but as an architectural bias that can improve behavioral reliability.
\end{abstract}

\paragraph{Keywords:}
Transformer, attention mechanism, low-rank, attention drift, context hallucination, structural regularization, KV-cache, memory efficiency, robustness

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}

Modern Transformer architectures \cite{vaswani2017attention} achieve remarkable performance across language modeling, translation, and reasoning tasks. However, in autoregressive language modeling we argue that \textbf{high-rank semantic routing is often unnecessary and can be harmful}: allocating large bandwidth to token--token matching can encourage the model to attend broadly to context patterns rather than the immediate instruction.

When a 64-dimensional attention head looks back at context, it can see \textit{everything} with high fidelity. This bandwidth can allow the model to attend to irrelevant tokens---few-shot examples, repeated patterns, distractor content---causing it to \textit{overfit to context noise} rather than focus on the current input. We call this observed failure mode \textbf{attention drift}. Our experiments provide evidence consistent with a bandwidth/regularization mechanism, but we treat the causal story as a hypothesis rather than a settled fact.

\subsection{The Bottleneck as Regularizer}

We propose a simple architectural fix: force attention through a narrow bottleneck. By constraining the semantic routing path to extremely low rank ($d_{\text{sem}}{=}8$ dims/head), the model physically \textit{cannot} hold all context information simultaneously. It must prioritize. The most relevant signal survives the bottleneck; the noise gets filtered out.

This reframes efficient attention not merely as a memory optimization, but as a \textbf{structural regularizer} that prevents attention drift. We build a hardware-level ``distractor filter.''

\subsection{The Redundancy Hypothesis}

Our approach is motivated by a simple observation: in a 512-dimensional layer, the neurons are not independent. They move in \textit{sympathetic clusters}---correlated groups that reduce the intrinsic dimensionality of the representation. Prior work on LoRA \cite{hu2021lora} demonstrated that weight \textit{updates} during fine-tuning are low-rank (typically $r \leq 64$). Recent work on gradient dynamics \cite{refael2024adarankgrad} shows that optimization naturally collapses to low rank. We extend this observation to argue that the \textit{architecture itself}---specifically the attention mechanism---should be structurally constrained to match this intrinsic rank.

Motivated by the redundancy hypothesis, we expect the \textit{Q/K projection activations} feeding attention to have low effective rank. We reserve Appendix~\ref{app:effective_rank} for empirical effective-rank measurements on the final production checkpoints.

\subsection{Comparison with Existing Approaches}

\paragraph{Grouped-Query Attention (GQA).}
While Grouped-Query Attention \cite{ainslie2023gqa} successfully reduces KV-cache memory by sharing key-value heads across multiple query heads, it maintains the full computational cost of the query projection and attention scoring in the high-dimensional space. Each query still operates in $\mathbb{R}^{d}$, and every attention score still requires a $d$-dimensional dot product---GQA merely amortizes the \textit{storage} cost, not the \textit{interaction} cost.

Our Bottleneck approach reduces both memory \textit{and} compute by compressing the interaction manifold. Rather than sharing high-dimensional KV pairs, we project queries and keys into a low-rank semantic subspace ($r \ll d$) \textit{before} computing attention, reducing dot-product complexity from $O(n^2 d)$ to $O(n^2 r)$.

\paragraph{Multi-Head Latent Attention (MLA).}
DeepSeek-V2 \cite{deepseek2024v2} introduced MLA, which compresses KV storage into a latent vector, achieving 93\% cache reduction. However, MLA \textit{up-projects} during the forward pass to perform attention in the original high-dimensional space. Our method remains low-rank throughout, saving both memory and compute.

\paragraph{Summary (for skimmers).}
\textbf{GQA} shares KV \emph{storage}; \textbf{MLA} compresses KV \emph{storage} but expands for scoring; \textbf{DBA} compresses the \emph{interaction dimension} used to compute attention scores, while preserving a higher-fidelity geometric (RoPE) path.

\paragraph{Disentangled Attention.}
DeBERTa \cite{he2020deberta} pioneered the separation of content and position representations in attention scoring. We adopt this disentanglement principle but leverage it for \textit{efficiency}: applying aggressive compression to the semantic (content) path while preserving fidelity in the geometric (position) path.

\subsection{Contributions}

\begin{enumerate}
    \item We characterize \textbf{attention drift} on behavioral probes: the baseline model enters pathological repetition loops on in-context copy tasks, outputting ``Red Green Blue...'' when asked to copy ``A7 B4 C9 D2'' (Table~\ref{tab:behavior_qualitative}). This is consistent with over-attending to context patterns rather than the immediate input.
    \item We propose \textbf{Decoupled Bottleneck Attention (DBA)} as a structural change, separating semantic routing ($d_{\text{sem}}{=}8$ dims/head) from positional geometry ($d_{\text{geo}}{=}32$ dims/head). At 10k steps, the aggressive bottleneck \textbf{removes repetition loops on our probe suite} and correctly recalls target content.
    \item We provide early evidence that DBA can be \textbf{behaviorally more reliable}: the aggressively compressed variant outperforms both baseline and moderate compression on behavioral probes (Table~\ref{tab:behavior_results}). Downstream accuracy remains preliminary at 10k steps (Table~\ref{tab:downstream_10k}).
    \item We provide efficiency gains as a \textit{bonus}: \textbf{27\% memory reduction} and \textbf{32\% throughput improvement} for the aggressive variant (Figures~\ref{fig:gpu_memory}--\ref{fig:tok_s}), with near loss-parity (3.36 vs.\ 3.38 at 10k steps).
    \item We define a reproducible evaluation harness (\path{research/dba/benchmark.yml}) for behavioral probes, downstream accuracy, latency, and long-context sweeps. Extended 100k-step runs are in progress.
\end{enumerate}

% ============================================================================
% RELATED WORK
% ============================================================================
\section{Related Work}
\label{sec:related_work}

\paragraph{Low-rank and approximate attention.}
A long line of work seeks to reduce the quadratic cost of attention by approximating the score computation or constraining its rank. Linformer \cite{wang2020linformer} projects keys and values into a lower-dimensional subspace along the sequence dimension, yielding linear-time attention under a low-rank assumption. Kernel and hashing methods such as Performer \cite{choromanski2021performer} and Reformer \cite{kitaev2020reformer} similarly reduce attention cost via randomized features or locality-sensitive hashing. Our setting is different: we train standard causal LMs, but explicitly reduce the query/key interaction dimension inside each layer, targeting both compute (\(O(n^2 r)\)) and KV-cache memory (\(O(n r)\)).

\paragraph{Sparse/local attention for long documents.}
Sparse patterns (e.g., sliding window with global tokens) as in Longformer \cite{beltagy2020longformer} and BigBird \cite{zaheer2020bigbird} reduce attention compute while retaining access to distant context. However, for autoregressive decoding these methods still accumulate a KV cache whose size grows linearly with context length. Our work instead reduces the per-token cache footprint, which is complementary to sparse attention and other long-context strategies \cite{huang2023longcontextsurvey}.

\paragraph{KV-cache optimization.}
Sharing KV heads reduces cache storage by amortizing keys and values across query heads (MQA/GQA) \cite{shazeer2019mqa,ainslie2023gqa}. Latent KV schemes such as MLA compress the cache into a lower-dimensional latent that is expanded during attention \cite{deepseek2024v2}. Orthogonally, quantizing the KV cache reduces memory at fixed architecture \cite{hooper2024kvquant,li2025commvq}. Our decoupled bottleneck reduces the interaction dimension before scoring (saving compute) and also makes heterogeneous KV quantization natural: semantic keys can often be quantized more aggressively than geometric keys.

\paragraph{Expressiveness limits and structured alternatives.}
Reducing interaction rank too far can harm representation power: theory and empirical evidence show regimes where increasing heads under fixed head dimension does not recover lost capacity \cite{bhojanapalli2020lowrank,amsel2025qualityheads}. Recent structured-matrix formulations aim to increase effective rank without full cost by parameterizing attention maps with richer structured operators \cite{kuang2025structuredmatrices}. Decoupling is a simple architectural compromise: we keep a higher-dimensional geometric path (with RoPE) while aggressively compressing only the semantic routing path.

% ============================================================================
% 2. METHODOLOGY
% ============================================================================
\section{Methodology}

\subsection{Standard Multi-Head Attention}

In standard scaled dot-product attention with $H$ heads:
\begin{equation}
    \text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
\end{equation}
where $Q, K, V \in \mathbb{R}^{n \times d}$ are obtained by linear projection from the input $X \in \mathbb{R}^{n \times d_{\text{model}}}$:
\begin{equation}
    Q = XW_Q, \quad K = XW_K, \quad V = XW_V
\end{equation}
with $W_Q, W_K, W_V \in \mathbb{R}^{d_{\text{model}} \times d}$. For language modeling with context length $n$ and dimension $d$, the KV-cache requires $O(2 \cdot L \cdot n \cdot d)$ memory, where $L$ is the number of layers.

\subsection{Bottleneck Attention}

We introduce a simple modification: project $Q$ and $K$ to a lower-dimensional space \textit{before} computing attention scores.\footnote{Our use of ``bottleneck'' refers to dimensionality reduction in the query/key space, distinct from Park et al.'s BAM \cite{park2018bam}, which applies channel and spatial attention in CNNs for computer vision.}
\begin{equation}
    Q' = XW_Q', \quad K' = XW_K'
\end{equation}
where $W_Q', W_K' \in \mathbb{R}^{d_{\text{model}} \times d_{\text{attn}}}$ with $d_{\text{attn}} \ll d_{\text{model}}$. The attention computation becomes:
\begin{equation}
    \text{Attn}_{\text{bottleneck}}(Q', K', V') = \text{softmax}\left(\frac{Q'K'^\top}{\sqrt{d_{\text{attn}}/H}}\right) V'
\end{equation}

This reduces the dot-product complexity from $O(n^2 \cdot d_{\text{model}})$ to $O(n^2 \cdot d_{\text{attn}})$ and the KV-cache from $O(n \cdot d_{\text{model}})$ to $O(n \cdot d_{\text{attn}})$.

\subsection{Decoupled Bottleneck Attention}

The key insight motivating decoupling is that \textit{semantic matching} (``is this token semantically related?'') and \textit{geometric positioning} (``how far away is this token?'') have different intrinsic dimensionality requirements.

We decompose the attention score into two additive components:
\begin{equation}
    \text{Score} = \underbrace{\frac{Q_{\text{sem}} K_{\text{sem}}^\top}{\sqrt{d_{\text{sem}}/H}}}_{\text{Semantic Path}} + \underbrace{\frac{Q_{\text{geo}} K_{\text{geo}}^\top}{\sqrt{d_{\text{geo}}/H}}}_{\text{Geometric Path}}
\end{equation}

where:
\begin{align}
    Q_{\text{sem}} &= XW_{Q,\text{sem}}, \quad K_{\text{sem}} = XW_{K,\text{sem}} \quad &(d_{\text{sem}} = 512 \text{ in primary config}) \\
    Q_{\text{geo}} &= XW_{Q,\text{geo}}, \quad K_{\text{geo}} = XW_{K,\text{geo}} \quad &(d_{\text{geo}} = 1024 \text{ in primary config})
\end{align}

Critically, we apply \textbf{Rotary Position Embeddings (RoPE)} \cite{su2021roformer} \textit{only} to the geometric path:
\begin{equation}
    Q_{\text{geo}}, K_{\text{geo}} \leftarrow \text{RoPE}(Q_{\text{geo}}, K_{\text{geo}}, \text{position})
\end{equation}

The semantic path operates on pure content similarity (controlled by \texttt{rope\_semantic=false} in the manifest), while the geometric path encodes positional relationships. The value projection uses the combined dimension:
\begin{equation}
    V = XW_V, \quad W_V \in \mathbb{R}^{d_{\text{model}} \times d_{\text{attn}}}
\end{equation}
where $d_{\text{attn}} = d_{\text{sem}} + d_{\text{geo}} = 1536$ in our primary configuration. With $H{=}32$ heads, this yields 16 dims/head for semantic and 32 dims/head for geometric routing, preserving higher fidelity for position-dependent RoPE interactions.

\paragraph{Optional semantic/geometric gating (not used in primary experiments).}
In addition to the additive score composition above, our implementation optionally enables a learnable per-head gate that rescales the semantic vs.\ geometric query streams before scoring. In this draft, unless explicitly labeled otherwise, all reported DBA results use the \emph{ungated} variant (\texttt{decoupled\_gate=false}) to keep the primary comparison focused on the bottleneck/decoupling itself.

\subsection{Optional Null Token (stability at extreme rank)}

Low-rank attention can become unstable when queries lack semantically appropriate keys. We introduce a learnable \textbf{null token} $k_\emptyset$ providing an explicit ``attend nowhere'' option:
\begin{equation}
    \text{Score}_{\text{null}} = \frac{Q_{\text{sem}} k_{\emptyset,\text{sem}}^\top}{\sqrt{d_{\text{sem}}/H}} + \frac{Q_{\text{geo}} k_{\emptyset,\text{geo}}^\top}{\sqrt{d_{\text{geo}}/H}}
\end{equation}

This score is concatenated to the attention matrix before softmax, allowing the model to ``dump'' attention mass when no key is appropriate, which can stabilize training at very low ranks. In our flagship decoupled preset, the null token is disabled by default and treated as an ablation (Appendix~\ref{app:decoupled_ablations}).

\subsection{Tied Q-K Projections}

For the semantic path, we optionally \textbf{tie} the query and key projections: $W_{Q,\text{sem}} = W_{K,\text{sem}}$. This enforces symmetric similarity (``A attends to B iff B attends to A''), which is appropriate for content matching but not for position-dependent relationships.

\subsection{Quantized Inference}

For inference, we apply aggressive quantization to the KV-cache. Recent work has demonstrated that 4-bit KV cache quantization preserves model quality remarkably well. Turboderp's ExLlamaV2 implementation \cite{turboderp2024qcache} showed Q4 cache performs comparably to FP16, and this capability has been integrated into production inference engines like llama.cpp \cite{llamacpp2024kvcache}. We implement block-wise Q4\_0 quantization following this approach:
\begin{equation}
    x_{\text{quantized}} = \text{round}\left(\frac{x}{\text{scale}}\right), \quad \text{scale} = \frac{\max(|x_{\text{block}}|)}{7}
\end{equation}
where each block of 32 elements shares a single FP16 scale factor. In the idealized limit (ignoring scale metadata) 4-bit values correspond to 0.5 bytes/value. With Q4\_0 block scales, the effective bytes/value is slightly larger (18 bytes per 32 values $\Rightarrow$ 0.5625 bytes/value), so the ideal 4$\times$ factor becomes $\approx 3.56\times$ in practice. Combined with dimension reduction, the per-layer KV-cache reduction is approximately:
\begin{equation}
    \text{Compression} \approx \underbrace{\frac{d_{\text{model}}}{d_{\text{attn}}}}_{\text{Dimension}} \times \underbrace{\frac{2~\text{bytes}}{0.5625~\text{bytes}}}_{\text{Q4\_0 (incl.\ scale)}} \;\;\approx\;\; \frac{d_{\text{model}}}{d_{\text{attn}}}\times 3.56.
\end{equation}
For a representative setting with $d_{\text{model}}/d_{\text{attn}} \approx 1.33$ (e.g., $2048\!\rightarrow\!1536$), homogeneous Q4\_0 KV-cache quantization implies an implementation-aligned compression of $\approx 1.33\times 3.56 \approx 4.7\times$ versus a standard FP16 baseline (before accounting for any heterogeneous policy choices).

\paragraph{Scaling arithmetic (context only; not validated at scale).}
The KV-cache memory at long context depends on the choice of attention dimension $d_{\text{attn}}$ at scale. For a rough Llama-like configuration (32 layers, $d_{\text{model}}=4096$, 128k context, batch=1), the FP16 KV cache is:
\[
M_{\text{FP16}} \approx 2 \cdot 32 \cdot 4096 \cdot 128\text{k} \cdot 2 \text{ bytes} \approx 64~\text{GiB}.
\]
With 4-bit KV-cache quantization (idealized 0.5 bytes/value), the memory becomes:
\[
M_{\text{Q4}} \approx 2 \cdot 32 \cdot d_{\text{attn}} \cdot 128\text{k} \cdot 0.5 \text{ bytes}.
\]
This yields two reference scenarios (for context):
\begin{itemize}
    \item \textbf{Constant-fraction $d_{\text{attn}}$ (e.g., $d_{\text{attn}}=768$):} $M_{\text{Q4}} \approx 3.0$~GiB, for an overall reduction of $\sim 21\times$.
    \item \textbf{Speculative fixed-rank $d_{\text{attn}}$ (intuition only):} If one could keep $d_{\text{attn}}$ roughly constant while scaling $d_{\text{model}}$ (e.g., $d_{\text{attn}}{=}96$ at $d_{\text{model}}{=}4096$), the same linear arithmetic yields an $\mathcal{O}(10^2)$ reduction versus a standard FP16 baseline.\footnote{This is the origin of the often-quoted ``168$\times$'': \((4096/96)\times 4 \approx 171\), sometimes rounded. Including Q4\_0 scale metadata gives \((4096/96)\times (2/0.5625)\approx 152\). We do \emph{not} validate fixed-rank scaling in this work.}
\end{itemize}
The architectural contribution is the \textit{dimension reduction} (the ratio $4096/d_{\text{attn}}$); the additional factor of $4\times$ comes from standard 16$\rightarrow$4-bit quantization (idealized). For fair comparisons, note that GQA caches can also be quantized; e.g., an $8\times$ GQA KV cache with Q4 would already yield $\sim$32$\times$ reduction vs FP16 standard. We therefore treat fixed-rank scaling numbers as back-of-the-envelope upper bounds, not a primary experimental claim.

\paragraph{Heterogeneous KV-cache quantization (decoupled).}
A practical benefit of decoupling is that it enables \textit{heterogeneous} KV-cache quantization: we can compress the semantic path more aggressively (e.g., Q4) while keeping the geometric (RoPE) path at higher fidelity (e.g., Q8). In this draft, heterogeneous KV-cache policies are treated as \textit{planned work}: we will report both quality deltas (held-out perplexity) and end-to-end device memory deltas at long context once the corresponding inference benchmarks are finalized.

In this draft, we do not yet report the end-to-end device memory deltas at 128k context; instead we report (i) theoretical KV-cache bytes/token estimates derived from the checkpoint \texttt{ModelConfig} and (ii) empirical long-context stability and decode-at-context timing from the benchmark harness (\path{research/dba/benchmark.yml}). End-to-end memory instrumentation remains planned work.

While we report training throughput in our experiments, the theoretical FLOPs reduction in the attention mechanism ($O(n^2 d) \to O(n^2 r)$) implies a proportional speedup in the \textit{prefill phase} of inference, where the KV-cache is populated. For autoregressive decoding, the memory bandwidth savings from the smaller cache dominate latency improvements.

% ============================================================================
% 3. EXPERIMENTS
% ============================================================================
\section{Experiments}

\subsection{Experimental Setup}

\paragraph{Reproducibility.}
All paper experiments are executed via declarative YAML manifests.\footnote{Experiments are run using Caramba, a manifest-driven ML research framework developed for this work. Caramba provides declarative specification of model topology, training protocols, and evaluation harnesses, with custom Triton and Metal kernels for improved throughput. Available at \url{https://github.com/theapemachine/caramba}.} Two configurations are defined:

\begin{table}[htbp]
\centering
\footnotesize
\caption{Experimental configurations (from manifest presets).}
\label{tab:configs}
\begin{tabular}{@{}llccccc@{}}
\toprule
\textbf{Suite} & \textbf{Configuration} & $d_{\text{sem}}$ & $d_{\text{geo}}$ & \textbf{Per-head} & \textbf{Steps} & \textbf{Status} \\
\midrule
\multirow{3}{*}{A100 (22L, 1B)}
  & Baseline & --- & --- & 64 & 10k & Complete \\
  & Decoupled (primary) & 512 & 1024 & 16+32 & 10k & Complete \\
  & Decoupled (aggressive) & 256 & 1024 & 8+32 & 10k & Complete \\
\midrule
\multirow{4}{*}{Local (12L, 550M)}
  & Baseline ($\times$3) & --- & --- & 64 & 5k & Planned \\
  & Bottleneck ($\times$3) & --- & --- & 48 & 5k & Planned \\
  & Decoupled ($\times$3) & 512 & 1024 & 16+32 & 5k & Planned \\
  & GQA 32Q/4KV ($\times$3) & --- & --- & 64 & 5k & Planned \\
\bottomrule
\end{tabular}
\end{table}

\noindent \textbf{A100 suite:} $d_{\text{model}}{=}2048$, $n_{\text{layers}}{=}22$, $n_{\text{heads}}{=}32$, $d_{\text{ff}}{=}5632$, vocab${=}50304$, \texttt{rope\_base=10000}, FineWeb-Edu 20B tokens.

\noindent \textbf{Local suite:} Same per-layer architecture but $n_{\text{layers}}{=}12$ ($\sim$550M params), FineWeb-Edu 1B tokens, 3 seeds (1337, 1338, 1339) for statistical validation. Manifest: \path{config/presets/dba_paper_local.yml}.

\noindent The canonical training invocations for the primary comparison are:
\begin{verbatim}
python -m caramba.cli run config/presets/dba_paper_rerun.yml --target baseline
python -m caramba.cli run config/presets/dba_paper_rerun.yml --target decoupled
\end{verbatim}

\paragraph{Datasets.}
Training uses a large tokenized FineWeb-Edu dump (\texttt{fineweb\_20b.npy}; \(\sim\)20B tokens). For lightweight post-hoc evaluation on local hardware we use smaller token shards from the same pipeline: \texttt{fineweb\_100m.npy} and \texttt{fineweb\_1b.npy}.

\paragraph{Evaluation and artifacts (this paper).}
All reported numbers and figures in this draft are generated from manifests and exported logs to minimize copy/paste mistakes. We use a local benchmark harness (Section~3; \path{research/dba/benchmark.yml} and variants) for paired checkpoint comparisons; it writes plots/tables into \path{research/dba/} via \texttt{artifacts\_dir} to avoid manual copying. For behavioral probes (identity retention and semantic collision), we include per-case teacher/student outputs as a generated appendix table when available (Appendix~\ref{app:behavior_cases}).

\paragraph{Training dynamics (A100; 1B; 10k steps).}
Figure~\ref{fig:a100_training_curves} summarizes the training loss and a perplexity proxy (computed as $\exp(\text{loss})$) for the A100 rerun (\texttt{baseline} vs.\ \texttt{decoupled}). These plots are generated directly from the exported W\&B CSV in \path{research/dba/} to avoid manual copy errors. The primary comparison uses the configuration from \path{config/presets/dba_paper_rerun.yml}: $d_{\text{sem}}{=}512$, $d_{\text{geo}}{=}1024$, $d_{\text{attn}}{=}1536$, trained for 10{,}000 steps with seed 1337.

\begin{figure}[htbp]
\centering
\begin{subfigure}[t]{0.49\textwidth}
  \centering
  \IfFileExists{A100-1b-10k-loss.png}{
    \includegraphics[width=\textwidth]{\detokenize{A100-1b-10k-loss.png}}
  }{
    \fbox{\parbox{0.92\textwidth}{\textbf{Missing figure:} \texttt{A100-1b-10k-loss.png}.}}
  }
  \caption{Training loss vs.\ step.}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.49\textwidth}
  \centering
  \IfFileExists{A100-1b-10k-ppl.png}{
    \includegraphics[width=\textwidth]{\detokenize{A100-1b-10k-ppl.png}}
  }{
    \fbox{\parbox{0.92\textwidth}{\textbf{Missing figure:} \texttt{A100-1b-10k-ppl.png}.}}
  }
  \caption{Perplexity proxy ($\exp(\text{loss})$) vs.\ step.}
\end{subfigure}
\caption{A100 training curves for the 1B/10k-step rerun (\texttt{baseline} vs.\ \texttt{decoupled}).}
\label{fig:a100_training_curves}
\end{figure}

% Auto-generated summary table from the W\&B export CSV.
\IfFileExists{A100-1b-10k-training_summary.tex}{
  \input{\detokenize{A100-1b-10k-training_summary.tex}}
}{}

\paragraph{Optimization stability note (warmup boundary).}
In early decoupled runs we observed transient loss spikes at the end of LR warmup (peak LR), even when the overall training curve matched the baseline. In the current reruns we mitigate this by using a more conservative peak LR for the decoupled configuration, adding gradient clipping, and applying small implementation-level compensations for the decoupled bottleneck. For transparency, we include a diagnostic plot (loss min/max band and LR) when available:

\begin{figure}[htbp]
\centering
\IfFileExists{A100-1b-10k-warmup_spikes.png}{
  \includegraphics[width=0.9\textwidth]{\detokenize{A100-1b-10k-warmup_spikes.png}}
}{}
\caption{Warmup-boundary diagnostic: loss (with min/max envelope when logged) and learning-rate schedule.}
\label{fig:warmup_spikes}
\end{figure}

\paragraph{Completed: A100 1B 10k-step pilot.}
The 10k-step pilot has completed for both baseline and decoupled configurations (Table~\ref{tab:a100-training-summary-10k}). The baseline achieves loss 3.36 (PPL proxy 28.9) at step 10{,}001, while decoupled achieves loss 3.38 (PPL proxy 29.5)---demonstrating near-parity between standard and bottleneck attention at this training budget.

\paragraph{Training efficiency.}
\label{sec:training_efficiency}
Figures~\ref{fig:gpu_memory}--\ref{fig:grad_norms} present training efficiency metrics from the A100 runs. The data reveals a compelling efficiency story:

\begin{figure}[htbp]
\centering
\IfFileExists{A100-1b-10k-gpu_memory.png}{
  \includegraphics[width=0.85\textwidth]{\detokenize{A100-1b-10k-gpu_memory.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Missing figure:} \texttt{A100-1b-10k-gpu\_memory.png}.}}
}
\caption{GPU memory allocated during training. Baseline uses $\sim$55GB while DBA decoupled uses $\sim$40GB---a \textbf{27\% reduction} in training memory. The aggressive variant (sem8/geo32) tracks with primary decoupled.}
\label{fig:gpu_memory}
\end{figure}

\begin{figure}[htbp]
\centering
\IfFileExists{A100-1b-10k-tok_s.png}{
  \includegraphics[width=0.85\textwidth]{\detokenize{A100-1b-10k-tok_s.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Missing figure:} \texttt{A100-1b-10k-tok\_s.png}.}}
}
\caption{Training throughput (tokens/sec). During warmup (steps 0--2000), all configurations achieve $\sim$18k tok/s. At step 2000 (warmup end), baseline throughput drops to $\sim$13.5k tok/s and remains there, while the aggressive DBA variant maintains $\sim$17.5k tok/s---a \textbf{32\% throughput advantage} post-warmup. Primary decoupled runs at $\sim$13k tok/s throughout.}
\label{fig:tok_s}
\end{figure}

\begin{figure}[htbp]
\centering
\IfFileExists{A100-1b-10k-grad_norms.png}{
  \includegraphics[width=0.85\textwidth]{\detokenize{A100-1b-10k-grad_norms.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Missing figure:} \texttt{A100-1b-10k-grad\_norms.png}.}}
}
\caption{Gradient norms during training. Baseline exhibits spikes up to 2.5 early in training (around warmup end at step 2000). DBA runs use \texttt{grad\_clip\_norm=1.0}, visible as clipped peaks, providing more stable optimization dynamics. All configurations converge to similar gradient magnitudes ($\sim$0.3--0.5) by end of training.}
\label{fig:grad_norms}
\end{figure}

\noindent\textbf{Key observations:}
\begin{itemize}
    \item \textbf{Memory:} DBA reduces training memory by 27\% (55GB $\rightarrow$ 40GB), enabling larger batch sizes or longer contexts on the same hardware.
    \item \textbf{Throughput:} The aggressive DBA variant (sem=8/head, geo=32/head) maintains 32\% higher throughput than baseline after warmup, likely due to reduced memory bandwidth pressure from the smaller KV projections.
    \item \textbf{Stability:} Gradient clipping (\texttt{grad\_clip\_norm=1.0}) combined with lower LR ($2 \times 10^{-4}$ vs.\ $3 \times 10^{-4}$) stabilizes DBA training, avoiding the spikes observed in baseline around step 2000.
\end{itemize}

\paragraph{Planned: A100 1B 100k-step runs.}
Extended 100k-step runs will be executed under the same manifest discipline (\path{config/presets/dba_paper_rerun.yml}). The corresponding plots/tables will be generated in the same way (exported logs $\rightarrow$ \path{research/dba/} artifacts).

\paragraph{Variant: Aggressive bottleneck (sem=8/head, geo=32/head).}
A second configuration (\path{config/presets/dba_paper_rerun_sem8_geo32_v40.yml}) uses more aggressive compression: $d_{\text{sem}}{=}256$ (8/head), $d_{\text{geo}}{=}1024$ (32/head), $d_{\text{attn}}{=}1280$ (40/head). Despite being incomplete (step 1{,}041), this variant shows promising efficiency characteristics: matching baseline throughput during warmup and maintaining it afterward while using less memory. Training will be continued to assess final loss parity.

\subsection{FineWeb-Edu Results}

\paragraph{Training loss comparison (10k steps).}
Table~\ref{tab:a100-training-summary-10k} summarizes the 10k-step training results. The baseline (standard attention) and decoupled (DBA with $d_{\text{sem}}{=}512$, $d_{\text{geo}}{=}1024$) achieve near-identical final loss (3.36 vs.\ 3.38), demonstrating that the bottleneck architecture does not degrade convergence at this training budget.

\paragraph{Held-out evaluation (pending).}
The benchmark harness defines perplexity evaluation on held-out FineWeb-Edu shards, but results have not yet been generated. The harness will evaluate both the baseline and decoupled checkpoints at 10k steps.

\begin{table}[htbp]
\centering
\small
\caption{Planned held-out perplexity evaluation (pending benchmark execution).}
\label{tab:ppl_pending}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Checkpoint} & \textbf{Status} & \textbf{Perplexity} \\
\midrule
Baseline (10k steps) & Pending & --- \\
Decoupled (10k steps) & Pending & --- \\
\bottomrule
\end{tabular}
\end{table}
\subsection{Behavioral Probes: Attention Focusing Effect}

We evaluate all three architectures (baseline, primary DBA, aggressive DBA) on 23 behavioral probe cases designed to test in-context learning, copy fidelity, and pattern completion. Table~\ref{tab:behavior_results} summarizes the automated pass rates; however, manual inspection reveals a striking qualitative difference.

\begin{table}[htbp]
\centering
\small
\caption{Behavioral probe results (10k-step checkpoints). Automated pass rate shown; manual analysis reveals additional correct responses with formatting artifacts.}
\label{tab:behavior_results}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{DBA (16+32)} & \textbf{DBA (8+32)} \\
\midrule
Automated pass rate & 4/23 (17.4\%) & 3/23 (13.0\%) & 4/23 (17.4\%) \\
Correct content (manual) & 4/23 & 5/23 & \textbf{9/23} \\
Repetition loops observed & 5 cases & 4 cases & \textbf{0 cases} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key finding: Attention focusing.}
The baseline model exhibits pathological repetition loops on copy tasks---for example, when prompted to copy ``A7 B4 C9 D2'', the baseline produces ``Red Green Blue. Output: Red Green Blue. Output: Red Green Blue...'' (looping on earlier context). The primary DBA variant (16+32 dims/head) shows reduced but still present looping. The aggressive DBA variant (8+32 dims/head) \textbf{eliminates repetition loops entirely} and correctly recalls the target content, albeit sometimes with formatting prefixes (e.g., ``Output: A7 B4 C9 D2'').

\paragraph{Progression of regularization.}
Table~\ref{tab:behavior_qualitative} shows representative cases demonstrating the regularization progression:

\begin{table}[htbp]
\centering
\footnotesize
\caption{Qualitative comparison on copy tasks (10k steps). \cmark~=~correct, \xmark~=~wrong, $\circlearrowleft$~=~loop.}
\label{tab:behavior_qualitative}
\begin{tabular}{@{}p{2.2cm}p{2.8cm}p{2.8cm}p{2.8cm}@{}}
\toprule
\textbf{Case} & \textbf{Baseline} & \textbf{DBA (16+32)} & \textbf{DBA (8+32)} \\
\midrule
copy\_fewshot & $\circlearrowleft$ ``Red Green...'' & $\circlearrowleft$ ``Red Green...'' & \cmark ``A7 B4 C9'' \\
copy\_commas & $\circlearrowleft$ ``Blue, Blue...'' & $\circlearrowleft$ ``Red, Green...'' & \cmark ``A7, B4, C9'' \\
repeat\_exact & \xmark wrong & \xmark wrong & \cmark correct \\
copy\_brackets & \xmark partial & \cmark correct & \cmark correct \\
spelling & \xmark ``t'' & \cmark ``n'' & \cmark ``n'' \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{The mechanism: why high-rank attention fails.}
We hypothesize that \textbf{high-rank semantic routing can be counterproductive} on in-context copy tasks, where the desired behavior is to follow the immediate instruction rather than latch onto earlier exemplars:

\begin{enumerate}
    \item \textbf{Baseline ($d=64$ per head):} High bandwidth allows the model to represent many competing context features with high fidelity---``Red'', ``Green'', ``Blue'', ``1'', ``2'', ``3'', ``A7''. In our probes, this correlates with the model over-weighting the \textit{pattern} of earlier examples (``Oh, we are listing colors!'') rather than the \textit{current input} (``A7''). \textbf{Observed result:} attention drift $\rightarrow$ repetition.

    \item \textbf{DBA extreme ($d=8$ per head, semantic path):} With a tight semantic bottleneck, the model is forced to prioritize which features survive in the routing subspace. On our probes, this coincides with attention mass concentrating on the immediate input, yielding \textbf{correct recall}.
\end{enumerate}

\noindent These results support a \textbf{bandwidth-as-regularization} interpretation, but do not rule out other contributors (e.g., optimization dynamics, schedule interactions). We therefore present this as a mechanistic hypothesis and validate it primarily through controlled architectural changes.

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.92\textwidth}{\small\raggedright
\textbf{Prompt:}\\
\texttt{Input: 1 2 3. Output: 1 2 3.}\\
\texttt{Input: Red Green Blue. Output: Red Green Blue.}\\
\texttt{Input: A7 B4 C9 D2. Output:}\hfill\textbf{Expected:} \texttt{A7 B4 C9 D2.}\\[0.8em]
\hrule
\vspace{0.5em}
\textbf{Baseline ($d=64$ per head):}\hfill\textcolor{red!70!black}{\textit{Attention Drift}}\\
\texttt{Red Green Blue. Output: Red Green Blue. Output: Red Green...}\\[0.5em]
\hrule
\vspace{0.5em}
\textbf{DBA Extreme ($d=8$ per head):}\hfill\textcolor{green!60!black}{\textit{Correct Recall}}\\
\texttt{Output: A7 B4 C9 D2. Output:}
}}
\caption{Attention drift in action. The baseline loops on distractor context; DBA correctly recalls the target. The bottleneck forces prioritization of the immediate instruction.}
\label{fig:attention_drift}
\end{figure}

\paragraph{What does the model attend to?}
To test the attention-drift hypothesis directly, we record attention weights for the \emph{final query token} (the token immediately after the last \texttt{Output:}) and split attention mass into two regions: (i) the earlier exemplar lines (distractors) and (ii) the target line beginning at the \texttt{A7} anchor. Figure~\ref{fig:attn_mass_copy_commas} shows that the baseline devotes a large fraction of its attention budget to the exemplar region across depth, whereas DBA shifts more mass toward the target region on the same prompt.

\begin{figure}[htbp]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=\linewidth]{copy_fewshot_commas_baseline_mass_by_layer.png}
\caption{Baseline}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=\linewidth]{copy_fewshot_commas_dba_mass_by_layer.png}
\caption{DBA (decoupled)}
\end{subfigure}
\caption{Final-query attention mass vs.\ depth on \texttt{copy\_fewshot\_commas}. The blue curve is attention mass on the exemplar region (before the \texttt{A7} anchor); the orange curve is mass on the target region (from \texttt{A7} onward).}
\label{fig:attn_mass_copy_commas}
\end{figure}

\begin{figure}[htbp]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=\linewidth]{copy_fewshot_commas_baseline_heatmap.png}
\caption{Baseline}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=\linewidth]{copy_fewshot_commas_dba_heatmap.png}
\caption{DBA (decoupled)}
\end{subfigure}
\caption{Final-query attention heatmaps (mean over sampled heads) for \texttt{copy\_fewshot\_commas}. Rows are attention modules across depth; columns are prompt tokens. The vertical white line marks the \texttt{A7} anchor separating exemplar context (left) from the target line (right).}
\label{fig:attn_heatmap_copy_commas}
\end{figure}

\paragraph{Aggressive DBA (sem8/geo32/v40).}
We repeat the same attention-mass analysis for the more aggressively compressed DBA variant (semantic 8 dims/head, geometric 32 dims/head, value 40 dims/head). When available, Figure~\ref{fig:attn_mass_copy_commas_sem8} and Figure~\ref{fig:attn_heatmap_copy_commas_sem8} provide a direct baseline vs.\ DBA (sem8/geo32/v40) comparison on the same \texttt{copy\_fewshot\_commas} prompt.

\begin{figure}[htbp]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=\linewidth]{copy_fewshot_commas_baseline_mass_by_layer.png}
\caption{Baseline}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{figs/attention/copy_fewshot_commas_dba_sem8geo32v40_mass_by_layer.png}{
\includegraphics[width=\linewidth]{copy_fewshot_commas_dba_sem8geo32v40_mass_by_layer.png}
}{
\IfFileExists{research/dba/figs/attention/copy_fewshot_commas_dba_sem8geo32v40_mass_by_layer.png}{
\includegraphics[width=\linewidth]{copy_fewshot_commas_dba_sem8geo32v40_mass_by_layer.png}
}{
\fbox{\parbox{0.95\linewidth}{\small Missing figure: run \texttt{baseline\_vs\_decoupled\_sem8geo32v40} with \texttt{dump\_attention=true}.}}
}
}
\caption{DBA (sem8/geo32/v40)}
\end{subfigure}
\caption{Final-query attention mass vs.\ depth on \texttt{copy\_fewshot\_commas}: baseline vs DBA (sem8/geo32/v40).}
\label{fig:attn_mass_copy_commas_sem8}
\end{figure}

\begin{figure}[htbp]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=\linewidth]{copy_fewshot_commas_baseline_heatmap.png}
\caption{Baseline}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{figs/attention/copy_fewshot_commas_dba_sem8geo32v40_heatmap.png}{
\includegraphics[width=\linewidth]{copy_fewshot_commas_dba_sem8geo32v40_heatmap.png}
}{
\IfFileExists{research/dba/figs/attention/copy_fewshot_commas_dba_sem8geo32v40_heatmap.png}{
\includegraphics[width=\linewidth]{copy_fewshot_commas_dba_sem8geo32v40_heatmap.png}
}{
\fbox{\parbox{0.95\linewidth}{\small Missing figure: run \texttt{baseline\_vs\_decoupled\_sem8geo32v40} with \texttt{dump\_attention=true}.}}
}
}
\caption{DBA (sem8/geo32/v40)}
\end{subfigure}
\caption{Final-query attention heatmaps (mean over sampled heads) for \texttt{copy\_fewshot\_commas}: baseline vs DBA (sem8/geo32/v40).}
\label{fig:attn_heatmap_copy_commas_sem8}
\end{figure}

\paragraph{Implications for 100k-step runs.}
These results from 10k-step checkpoints demonstrate that aggressive DBA compression yields \textit{qualitative} improvements beyond efficiency gains. We will re-run this analysis on 100k-step checkpoints to determine whether (a) the effect persists or amplifies with longer training, and (b) whether the minor formatting artifacts (e.g., extra ``Output:'' prefix) diminish as the model learns task structure.

\subsection{Downstream Accuracy: 10k-Step Results}

We evaluate all three architectures on standard multiple-choice benchmarks using log-probability scoring. Table~\ref{tab:downstream_10k} presents the results.

\begin{table}[htbp]
\centering
\small
\caption{Downstream accuracy at 10k training steps. Random baseline: Winogrande 50\%, ARC-Easy 25\%. All models are undertrained; differences are small but directionally informative.}
\label{tab:downstream_10k}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Task} & \textbf{Baseline} & \textbf{DBA (16+32)} & \textbf{DBA (8+32)} \\
\midrule
Winogrande & 51.85\% & 50.43\% & \textbf{52.01\%} \\
ARC-Easy & \textbf{48.42\%} & 43.86\% & 45.61\% \\
\midrule
\textit{$\Delta$ vs.\ Baseline} & --- & $-$4.0 avg & $-$1.3 avg \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key observations.}
\begin{itemize}
    \item \textbf{Extreme compression is viable:} The aggressive DBA variant (8+32 dims/head, 37.5\% of baseline attention dimension) achieves 52.01\% on Winogrande---\textit{outperforming} the baseline (51.85\%). This demonstrates that extreme compression does not lobotomize the model.
    \item \textbf{U-curve effect:} The primary DBA variant (16+32) underperforms both baseline and extreme DBA on both tasks. The more aggressive compression appears to provide stronger regularization that compensates for capacity reduction.
    \item \textbf{ARC-Easy gap:} Baseline leads on ARC-Easy (48.42\% vs.\ 45.61\%), a 2.8 percentage point difference. However, at 10k steps all models are undertrained (ARC-Easy random is 25\%), so this gap may close with extended training.
    \item \textbf{All models are near-random on Winogrande:} The $\sim$50\% scores indicate that commonsense reasoning has not yet emerged at 10k steps, regardless of architecture.
\end{itemize}

\paragraph{Interpretation.}
The most important finding is that \textbf{aggressive DBA compression remains viable at 10k steps}. Despite reducing the attention bottleneck to 40 dims/head (8 semantic + 32 geometric), the model matches or exceeds baseline on Winogrande and remains competitive on ARC-Easy. Combined with the 27\% memory reduction and 32\% throughput improvement documented in Section~\ref{sec:training_efficiency}, this justifies proceeding with the aggressive variant for 100k-step training runs.

\paragraph{Long-context stability (planned).}
The benchmark harness (\path{research/dba/benchmark.yml}) defines a \texttt{context\_sweep} benchmark that tests chunked prefill and decode-at-context up to 131{,}072 tokens. This benchmark will measure:
\begin{itemize}
    \item Total prefill time across chunk boundaries
    \item Decode-at-context latency (ms/token)
    \item Last-chunk perplexity (noting RoPE extrapolation beyond 2k training context)
\end{itemize}
Results are pending execution of the benchmark harness on the finalized checkpoints.

\begin{figure}[htbp]
\centering
\IfFileExists{compare_context_decode_tok_per_sec.png}{
  \includegraphics[width=0.9\textwidth]{\detokenize{compare_context_decode_tok_per_sec.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Pending figure:} \texttt{compare\_context\_decode\_tok\_per\_sec.png}.\\
  Generated by the benchmark harness (\path{research/dba/benchmark.yml}).}}
}
\caption{Planned: cached decode throughput (tokens/sec) vs.\ context length using chunked prefill + cached decode.}
\label{fig:compare_context_decode_tok_per_sec}
\end{figure}

\subsection{Local Suite: Multi-Architecture Comparison (Planned)}

The local suite (\path{config/presets/dba_paper_local.yml}) provides broader architectural comparisons on a 12-layer model ($\sim$550M params) with 3 seeds for statistical significance:

\begin{table}[htbp]
\centering
\small
\caption{Local suite: architecture comparison (pending execution).}
\label{tab:local_arch_compare}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Architecture} & \textbf{Loss (mean $\pm$ std)} & \textbf{PPL} & \textbf{KV bytes/token} \\
\midrule
Baseline (standard) & --- & --- & 4096 \\
Bottleneck & --- & --- & 3072 \\
Decoupled (DBA) & --- & --- & 3072 \\
GQA (32Q/4KV) & --- & --- & 512 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{DBA Design Ablations (Planned)}

The following DBA ablations are defined in the local manifest and will isolate the contribution of each design choice:

\begin{table}[htbp]
\centering
\small
\caption{DBA ablations (pending execution).}
\label{tab:dba_ablations}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Ablation} & \textbf{null\_attn} & \textbf{tie\_qk} & \textbf{gate} & \textbf{RoPE} & \textbf{Loss} \\
\midrule
Decoupled (default) & false & false & false & geo only & --- \\
+ Null token & true & false & false & geo only & --- \\
+ Tied Q-K & false & true & false & geo only & --- \\
+ Gate & false & false & true & geo only & --- \\
-- RoPE (none) & false & false & false & none & --- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{LR Sensitivity (Planned)}

We sweep learning rate for the baseline to establish fair comparison bounds:

\begin{table}[htbp]
\centering
\small
\caption{Learning rate sensitivity (baseline, seed 1337).}
\label{tab:lr_sweep}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Learning Rate} & \textbf{Final Loss} & \textbf{PPL} \\
\midrule
$2 \times 10^{-4}$ & --- & --- \\
$3 \times 10^{-4}$ (default) & --- & --- \\
$4 \times 10^{-4}$ & --- & --- \\
\bottomrule
\end{tabular}
\end{table}

\noindent Note: Decoupled runs use $\text{lr}=2 \times 10^{-4}$ (lower than baseline) with \texttt{grad\_clip\_norm=1.0} to prevent loss spikes after warmup.

\subsection{Inference Benchmarks (Planned)}

The following benchmarks are defined in \path{research/dba/benchmark.yml}:
\begin{itemize}
    \item \textbf{Held-out perplexity:} \texttt{ppl\_fineweb\_100m} (50--200 batches) comparing baseline vs.\ DBA.
    \item \textbf{Downstream accuracy:} \texttt{downstream\_accuracy} for HellaSwag, Winogrande, ARC-Easy.
    \item \textbf{Latency:} \texttt{latency\_cached} measuring decode throughput at prompt lengths 128--8192.
    \item \textbf{Memory:} \texttt{memory\_kv} measuring KV-cache footprint at sequence lengths 512--16384.
    \item \textbf{Behavioral probes:} \texttt{behavior\_sanity} using 22 test cases (copy tasks, arithmetic, passkey retrieval).
    \item \textbf{Context sweep:} \texttt{context\_sweep} testing chunked prefill + decode up to 131k tokens.
\end{itemize}

\subsection{Memory--Quality Trade-off (Pending)}

We will report a Pareto-style comparison (perplexity vs.\ KV-cache footprint and decode throughput).

\IfFileExists{table_scale.tex}{
  \input{\detokenize{table_scale.tex}}
}{
  % Optional: generate via generate_paper_figures.py --paper-dir paper
}

\begin{figure}[htbp]
\centering
\IfFileExists{pareto_curve.png}{
  \includegraphics[width=0.85\textwidth]{\detokenize{pareto_curve.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Pending figure:} \texttt{pareto\_curve.png}.\\
  Will be generated from \texttt{mem128k.json} + training logs via \path{generate_paper_figures.py}.}}
}
\caption{Planned Pareto curve: quality (perplexity) vs.\ efficiency (KV-cache bytes/token and decode throughput).}
\label{fig:pareto}
\end{figure}

\subsection{Memory Footprint Analysis}

Table~\ref{tab:memory} gives an illustrative KV-cache scaling projection for a 128k context in a Llama-like configuration (32 layers, $d_{\text{model}} = 4096$). We intentionally \emph{do not} foreground optimistic fixed-rank ``upper bound'' numbers here; the experimentally grounded takeaway is the linear dependence on the interaction dimension and the fact that architectural reduction composes multiplicatively with KV-cache quantization. End-to-end device memory deltas at 128k are planned and will be reported in a future revision.

\begin{table}[htbp]
\centering
\small
\caption{KV-Cache Memory for 128k Context (Llama-like scale; projected)}
\label{tab:memory}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Architecture} & \textbf{VRAM} & \textbf{Compression} \\
\midrule
Standard (FP16) & 64.0 GB & 1$\times$ \\
GQA (32Q/4KV; FP16) & 8.0 GB & 8$\times$ \\
GQA (32Q/4KV; Q4, ideal) & 2.0 GB & 32$\times$ \\
MLA (FP16) & 4.3 GB & 15$\times$ \\
Bottleneck (FP16) & 1.5 GB & 43$\times$ \\
Decoupled (Q4, constant-fraction $d_{\text{attn}}{=}768$) & 3.0 GB & 21$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\IfFileExists{memory_footprint.png}{
  \includegraphics[width=0.85\textwidth]{\detokenize{memory_footprint.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Pending figure:} \texttt{memory\_footprint.png}.\\
  Generated from \texttt{mem128k.json} via \path{generate_paper_figures.py}.}}
}
\caption{KV-cache memory comparison at long context (illustrative projection).}
\label{fig:memory}
\end{figure}

% ============================================================================
% 4. DISCUSSION
% ============================================================================
\section{Discussion}

\subsection{Why Does Low-Rank Attention Work?}

We hypothesize two complementary explanations:

\paragraph{Intrinsic Dimensionality.}
Following Aghajanyan et al. \cite{aghajanyan2021intrinsic}, natural language representations lie on low-dimensional manifolds. The attention mechanism's role is \textit{routing}---selecting which tokens to aggregate---not computing complex transformations. Routing decisions are inherently low-entropy and thus low-rank.

\paragraph{Regularization Effect.}
The bottleneck may act as an implicit regularizer by reducing the capacity of token--token interactions. In the 10k-step comparison (Table~\ref{tab:a100-training-summary-10k}), baseline achieves slightly lower loss (3.36 vs.\ 3.38), suggesting DBA's primary benefit is efficiency rather than perplexity improvement.

\paragraph{Gradient Rank Dynamics.}
AdaRankGrad \cite{refael2024adarankgrad} proves that gradient rank decreases monotonically during training, eventually approaching rank one. This suggests that \textit{architectural} bottlenecks become increasingly appropriate as training progresses---the model naturally ``wants'' to operate in a low-rank subspace. By hard-wiring this constraint from the start, we may accelerate convergence by matching the architecture to the optimization landscape.

\subsection{When to Use Each Architecture}

Our experiments are organized into a local suite (FineWeb-Edu 100M) for broader comparisons and a scale suite (FineWeb-Edu 20B tokens) for confirmation.

\begin{itemize}
    \item \textbf{Decoupled Bottleneck:} On FineWeb-Edu, the decoupled bottleneck is a strong default that preserves the KV memory benefits of low-rank attention while enabling \textbf{heterogeneous quantization} (e.g., Q4 semantic, Q8 geometric).
    \item \textbf{Standard Attention:} A strong baseline and simplest implementation, but can be memory-inefficient for long contexts.
\end{itemize}

\paragraph{Recommendation.}
For \textit{training}, iterate on the local FineWeb-Edu suite and validate at scale with the A100 suite. For \textit{inference} under memory constraints, use Decoupled with heterogeneous quantization (aggressively compress semantic, preserve geometric fidelity).

\paragraph{Flash/SDPA compatibility.}
Decoupled Bottleneck Attention can be implemented using PyTorch's fused \texttt{scaled\_dot\_product\_attention} by concatenating the scaled semantic and geometric Q/K projections along the head dimension, making it compatible with modern Flash Attention kernels.

\subsection{Limitations}

\begin{itemize}
    \item \textbf{Quality--efficiency trade-off:} In the 10k-step pilot, baseline achieves slightly lower loss (3.36 vs.\ 3.38). Latency and memory benefits are expected but not yet measured at inference time.
    \item \textbf{Preliminary downstream evaluation:} Winogrande and ARC-Easy results (Table~\ref{tab:downstream_10k}) show competitive but not conclusive performance at 10k steps. Extended evaluation on HellaSwag, PIQA, and BoolQ is pending.
    \item \textbf{Long-context stability not yet validated:} The \texttt{context\_sweep} benchmark is defined but not yet run. We expect RoPE extrapolation issues beyond the 2k training context (\texttt{rope\_base=10000}).
    \item \textbf{Single training budget:} Only 10k steps completed; 100k-step runs are planned to validate convergence at scale.
    \item \textbf{Scope of the claim:} DBA targets autoregressive LMs where semantic routing competes with in-context exemplars; other settings (e.g., dense retrieval, multimodal alignment, algorithmic tasks) may benefit from higher-rank routing and should be evaluated separately.
    \item The optimal $(d_{\text{sem}}, d_{\text{geo}})$ split may vary with model scale and tokenizer.
\end{itemize}

% ============================================================================
% 5. CONCLUSION
% ============================================================================
\section{Conclusion}

We have demonstrated that attention in Transformers can be compressed via decoupled bottleneck attention without degrading convergence. Our 10k-step training results (Table~\ref{tab:a100-training-summary-10k}) show near loss-parity between baseline and DBA. Downstream benchmarks and long-context sweeps are defined and pending execution.

The core insight is architectural: \textbf{Attention is a router, not a processor.} The heavy computation should happen in the feedforward layers (which we leave at full rank), while attention merely selects which tokens to aggregate. By matching the architecture to this functional role, we unlock dramatic efficiency gains.

Our Decoupled Bottleneck Attention separates semantic matching from positional geometry, allowing aggressive compression on the former while preserving RoPE fidelity on the latter. Combined with 4-bit KV-cache quantization, the memory arithmetic suggests substantial compression (Table~\ref{tab:memory}); empirical validation is pending.

\paragraph{Future Work.}
We plan to: (1) run the benchmark harness to generate held-out perplexity, latency, and memory comparisons; (2) execute long-context sweeps up to 128k and record the breaking point; (3) complete 100k-step training runs for both baseline and aggressive DBA (8+32) to determine whether the downstream accuracy gap closes; (4) extend downstream evaluation to HellaSwag, PIQA, and BoolQ; and (5) run \textbf{Attention Surgery} experiments---structured architectural edits on trained checkpoints to isolate which components of DBA are necessary and sufficient.

\paragraph{Attention Surgery.}
Ongoing ``Attention Surgery'' experiments aim to (i) modify attention modules post-hoc on trained checkpoints, (ii) verify functional parity and quantify drift, and (iii) benchmark quality/memory/latency trade-offs under controlled architectural edits.

% ============================================================================
% STATEMENTS
% ============================================================================
\section*{Statements and Declarations}

\paragraph{Conflict of Interest.}
The author declares no competing interests. This research was conducted independently without corporate affiliation or funding from entities with financial interests in the outcomes.

\paragraph{Data Availability.}
All datasets used in this study are publicly available: FineWeb-Edu is available from Hugging Face. The code, trained model checkpoints, and all experimental logs are available at \url{https://github.com/theapemachine/caramba}. Manifest presets are in \path{config/presets/dba_paper_rerun*.yml}; benchmark definitions in \path{research/dba/benchmark*.yml}.

\paragraph{Funding.}
This research was conducted without external funding. All computational resources were provided by the author.

% ============================================================================
% APPENDIX
% ============================================================================
\appendix

\section{Pending: Effective Rank Evidence}
\label{app:effective_rank}

This appendix reserves space for empirical effective-rank measurements of Q/K projection activations (singular value spectra and entropy effective rank) for the \texttt{paper\_baseline} and \texttt{paper\_decoupled} checkpoints. These results will be generated from the production checkpoints and copied into the paper directory for arXiv.

\section{Decoupled Ablations}
\label{app:decoupled_ablations}

This appendix presents detailed results for DBA design ablations, run via \path{config/presets/dba_paper_local.yml}:

\begin{itemize}
    \item \textbf{Null token} (\texttt{null\_attn=true}): Provides an explicit ``attend nowhere'' option. May stabilize training at very low ranks.
    \item \textbf{Tied Q-K} (\texttt{tie\_qk=true}): Forces symmetric attention ($W_Q = W_K$ for semantic path). Reduces parameters but may limit expressivity.
    \item \textbf{Gating} (\texttt{decoupled\_gate=true}): Enables a learnable per-head mixing gate between semantic and geometric routing paths. Treated as a distinct variant from the ungated DBA used in the primary comparison.
    \item \textbf{No RoPE} (\texttt{rope\_enabled=false}): Removes positional encoding entirely. Expected to degrade significantly on position-sensitive tasks.
\end{itemize}

\noindent Full training curves and behavioral probe results will be included once the local suite completes.

\section{Behavioral Probe Cases (Per-Case Outputs)}
\label{app:behavior_cases}

For transparency, we include a per-case table of behavioral probes (teacher vs.\ student pass/fail, plus a truncated preview of each model's output). This table is generated automatically by the benchmark harness and included here via \texttt{\textbackslash input} when present.

\IfFileExists{behavior_cases_table.tex}{
  \input{\detokenize{behavior_cases_table.tex}}
}{}

\section{Long-Context Sweep (Up to 131{,}072 Tokens)}
\label{app:long_context_sweep}

The \texttt{context\_sweep} benchmark (\path{research/dba/benchmark.yml}) will test chunked prefill and decode-at-context for the paired checkpoints. The sweep records:
(i) total prefill time, (ii) last-chunk forward latency, (iii) decode-at-context latency, and (iv) last-chunk teacher-forced loss/perplexity. We stress that the loss/perplexity at long context will reflect RoPE extrapolation beyond the 2k training context and should not be interpreted as long-context \emph{capability} without dedicated long-context training.

\begin{table}[htbp]
\centering
\small
\caption{Context sweep results (pending benchmark execution).}
\label{tab:context_sweep}
\begin{tabular}{@{}rcccc@{}}
\toprule
\textbf{Context} & \textbf{Prefill (s)} & \textbf{Decode 1 tok (ms)} & \textbf{PPL (last chunk)} & \textbf{OK} \\
\midrule
2{,}048 & --- & --- & --- & --- \\
4{,}096 & --- & --- & --- & --- \\
8{,}192 & --- & --- & --- & --- \\
16{,}384 & --- & --- & --- & --- \\
32{,}768 & --- & --- & --- & --- \\
65{,}536 & --- & --- & --- & --- \\
98{,}304 & --- & --- & --- & --- \\
131{,}072 & --- & --- & --- & --- \\
\bottomrule
\end{tabular}
\end{table}

\noindent\textit{Note: The benchmark harness defines context lengths \{2048, 4096, 8192, 16384, 32768, 65536, 98304, 131072\} with chunk\_size=1024 and decode\_len=128. Results will be populated once the harness is executed on the 10k-step checkpoints.}

\begin{figure}[htbp]
\centering
\IfFileExists{context_decode_one_ms.png}{
  \includegraphics[width=0.9\textwidth]{\detokenize{context_decode_one_ms.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Pending figure:} \texttt{context\_decode\_one\_ms.png}.\\
  Will be generated by \texttt{context\_sweep} benchmark once executed.}}
}
\caption{Planned: Decode-at-context cost (ms/token) vs.\ context length.}
\label{fig:context_decode_one_ms}
\end{figure}

\begin{figure}[htbp]
\centering
\IfFileExists{latency_tokens_per_sec.png}{
  \includegraphics[width=0.9\textwidth]{\detokenize{latency_tokens_per_sec.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Pending figure:} \texttt{latency\_tokens\_per\_sec.png}.\\
  Will be generated by \texttt{latency\_cached} benchmark once executed.}}
}
\caption{Planned: Cached decode throughput microbenchmark (batch=1).}
\label{fig:latency_tokens_per_sec}
\end{figure}

\begin{figure}[htbp]
\centering
\IfFileExists{perplexity.png}{
  \includegraphics[width=0.85\textwidth]{\detokenize{perplexity.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Pending figure:} \texttt{perplexity.png}.\\
  Will be generated by \texttt{ppl\_fineweb\_100m} benchmark once executed.}}
}
\caption{Planned: Held-out perplexity on FineWeb-Edu token shards.}
\label{fig:perplexity_plot}
\end{figure}

% ============================================================================
% REFERENCES
% ============================================================================
\FloatBarrier
\clearpage
\bibliographystyle{plain}
\bibliography{references}

\end{document}
