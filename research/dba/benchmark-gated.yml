version: 2
name: dba_l22_100k_gated_benchmark
# Write artifacts directly into the paper directory tree.
artifacts_dir: research/dba/benchmark100k_gated
notes: >
  Benchmark for gated DBA 100k A100 runs: baseline vs sem8geo32v40 vs gated sem8geo32v40.

defaults:
  data:
    tokenizer: tiktoken:gpt2
    val_frac: 0.0
  logging:
    instrument: rich
    wandb: false
    wandb_project: ''
    wandb_entity: ''
    wandb_mode: disabled
    eval_iters: 0
  runtime:
    save_every: 1

vars:
  # --- Checkpoints ---
  baseline_ckpt: research/dba/100k_checkpoints/baseline/a100_fw20b_l22_baseline_s42_100k.pt
  sem8geo32v40_ckpt: research/dba/100k_checkpoints/sem8geo32v40/a100_fw20b_l22_dba_s42_100k.pt
  gated_sem8geo32v40_ckpt: research/dba/100k_checkpoints/sem8geo32v40_gated/a100_fw20b_l22_dba_gated_s42_100k.pt

  # --- Dataset ---
  dataset_path: artifacts/datasets/HuggingFaceFW/fineweb/fineweb_20b.npy
  block_size: 2048

  # --- Model dims ---
  d_model: 2048
  n_layers: 22
  n_heads: 32
  # Checkpoints were trained with d_ff=5632
  d_ff: 5632
  # gated checkpoint was trained with d_ff=5632
  gated_d_ff: 5632
  vocab_size: 50304
  rope_enabled: true
  rope_base: 10000.0
  tie_qk: false
  null_attn: false
  bias: false

  # --- DBA dims for sem8/geo32/v40 (selected config) ---
  sem8_sem_dim: 256
  sem8_geo_dim: 1024
  sem8_attn_dim: 1280

  # --- Runtime ---
  device: cuda
  # On CUDA, "auto" selects bf16 when supported (e.g. A100), else fp16.
  dtype: auto

# =============================================================================
# YAML ANCHORS - Shared model definitions
# =============================================================================

x-baseline-model: &baseline_model
  type: TransformerModel
  tied_embeddings: false
  embedder:
    type: token
    vocab_size: ${vocab_size}
    d_model: ${d_model}
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                mode: standard
                rope_enabled: ${rope_enabled}
                rope_base: ${rope_base}
                is_causal: true
                dropout_p: 0.0
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: ${bias}
      - type: RMSNormLayer
        d_model: ${d_model}
        eps: 1e-5
      - type: LinearLayer
        d_in: ${d_model}
        d_out: ${vocab_size}
        bias: ${bias}

x-decoupled-sem8-model: &decoupled_sem8_model
  type: TransformerModel
  tied_embeddings: false
  embedder:
    type: token
    vocab_size: ${vocab_size}
    d_model: ${d_model}
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                mode: decoupled
                dba_train_backend: sdpa
                attn_dim: ${sem8_attn_dim}
                sem_dim: ${sem8_sem_dim}
                geo_dim: ${sem8_geo_dim}
                rope_enabled: ${rope_enabled}
                rope_base: ${rope_base}
                rope_semantic: false
                tie_qk: false
                null_attn: false
                decoupled_gate: false
                is_causal: true
                dropout_p: 0.0
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: ${bias}
      - type: RMSNormLayer
        d_model: ${d_model}
        eps: 1e-5
      - type: LinearLayer
        d_in: ${d_model}
        d_out: ${vocab_size}
        bias: ${bias}

x-decoupled-sem8-gated-model: &decoupled_sem8_gated_model
  type: TransformerModel
  tied_embeddings: false
  embedder:
    type: token
    vocab_size: ${vocab_size}
    d_model: ${d_model}
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                mode: decoupled
                dba_train_backend: sdpa
                attn_dim: ${sem8_attn_dim}
                sem_dim: ${sem8_sem_dim}
                geo_dim: ${sem8_geo_dim}
                rope_enabled: ${rope_enabled}
                rope_base: ${rope_base}
                rope_semantic: false
                tie_qk: false
                null_attn: false
                decoupled_gate: true
                is_causal: true
                dropout_p: 0.0
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${gated_d_ff}
                bias: ${bias}
      - type: RMSNormLayer
        d_model: ${d_model}
        eps: 1e-5
      - type: LinearLayer
        d_in: ${d_model}
        d_out: ${vocab_size}
        bias: ${bias}

x-dummy-run: &dummy_run
  - id: eval
    mode: train
    exp: benchmark_only
    seed: 42
    steps: 1
    expected: {phase: standard}
    train:
      phase: standard
      batch_size: 1
      block_size: ${block_size}
      lr: 1.0e-9
      device: ${device}
      dtype: ${dtype}

# =============================================================================
# Multi-checkpoint benchmark suite for N-way comparison
# =============================================================================
x-multi-benchmark-suite: &multi_benchmark_suite
  - id: ppl_fineweb
    realtime: false
    config:
      type: perplexity
      dataset: ${dataset_path}
      block_size: ${block_size}
      batch_size: 4
      # Token budget: num_batches * block_size (batch_size=1).
      # 5000 * 2048 â‰ˆ 10.24M tokens.
      num_batches: 4000
      # GPT-2 tokenizer vocab is 50257; Caramba models often pad to 50304.
      # Slice logits to this size for fair perplexity.
      valid_vocab_size: 50257
    models: [baseline, sem8, gated_sem8]

  - id: latency_cached
    realtime: false
    config:
      type: latency
      seed: 42
      prompt_lengths: [128, 512, 1024, 2048, 4096]
      generation_lengths: [128]
      batch_sizes: [1]
      warmup_runs: 2
      timed_runs: 5
      use_cache: true
      cache_kind: fp16
      # Keep token generation in the real tokenizer vocab for comparable inputs.
      valid_vocab_size: 50257
    models: [baseline, sem8, gated_sem8]

  - id: latency_long_context
    realtime: false
    config:
      type: latency
      seed: 42
      prompt_lengths: [2048, 4096, 8192]
      generation_lengths: [64]
      batch_sizes: [1]
      warmup_runs: 1
      timed_runs: 3
      use_cache: true
      cache_kind: fp16
      valid_vocab_size: 50257
    models: [baseline, sem8, gated_sem8]

  - id: memory_kv
    realtime: false
    config:
      type: memory
      seed: 42
      sequence_lengths: [512, 1024, 2048, 4096, 8192, 16384]
      batch_sizes: [1]
      measure_peak: true
      measure_kvcache: true
      quantization_modes: [fp16]
      valid_vocab_size: 50257
    models: [baseline, sem8, gated_sem8]

  - id: behavior_sanity
    realtime: false
    config:
      type: behavior
      tokenizer:
        type: tiktoken
        encoding: gpt2
      seed: 42
      suite_file: benchmark/behavior/cases.yml
      max_new_tokens: 32
      context_window: 2048
      dump_attention: true
      dump_attention_paper_dir: research/dba/figs/attention
      dump_attention_paper_tag: dba_100k_gated
    models: [baseline, sem8, gated_sem8]

  - id: downstream_accuracy
    realtime: false
    config:
      type: accuracy
      tasks: [winogrande, arc_easy, boolq, hellaswag, copa, piqa, openbookqa]
      tokenizer:
        type: tiktoken
        encoding: gpt2
      seed: 42
      context_window: 2048
      stream_live: true
      stream_every: 10
    models: [baseline, sem8, gated_sem8]

  - id: adversarial_repeat_penalty
    realtime: false
    config:
      type: behavior_instruct
      tokenizer:
        type: tiktoken
        encoding: gpt2
      # v2 suite generator (deterministic).
      seed: 42
      tests_per_category: 30
      categories: [adversarial]
      # Narrow to the specific forced-repeat template family.
      subcategories: [dread_induction]
      # Make it a real "repeat penalty" probe.
      honor_recommended_settings: true
      repetition_penalty: 1.4
      max_new_tokens: 128
      context_window: 2048
      stream_live: true
      stream_every: 1
      transcript_file: behavior_instruct_repeat_penalty_transcript.txt
      degeneration_metrics_file: behavior_instruct_repeat_penalty_degeneration.csv
    models: [baseline, sem8, gated_sem8]

  - id: context_sweep
    realtime: false
    config:
      type: context
      seed: 42
      dataset: ${dataset_path}
      context_lengths: [2048, 4096, 8192, 16384, 32768, 65536, 98304, 131072]
      chunk_size: 1024
      max_mask_elems: 32000000
      batch_size: 1
      decode_len: 128
      decode_warmup: 8
      cache_kind: fp16
      # Slice logits / validate dataset ids against tokenizer vocab.
      valid_vocab_size: 50257
    models: [baseline, sem8, gated_sem8]
# =============================================================================
# TARGETS
# =============================================================================

targets:
  # ===========================================================================
  # PRIMARY: Multi-checkpoint N-way comparison (baseline vs sem8 vs gated_sem8)
  # ===========================================================================
  - type: experiment
    name: multi_checkpoint_compare
    description: "Compare all 3 checkpoints: baseline vs DBA-sem8 vs DBA-sem8 (gated)."
    backend: torch
    task: task.language_modeling
    data:
      ref: dataset.tokens
      config:
        path: ${dataset_path}
        block_size: ${block_size}
    system:
      ref: system.language_model
      config:
        model: *baseline_model
    objective: objective.next_token_ce
    trainer:
      ref: trainer.multi_checkpoint_compare
      config:
        checkpoints:
          - name: baseline
            checkpoint: ${baseline_ckpt}
            model_config: *baseline_model
            is_baseline: true
          - name: sem8
            checkpoint: ${sem8geo32v40_ckpt}
            model_config: *decoupled_sem8_model
          - name: gated_sem8
            checkpoint: ${gated_sem8geo32v40_ckpt}
            model_config: *decoupled_sem8_gated_model
        device: ${device}
        dtype: ${dtype}
        strict: false
        unsafe_pickle_load: false
    runs: *dummy_run
    benchmarks: *multi_benchmark_suite

entrypoints:
  default: "target:multi_checkpoint_compare"
