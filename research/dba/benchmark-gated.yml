version: 2
name: dba_l22_100k_benchmark_gated
# Write artifacts directly into the paper directory tree.
artifacts_dir: research/dba/benchmark100k_gated
notes: >
  Benchmark for exploratory 100k A100 runs: baseline vs sem8geo32v40 vs gated sem8geo32v40.

defaults:
  data:
    tokenizer: tiktoken:gpt2
    val_frac: 0.0
  logging:
    instrument: rich
    wandb: false
    wandb_project: ''
    wandb_entity: ''
    wandb_mode: disabled
    eval_iters: 0
  runtime:
    save_every: 1

vars:
  # --- Checkpoints ---
  baseline_ckpt: research/dba/100k_checkpoints/baseline/a100_fw1b_l22_baseline_s42_100k.pt
  sem8geo32v40_ckpt: research/dba/100k_checkpoints/sem8geo32v40/a100_fw1b_l22_dba_s42_100k.pt
  gated_sem8geo32v40_ckpt: research/dba/100k_checkpoints/sem8geo32v40_gated/train_standard_final.pt

  # --- Dataset ---
  dataset_path: artifacts/datasets/HuggingFaceFW/fineweb/fineweb_20b.npy
  block_size: 2048

  # --- Model dims ---
  d_model: 2048
  n_layers: 22
  n_heads: 32
  # baseline + sem8 checkpoints were trained with d_ff=5632
  d_ff: 5632
  # gated checkpoint was trained with d_ff=4096
  gated_d_ff: 4096
  vocab_size: 50304
  rope_base: 10000.0

  # --- DBA dims for sem8/geo32/v40 (selected config) ---
  sem8_sem_dim: 256
  sem8_geo_dim: 1024
  sem8_attn_dim: 1280

  # --- Runtime ---
  device: mps
  dtype: float16

# =============================================================================
# YAML ANCHORS - Shared model definitions
# =============================================================================

x-baseline-model: &baseline_model
  type: TransformerModel
  tied_embeddings: false
  embedder:
    type: token
    vocab_size: ${vocab_size}
    d_model: ${d_model}
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                mode: standard
                rope_enabled: true
                rope_base: ${rope_base}
                is_causal: true
                dropout_p: 0.0
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: false
      - type: RMSNormLayer
        d_model: ${d_model}
        eps: 1e-5
      - type: LinearLayer
        d_in: ${d_model}
        d_out: ${vocab_size}
        bias: false

x-decoupled-sem8-model: &decoupled_sem8_model
  type: TransformerModel
  tied_embeddings: false
  embedder:
    type: token
    vocab_size: ${vocab_size}
    d_model: ${d_model}
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                mode: decoupled
                dba_train_backend: sdpa
                attn_dim: ${sem8_attn_dim}
                sem_dim: ${sem8_sem_dim}
                geo_dim: ${sem8_geo_dim}
                rope_enabled: true
                rope_base: ${rope_base}
                rope_semantic: false
                tie_qk: false
                null_attn: false
                decoupled_gate: false
                is_causal: true
                dropout_p: 0.0
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: false
      - type: RMSNormLayer
        d_model: ${d_model}
        eps: 1e-5
      - type: LinearLayer
        d_in: ${d_model}
        d_out: ${vocab_size}
        bias: false

x-decoupled-sem8-gated-model: &decoupled_sem8_gated_model
  type: TransformerModel
  tied_embeddings: false
  embedder:
    type: token
    vocab_size: ${vocab_size}
    d_model: ${d_model}
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                mode: decoupled
                dba_train_backend: sdpa
                attn_dim: ${sem8_attn_dim}
                sem_dim: ${sem8_sem_dim}
                geo_dim: ${sem8_geo_dim}
                rope_enabled: true
                rope_base: ${rope_base}
                rope_semantic: false
                tie_qk: false
                null_attn: false
                decoupled_gate: true
                is_causal: true
                dropout_p: 0.0
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${gated_d_ff}
                bias: false
      - type: RMSNormLayer
        d_model: ${d_model}
        eps: 1e-5
      - type: LinearLayer
        d_in: ${d_model}
        d_out: ${vocab_size}
        bias: false

x-dummy-run: &dummy_run
  - id: eval
    mode: train
    exp: benchmark_only
    seed: 42
    steps: 1
    expected: {phase: standard}
    train:
      phase: standard
      batch_size: 1
      block_size: ${block_size}
      lr: 1.0e-9
      device: ${device}
      dtype: ${dtype}

# =============================================================================
# Multi-checkpoint benchmark suite for N-way comparison
# =============================================================================
x-multi-benchmark-suite: &multi_benchmark_suite
  - id: ppl_fineweb
    config:
      type: perplexity
      dataset: ${dataset_path}
      block_size: ${block_size}
      batch_size: 1
      num_batches: 200
      # GPT-2 tokenizer vocab is 50257; Caramba models often pad to 50304.
      # Slice logits to this size for fair perplexity.
      valid_vocab_size: 50257
    models: [baseline, sem8, gated_sem8]

  - id: latency_cached
    config:
      type: latency
      prompt_lengths: [128, 512, 1024, 2048, 4096]
      generation_lengths: [128]
      batch_sizes: [1]
      warmup_runs: 2
      timed_runs: 5
      use_cache: true
      cache_kind: fp16
      # Keep token generation in the real tokenizer vocab for comparable inputs.
      valid_vocab_size: 50257
    models: [baseline, sem8, gated_sem8]

  - id: latency_long_context
    config:
      type: latency
      prompt_lengths: [2048, 4096, 8192]
      generation_lengths: [64]
      batch_sizes: [1]
      warmup_runs: 1
      timed_runs: 3
      use_cache: true
      cache_kind: fp16
      valid_vocab_size: 50257
    models: [baseline, sem8, gated_sem8]

  - id: memory_kv
    config:
      type: memory
      sequence_lengths: [512, 1024, 2048, 4096, 8192, 16384]
      batch_sizes: [1]
      measure_peak: true
      measure_kvcache: true
      quantization_modes: [fp16]
      valid_vocab_size: 50257
    models: [baseline, sem8, gated_sem8]

  - id: behavior_sanity
    config:
      type: behavior
      tokenizer:
        type: tiktoken
        encoding: gpt2
      cases_file: research/dba/behavior_cases.yml
      max_new_tokens: 32
      context_window: 2048
      dump_attention: true
      dump_attention_case_ids:
        - copy_simple_3
        - copy_simple_numbers
        - copy_medium_sequence
        - copy_long_sequence_15
        - copy_mixed_alphanumeric
        - copy_special_chars
        - copy_with_spaces_preserved
        - fewshot_simple_transform
        - fewshot_simple_prefix
        - fewshot_simple_suffix
        - fewshot_reverse_words
        - fewshot_first_last_swap
        - fewshot_double_letter
        - fewshot_increment
        - fewshot_double
        - fewshot_square
        - distractor_numbers_explicit
        - distractor_words_explicit
        - distractor_implicit_colors
        - distractor_implicit_numbers
        - distractor_typo_numbers
        - distractor_typo_words
        - distractor_semantic_animals
        - distractor_semantic_numbers
        - reason_compare_simple
        - reason_compare_equal
        - reason_transitive_simple
        - reason_transitive_chain
        - reason_negation_simple
        - reason_double_negation
        - reason_conditional_modus_ponens
        - reason_conditional_modus_tollens
        - reason_set_membership
        - reason_set_exclusion
        - math_add_single
        - math_sub_single
        - math_mul_single
        - math_div_single
        - math_add_two_digit
        - math_sub_two_digit
        - math_mul_two_digit
        - math_add_fewshot
        - math_pattern_fewshot
        - math_negative_result
        - math_negative_input
        - seq_arithmetic_add2
        - seq_arithmetic_add5
        - seq_arithmetic_sub3
        - seq_geometric_x2
        - seq_geometric_x3
        - seq_letters_simple
        - seq_letters_skip
        - seq_alternating
        - seq_fibonacci_start
        - fact_capital_france
        - fact_capital_japan
        - fact_water_formula
        - fact_planets_count
        - fact_days_week
        - fact_months_year
        - semantic_synonym_happy
        - semantic_synonym_big
        - semantic_antonym_hot
        - semantic_antonym_up
        - semantic_category_fruit
        - semantic_category_animal
        - semantic_analogy_animal
        - semantic_analogy_size
        - format_json_simple
        - format_json_nested
        - format_table_extract
        - format_csv_parse
        - format_list_first
        - format_list_last
        - format_bracket_complete
        - format_nested_brackets
        - context_early_retrieval
        - context_late_retrieval
        - context_update_value
        - context_multiple_facts
        - context_instruction_first
        - context_instruction_count
        - robust_rephrase_capital
        - robust_rephrase_capital_alt
        - robust_case_upper
        - robust_case_lower
        - robust_whitespace_normal
        - robust_whitespace_extra
        - edge_single_char
        - edge_single_digit
        - edge_zero_result
        - edge_multiply_zero
        - edge_large_add
        - edge_large_compare
        - edge_repeated_copy
        - edge_all_same
        - attention_fine_discrimination
        - attention_subtle_diff
        - attention_focus_noise
        - attention_recent_vs_distant
        - attention_similar_items
        - attention_binding
        - attention_crossref
        - attention_order_first
        - attention_order_last
        - attention_count_interference
        - instruct_return_first
        - instruct_return_last
        - instruct_count_words
        - instruct_uppercase
        - instruct_reverse
        - consist_add_equals
        - consist_add_question
        - consist_add_words
        - consist_copy_arrow
        - consist_copy_colon
        - consist_copy_echo
      dump_attention_max_tokens: 96
      dump_attention_max_heads: 4
      dump_attention_anchor: ""
      dump_attention_paper_dir: research/dba/figs/attention
      dump_attention_paper_tag: dba_10k
    models: [baseline, sem8, gated_sem8]

  - id: behavioral_v2
    config:
      type: behavioral_v2
      tokenizer:
        type: tiktoken
        encoding: gpt2
      seed: 42
      tests_per_category: 30
      max_new_tokens: 32
      context_window: 2048
      stream_live: true
      stream_every: 10
    models: [baseline, sem8, gated_sem8]

  - id: downstream_accuracy
    config:
      type: accuracy
      tasks: [winogrande, arc_easy, boolq, hellaswag]
      tokenizer:
        type: tiktoken
        encoding: gpt2
      context_window: 2048
      stream_live: true
      stream_every: 10
    models: [baseline, sem8, gated_sem8]

  - id: context_sweep
    config:
      type: context
      dataset: ${dataset_path}
      context_lengths: [2048, 4096, 8192, 16384, 32768, 65536, 98304, 131072]
      chunk_size: 1024
      max_mask_elems: 32000000
      batch_size: 1
      decode_len: 128
      decode_warmup: 8
      cache_kind: fp16
      # Slice logits / validate dataset ids against tokenizer vocab.
      valid_vocab_size: 50257
    models: [baseline, sem8, gated_sem8]
# =============================================================================
# TARGETS
# =============================================================================

targets:
  # ===========================================================================
  # PRIMARY: Multi-checkpoint N-way comparison (baseline vs sem8 vs gated_sem8)
  # ===========================================================================
  - type: experiment
    name: multi_checkpoint_compare
    description: "Compare all 3 checkpoints: baseline vs DBA-sem8 vs DBA-sem8 (gated)."
    backend: torch
    task: task.language_modeling
    data:
      ref: dataset.tokens
      config:
        path: ${dataset_path}
        block_size: ${block_size}
    system:
      ref: system.language_model
      config:
        model: *baseline_model
    objective: objective.next_token_ce
    trainer:
      ref: trainer.multi_checkpoint_compare
      config:
        checkpoints:
          - name: baseline
            checkpoint: ${baseline_ckpt}
            model_config: *baseline_model
            is_baseline: true
          - name: sem8
            checkpoint: ${sem8geo32v40_ckpt}
            model_config: *decoupled_sem8_model
          - name: gated_sem8
            checkpoint: ${gated_sem8geo32v40_ckpt}
            model_config: *decoupled_sem8_gated_model
        device: ${device}
        dtype: ${dtype}
        strict: false
        unsafe_pickle_load: false
    runs: *dummy_run
    benchmarks: *multi_benchmark_suite

entrypoints:
  default: "target:multi_checkpoint_compare"
