\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, mathtools}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{float}
\usepackage{placeins}
\usepackage{hyperref}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue
}

\newcommand{\Reals}{\mathbb{R}}

\title{\textbf{Thermodynamic Manifolds}\\
Emergent Grammar and Spectral Synthesis without Backprop\\
\large Working Paper --- January 2026}
\author{(anonymous draft)}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We present a thermodynamic, dynamical system for representation and generation that rejects backpropagation in favor of emergent energy flow.
The system is built from \emph{particles} (inputs) and \emph{attractors} (concepts) coupled by soft, dynamic bonds.
Learning is not a gradient update; it is the persistence of bonds that can metabolically pay for themselves.
We demonstrate a unified pipeline where a semantic manifold produces a next-token prediction and a spectral manifold ``speaks'' the result by diffusing noise into target frequencies.
Outputs are strictly observational: they measure the state without feeding back into the physics.
\end{abstract}

\tableofcontents
\newpage

% ============================================================
\section{System overview}

The system is a physics engine, not a neural network.
At every step, it computes distances between particles and attractors, assigns soft bonds, drifts particles toward attractors, and updates energies.
All parameters (sharpness, decay, noise scale) are derived from the system state rather than fixed constants.

\paragraph{Key objects.}
\begin{itemize}
  \item \textbf{Particles:} transient inputs (token embeddings or frequency bins).
  \item \textbf{Attractors:} persistent concepts or carrier modes.
  \item \textbf{Bonds:} soft assignments between particles and attractors (probabilistic ``gravity'').
\end{itemize}

\paragraph{No backprop.}
There is no supervised loss and no gradient descent.
Structure emerges when energy flow sustains bonds and decays those that cannot pay maintenance.

% ============================================================
\section{Thermodynamic engine}

Let particles have positions $x_i \in \Reals^d$ and attractors have positions $a_k \in \Reals^d$.
The engine computes a distance matrix $D_{ik}$ and converts it to bond weights with an emergent sharpness:
\[
W_{ik} = \softmax(-D_{ik} / \sigma_D),
\qquad \sigma_D = \mathrm{std}(D) + \varepsilon.
\]

Particles drift toward the weighted attractor target:
\[
\hat{x}_i = \sum_k W_{ik} a_k,
\qquad
x_i \leftarrow x_i + \Delta t (\hat{x}_i - x_i) + \Delta t\,\eta,
\]
where $\eta$ is thermal noise whose scale is derived from the current particle dispersion.

\paragraph{Energy update.}
Attractor energy is an exponential moving average of incoming bond mass, with time scale derived from the current energy scale.

% ============================================================
\section{Semantic manifold (grammar)}

Semantic particles represent the current context.
Attractors represent vocabulary concepts.
Grammar emerges as energy flow through a transition matrix that is not explicitly trained; it evolves metabolically based on observed context order.

\paragraph{Ghost field.}
Grammar bias is the flow of energy from the most recent concept through the transition matrix.
This produces a forward-looking field that competes with recency.

\paragraph{Metabolic dominance.}
Context and grammar signals are normalized by their own energy scales.
The system blends them by a dominance ratio derived from those scales, so whichever can pay maintenance dominates the prediction.

% ============================================================
\section{Spectral manifold (synthesis)}

Spectral particles live in frequency space.
Attractors represent target frequencies for a predicted concept.
Noise particles drift toward these targets through a soft-assignment diffusion process.
The resulting particle cloud is interpreted as a generated spectrum.

% ============================================================
\section{Outputs are observational}

Outputs do not participate in dynamics.
The system exposes an \texttt{OutputState} object containing:
\begin{itemize}
  \item semantic logits/probabilities and predicted token,
  \item spectral particle positions and target frequencies,
  \item per-step diagnostic metadata (entropy, dominance).
\end{itemize}

This preserves emergence: readouts measure resonance without shaping it.

% ============================================================
\section{Unified demo results}

All artifacts below are generated by a single command:
\[
\texttt{python3 tmp/rez/paper\_artifacts.py}.
\]
The script writes figures/tables into \texttt{tmp/rez/artifacts/}, which this file includes automatically via \texttt{\textbackslash IfFileExists}.

\IfFileExists{artifacts/thermo_summary_autogen.tex}{
  \input{artifacts/thermo_summary_autogen.tex}
}{
  \textit{(Artifacts not found. Run \texttt{python3 tmp/rez/paper\_artifacts.py}.)}
}

\IfFileExists{artifacts/thermo_entropy_dominance.png}{
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.90\linewidth]{artifacts/thermo_entropy_dominance.png}
    \caption{Entropy and grammar dominance over thinking steps.}
  \end{figure}
}{}

\IfFileExists{artifacts/thermo_audio_hist.png}{
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.90\linewidth]{artifacts/thermo_audio_hist.png}
    \caption{Generated particle frequency distribution with target markers.}
  \end{figure}
}{}

\IfFileExists{artifacts/thermo_spectrogram.png}{
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.90\linewidth]{artifacts/thermo_spectrogram.png}
    \caption{Spectrogram of the synthesized waveform (generated from particle clusters).}
  \end{figure}
}{}

\FloatBarrier

% ============================================================
\section{Limitations and next steps}

This draft uses a minimal demonstration vocabulary and a synthetic spectral synthesis routine.
Future work should scale the active set, incorporate richer semantic embeddings, and generate full audio waveforms with phase-aware synthesis.

\section{Training protocol and continuous learning}

\paragraph{Continuous learning.}
Yes: the system is designed to run continuously.
There is no epoch boundary or backpropagation phase.
As new context arrives, particles are ingested, bonds accumulate energy if they carry flow, and bonds snap if they cannot pay their metabolic cost.
This means the topology can keep adapting online as long as the system is running.

\paragraph{What ``training'' looks like.}
Training is exposure to a stream.
A stream may be a corpus, a sensor feed, or a synthetic generator.
The system updates its bond ledgers at every step; there is no discrete optimization step.
Evaluation is a readout on top of the state (e.g., next-token prediction accuracy), not a loss that drives the physics.

\paragraph{Comparison to Transformer training.}
Transformers (e.g., Llama) minimize cross-entropy via backprop over large batches.
This system has no global loss and no gradient updates.
Instead, learning is an emergent survival process: bonds persist if they are fed by flow and decay otherwise.
This makes the system more like an online, energy-based dynamical model than a static parameter fit.

\end{document}
