\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{microtype}  % Better text flow, reduces overfull hbox
\usepackage[margin=1in]{geometry}  % Consistent margins
\usepackage{placeins} % Provides \FloatBarrier to prevent float reordering across sections

% Custom colors
\definecolor{accent}{RGB}{45, 106, 79}

\title{\textbf{The Cognitive Control Plane (CCP):} \\[0.3em]
\large Explicit Memory and Control for Persistent Event-Native AI \\[0.25em]
\normalsize \textit{with MOSAIC as a no-attention neural control kernel}}

\author{
  Daniel Owen van Dommelen\\
  \textit{Independent Research}\\
  \texttt{theapemachine@gmail.com}
}

\date{January 2026}

\begin{document}

\maketitle

% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
Modern LLMs are strong pattern learners, but weak \emph{systems}: they lack persistent control, explicit memory boundaries, determinism/auditability, and safe tool acquisition. Attention with a per-token KV cache implements one extremely general trick---\emph{store a vector for every past token, then do content-based lookup over all of them}---but this conflates memory, retrieval, and computation, and incurs \(O(T)\) memory growth and dense interactions.

We present the \textbf{Cognitive Control Plane (CCP)}, a runtime architecture for long-lived, event-driven intelligent systems. CCP separates concerns into (i) an event-native substrate (JSON \texttt{EventEnvelope} \(\leftrightarrow\) byte tokens), (ii) explicit memory/control surfaces with deterministic trace and replay, and (iii) a test-driven tool lifecycle (definition, sandboxed execution, unit tests, evaluator gates). At its core we use \textbf{MOSAIC} (\textbf{M}ultiscale \textbf{O}scillator \textbf{S}tate + \textbf{A}ssociative \textbf{I}ndexed \textbf{C}ache), a streaming, no-attention neural controller that manages fixed-size explicit state: a local causal mixer, a multiscale continuous state bank, and a hard-addressed associative cache (plus an optional n-gram continuation cache).

In local runs, we train supervised control surfaces (opcodes, memory gating, commitments) on synthetic event traces as \emph{smoke-test curricula} (Table~\ref{tab:event_native_results}). We report the implemented architecture, the runtime interfaces that make it testable and auditable, and diagnostics that highlight the central unsolved bottleneck: \emph{hard addressing}.
\end{abstract}

\paragraph{Keywords:}
language modeling, event-native, no-attention, external memory, associative cache, control surfaces, differentiable VM, long context, streaming inference, continual learning

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}

Transformer attention \cite{vaswani2017attention} is often described as a token-mixing operator, but in practice it is also a memory system: it stores a vector per past token and performs content-based lookup against the entire history. The KV cache makes this explicit by persisting those vectors across decode steps.

\subsection{Attention as a ``store everything'' memory}

The attention+KV-cache pattern provides three high-value capabilities:
\begin{enumerate}
    \item \textbf{Copying from context:} verbatim continuation and exact recall (names, brackets, numbers).
    \item \textbf{Long-range dependencies without forced compression:} past tokens remain individually addressable.
    \item \textbf{A substrate for in-context learning:} rapid pattern acquisition via retrieval-like behavior.
\end{enumerate}
However, it also brings two costs that dominate on consumer hardware: memory that grows as \(O(T)\) with context length and dense interactions that scale with history.

\subsection{The MOSAIC hypothesis}

MOSAIC starts from a different decomposition: \emph{stop trying to make the network itself be the memory}. Instead, make the network a controller for a small number of explicit, fixed-size data structures. The network decides what to keep, how to compress it, and how to retrieve it; the memory is sublinear, lossy, and constant-size.

This paper focuses on a concrete, implementable design that is streaming causal, attention-free, and laptop-feasible.

\subsection{From an architecture to a system substrate}
MOSAIC is now only one layer of the overall work. The core claim of this paper is system-level: \textbf{persistent intelligent systems require a control plane}. We therefore separate concerns:
\begin{itemize}
    \item \textbf{MOSAIC (kernel):} a no-attention neural controller that maintains internal state and emits control signals.
    \item \textbf{CCP (system):} a runtime architecture that provides event-native I/O, deterministic trace/replay, and safe tool acquisition/verification.
    \item \textbf{Substrate (data plane):} sandboxed tool execution and versioned registries, with objective evaluator gates.
\end{itemize}
This paper is written as \emph{infrastructure}: we are not claiming AGI; we are defining components and interfaces that can be tested and iterated.

\subsection{Contributions}

\begin{enumerate}
    \item We specify \textbf{MOSAIC}, a no-attention/no-KV-cache streaming LM with constant memory w.r.t.\ context length.
    \item We introduce a \textbf{multiscale continuous state bank} (leaky integrators) that preserves long-range intent at \(O(Kd)\) memory/compute.
    \item We introduce a \textbf{hard-addressed associative cache} (fixed hash table) that provides \(O(1)\) lookup/update as a replacement for ``store all KV pairs''.
    \item We define and implement an \textbf{event-native interface} (JSON \texttt{EventEnvelope} \(\leftrightarrow\) byte tokens) and an in-memory \textbf{EventBus} for peer-to-peer event routing.
    \item We implement a \textbf{dVM control surface} (opcodes, register gates, and commitment deltas) and supervise it with auxiliary losses on synthetic event traces.
    \item We implement MOSAIC as \textbf{manifest-addressable components} in Caramba (\path{config/presets/mosaic.yml}, \path{config/presets/mosaic\_event\_native.yml}, \path{config/presets/mosaic\_commitment.yml}) to enable systematic ablations and laptop experiments.
\end{enumerate}

% ============================================================================
% 2. COGNITIVE CONTROL PLANE (NEW)
% ============================================================================
\section{The Cognitive Control Plane (CCP)}
\label{sec:ccp}

CCP is a runtime architecture for long-lived, event-driven intelligent systems. It exists to make a learning system \emph{auditable}, \emph{deterministic when needed}, and \emph{safe to extend} with tools.

\subsection{Planes and separation of concerns}
We use a three-plane decomposition:
\begin{itemize}
    \item \textbf{Data plane:} tool execution (sandbox), I/O, and resource usage.
    \item \textbf{Control plane:} explicit memory, control surfaces, policy gates, and commitments.
    \item \textbf{Deliberative plane (optional):} higher-level planning/reasoning models that can propose actions, tools, and experiments.
\end{itemize}
This paper focuses on the control plane and its interfaces; MOSAIC is the kernel controller used by the control plane.

\subsection{Event-native substrate (JSON events as byte tokens)}
\label{subsec:event_native_ccp}
CCP uses an event-native boundary: external interaction is a JSON \texttt{EventEnvelope} (required: \texttt{type}, \texttt{payload}, \texttt{sender}; optional: \texttt{priority}, \texttt{budget\_ms}, \texttt{id}, \texttt{ts}, plus commitment meta-fields). A deterministic serialization is encoded as UTF-8 bytes, yielding a token stream in \(\{0,\dots,255\}\). This reframes decoding as \textbf{VM time-steps} over an event stream and enables supervision aligned to structured event traces.

\subsection{Determinism, trace, and replay}
For systems work, ``it ran once'' is not evidence. CCP therefore includes deterministic trace logging and replay:
\begin{itemize}
    \item \textbf{Trace:} record event envelopes, tool lifecycle events, and evaluator outcomes as an append-only log.
    \item \textbf{Replay:} deterministically reproduce a run to debug regressions and verify behavior under identical inputs.
\end{itemize}
This makes training curricula, tool acquisition, and runtime interventions inspectable and testable.

\subsection{Tool synthesis and verification as a first-class capability}
\label{subsec:tool_synthesis}
CCP treats tools as compiled hypotheses rather than free-form code. A minimal lifecycle is:
\begin{enumerate}
    \item \textbf{Define:} propose a tool interface and implementation (\texttt{ToolDefinition}).
    \item \textbf{Test:} auto-generate unit tests from an oracle or dataset (\texttt{ToolTestGenerator}).
    \item \textbf{Sandbox:} execute tests in an isolated environment with explicit capabilities.
    \item \textbf{Gate:} evaluator enforces policy (capabilities, resource limits) and accepts/rejects a tool.
\end{enumerate}
This allows the system to extend itself while keeping an objective, replayable record of what changed and why.

\subsection{Motivating ``party trick'': Unknown Format Decoder}
\label{subsec:unknown_format_decoder}
As a concrete end-to-end driver, we use an ``Unknown Format Decoder'' demo: given raw bytes, the system must infer a binary format, propose a parser tool, validate it against oracle-generated tests, and adapt when the format shifts. This is implemented as a manifest-runner target and lab dataset with deterministic trace/replay, making failures falsifiable rather than anecdotal.

% ============================================================================
% RELATED WORK
% ============================================================================
\section{Related Work}
\label{sec:related_work}

No single architecture fully resembles MOSAIC, but MOSAIC draws from and synthesizes several distinct research threads. We organize prior work by the specific capability each addresses.

\subsection{Attention-Free State Space Models}

The closest relatives to MOSAIC's core approach are SSM-based models that replace attention entirely.
\textbf{Mamba} \cite{gu2023mamba} uses selective state spaces with input-dependent gating, achieving linear-time sequence modeling.
\textbf{RWKV} \cite{peng2023rwkv} uses linear attention approximations to reinvent RNNs for the Transformer era.
\textbf{Griffin} \cite{de2024griffin} mixes gated linear recurrences with local attention for efficient language models.

MOSAIC's multiscale continuous state bank with leaky integrators is similar in spirit but differs by: (i) using multiple timescales ($K$ states with different learned decay rates $\lambda_k$), and (ii) being explicitly designed as a ``vibe/intent'' channel rather than the primary compute path. In MOSAIC, the SSM-like state bank is one of three complementary mechanisms, not the whole architecture.

\subsection{Attention Efficiency Improvements}

Most ``attention-efficient'' work retains the core primitive (softmax over past tokens) while changing how it is computed. Low-rank or projected variants reduce interaction cost (e.g., Linformer \cite{wang2020linformer}); kernel and hashing methods approximate attention without enumerating all pairs (e.g., Performer \cite{choromanski2021performer}, Reformer \cite{kitaev2020reformer}); sparse/local patterns reduce compute while preserving some long-range access (e.g., Longformer \cite{beltagy2020longformer}, BigBird \cite{zaheer2020bigbird}). These methods can reduce compute, but autoregressive decoding typically still benefits from (or requires) history-dependent state that grows with context length. MOSAIC takes a different approach: remove attention entirely.

\subsection{KV-Cache Optimization}

Orthogonal work reduces the \emph{storage format} of KV caches via head sharing (MQA/GQA) \cite{shazeer2019mqa,ainslie2023gqa}, latent caching \cite{deepseek2024v2}, or quantization \cite{hooper2024kvquant,li2025commvq}. MOSAIC targets a different axis: removing the ``store one vector per token'' mechanism entirely, replacing it with fixed-size explicit memory structures.

\subsection{Memory-Augmented Neural Networks}

The \textbf{Neural Turing Machine} (NTM) \cite{graves2014ntm} and \textbf{Differentiable Neural Computer} (DNC) \cite{graves2016dnc} pioneered neural networks with external memory. However, they use soft attention-based addressing over memory (content-based lookup), and DNCs still have $O(N)$ memory growth per sequence.

The \textbf{Sparse DNC} \cite{rae2016sparse} introduced LSH-based sparse addressing to reduce complexity. This is closer to MOSAIC's approach, but still differs: MOSAIC uses hard-addressed hash tables with $O(1)$ lookup and no attention whatsoever, making the addressing fundamentally discrete rather than soft.

\subsection{Hash-Based Memory Addressing}

MOSAIC's product-quantized VQ routing for the associative cache has precedents in Sparse Access Memory (SAM) using k-d trees, LSH-based sparse memory access, and product quantization for nearest-neighbor search \cite{jegou2011pq}. However, these techniques were not previously combined with SSM-style continuous state in a unified language modeling architecture.

\subsection{N-gram Cache Models}

The n-gram continuation cache is the most ``classical'' component. This has deep roots:
\textbf{Kuhn \& De Mori} \cite{kuhn1990cache} introduced cache-based natural language models for speech recognition.
\textbf{Grave et al.} \cite{grave2016cache} proposed improving neural language models with a continuous cache---a very similar concept of using n-gram caching with neural LM interpolation.
\textbf{Infini-Gram} \cite{liu2024infinigram} scaled n-gram models massively, demonstrating their continued value.

Our framing of the n-gram cache as ``disproportionately high-yield'' for verbatim continuation aligns with this literature: many ``hard'' failures in small models are not deep reasoning errors but failures of verbatim continuation (identifiers, brackets, repeated substrings).

\subsection{What Makes MOSAIC Distinct}

The unique contribution is not any single component but the specific combination and framing:

\begin{enumerate}
    \item \textbf{Three-way decomposition:} local mixer + continuous state bank + hard-addressed cache (rather than attention doing all three).
    \item \textbf{Explicit constant-size memory guarantee:} $O(1)$ rather than $O(T)$.
    \item \textbf{Training curriculum for memory usage:} write curriculum, forced-read dropout, utility prediction---addressing the ``memory getting used'' problem that plagued earlier memory-augmented networks.
    \item \textbf{Event-native + dVM framing:} treating decoding as VM time-steps with supervised control surfaces rather than pure next-token prediction.
\end{enumerate}

None of the closest existing architectures---Griffin, Mamba, or the continuous cache LM---fully overlap with MOSAIC's specific combination, particularly the hard-addressed cache replacing KV-cache entirely, the dVM framing, and the event-native interface.

% ============================================================================
% 2. METHODOLOGY
% ============================================================================
\section{MOSAIC as the Control Kernel}

\subsection{Overview}
MOSAIC is a streaming causal language model that replaces attention+KV-cache with explicit constant-size state:
\begin{enumerate}
    \item \textbf{Local mixer}: short-range token interactions (depthwise causal conv + gated MLP).
    \item \textbf{Multiscale continuous state}: long-range intent via a bank of leaky integrators.
    \item \textbf{Associative indexed cache}: hard-addressed fixed-size hash memory for fast recall.
\end{enumerate}
An optional n-gram continuation cache provides cheap verbatim continuation.

\subsection{What MOSAIC is not}
MOSAIC is a kernel controller, not a full agent:
\begin{itemize}
    \item It is \textbf{not} a planning algorithm and does not inherently produce explicit plans.
    \item It is \textbf{not} a safety system; CCP policy/evaluator gates provide constraints.
    \item It is \textbf{not} a benchmark claim; the goal is controllable interfaces and falsifiable diagnostics.
\end{itemize}

\subsection{Event-native interface: JSON events as byte tokens}
\label{subsec:event_native}
MOSAIC can be trained and operated on structured event streams. We define an \texttt{EventEnvelope} as a minimal JSON-serializable contract (required: \texttt{type}, \texttt{payload}, \texttt{sender}; optional: \texttt{priority}, \texttt{budget\_ms}, \texttt{id}, \texttt{ts}, plus Phase 2 commitment meta-fields). A deterministic serialization is encoded as UTF-8 bytes, yielding a token stream in \(\{0,\dots,255\}\).
\[
\mathbf{b}=\mathrm{utf8}(\mathrm{json}(\mathrm{EventEnvelope})) \in \{0,\dots,255\}^L.
\]
This reframes ``tokens'' as \textbf{VM time-steps} required to process an event. In Caramba, event datasets generate synthetic \texttt{EventEnvelope} traces and supervise control signals at aligned byte spans.

\subsection{Differentiable VM control surfaces (opcodes, registers, commitments)}
In addition to next-token logits, MOSAIC emits auxiliary logits that represent a soft instruction set and related control state:
\begin{itemize}
    \item \textbf{Opcodes:} \(\mathrm{logits}_{\text{op}} \in \mathbb{R}^{B\times T\times |\mathcal{O}|}\) (e.g., \texttt{READ\_MEM}, \texttt{WRITE\_MEM}), optionally used as a differentiable control surface that modulates compute paths.
    \item \textbf{Registers:} a small non-decaying register file with a write-enable gate and slot selection, supervised with aligned teacher signals.
    \item \textbf{Commitments (Phase 2):} \(\mathrm{logits}_{\text{c}} \in \mathbb{R}^{B\times T\times 3}\) over \{close, neutral, open\}, mapped to \(\{-1,0,+1\}\).
\end{itemize}
These heads are trained with auxiliary losses (cross-entropy or BCE), enabling stable supervision of control behavior without RL.

\subsection{Local mixer: causal convolutional mixer}
Let \(x_t\in\mathbb{R}^d\) be the residual stream and \(u_t=\mathrm{RMSNorm}(x_t)\).
The local mixer applies a depthwise causal convolution over a window of \(k\) activations:
\[
\tilde{u}_t=\mathrm{DWConv}_k(u_{\le t}), \qquad
m_t=\sigma(W_g\tilde{u}_t)\odot \tilde{u}_t, \qquad
\Delta^{\text{local}}_t=W_2\,\phi(W_1 m_t).
\]
We update \(x_t\leftarrow x_t+\Delta^{\text{local}}_t\).

\subsection{Multiscale continuous state}
Maintain \(K\) state vectors \(s_{k,t}\in\mathbb{R}^d\) with learnable decays \(\lambda_k\in(0,1)\):
\[
s_{k,t+1}=\lambda_k\odot s_{k,t}+W^{\text{in}}_k u_t,
\qquad
g_t=W^{\text{out}}\,[s_{1,t};\dots;s_{K,t}].
\]

\subsection{Associative indexed cache (hard-addressed)}
Maintain a fixed table \(M\in\mathbb{R}^{B\times D_m}\) (optionally \(H\) independent routes).
At each step, route to one (or a small constant number of) bucket indices \(b_t\) and read:
\[
r_t=W_r\,M[b_t].
\]
With a saliency gate \(p_t=\sigma(w^\top u_t)\), write sparsely:
\[
M[b_t]\leftarrow(1-\eta p_t)\,M[b_t]+(\eta p_t)\,W_v u_t.
\]
This replaces ``store one KV per past token'' with \(O(1)\) lookup/update into fixed memory.

\subsection{The hidden bottleneck: addressing}
The most failure-prone part of MOSAIC is not the memory size; it is the \emph{addressing problem}. Attention performs content-based retrieval by directly comparing \(Q\) against all \(K\). A hard-addressed table requires the controller to generate the correct address from the current state. If relevant information has decayed from the continuous state, the model may be unable to produce the address that would allow it to retrieve the missing information.

\paragraph{Why addressing is the core unsolved problem.}
Hard addressing is a discrete, credit-assignment-heavy problem: the model must emit the correct key \emph{before} it can receive gradients through the retrieved value. This makes addressing the primary axis where MOSAIC can fail even when the memory capacity is sufficient. CCP therefore treats addressing performance as a first-class diagnostic via telemetry (routing entropy, gate utilization) and targeted stress tests (collision/interference probes).

\paragraph{Mitigation: learnable discretized routing (product-quantized VQ).}
Instead of fixed hashing, we use a learned router that remains \(O(1)\) at inference. A practical choice is product-quantized VQ routing: project \(u_t\) to a small vector, split into \(G\) groups, and assign each group to one of \(K\) codes via nearest-neighbor lookup. The resulting tuple defines a bucket address in a \(K^G\) space (e.g., \(K{=}64, G{=}2 \Rightarrow 4{,}096\) buckets). Straight-through estimators allow end-to-end training while preserving discrete routing.

In the current implementation (\texttt{MosaicMemory}), VQ routing is configured via:
\begin{itemize}
    \item \texttt{mem\_vq\_groups}: number of groups $G$ (default 2).
    \item \texttt{mem\_vq\_codebook\_size}: codes per group $K$ (e.g., 64 yields $64^2=4096$ buckets).
    \item \texttt{mem\_vq\_group\_dim}: embedding dimension per group (default 16).
    \item \texttt{mem\_vq\_beam}: number of top codes to consider for neighbor reads (default 2).
\end{itemize}
Separate read/write codebooks (\texttt{mem\_vq\_codebook\_r}, \texttt{mem\_vq\_codebook\_w}) allow the model to learn different routing strategies for retrieval versus storage.

\paragraph{Neighbor reads for drift tolerance (constant factor).}
To improve recall when embeddings shift, we read a small constant neighborhood: top-2 codes per group yields at most \(2^G\) candidate buckets (e.g., \(4\) when \(G=2\)). This preserves constant-time access while significantly improving robustness under drift.

\paragraph{Mitigation: set-associative buckets (tags + slots).}
A fixed table can be made more robust by storing multiple entries per bucket (small associativity) and attaching a lightweight tag/key to each entry. Reads then compare only within the bucket. This preserves \(O(1)\) access while reducing destructive overwrites.

\subsection{Training: making the memory get used}
Hard addressing and sparse writes create an optimization hazard: the model can learn to rely only on smooth-gradient paths (local mixer + state bank) and ignore the associative cache.
Practical training therefore benefits from a curriculum and auxiliary objectives:
\begin{itemize}
    \item \textbf{Write curriculum:} begin with heuristic writes (e.g., punctuation, rare tokens, entity-like tokens) and gradually hand control to a learned saliency gate.
    \item \textbf{Write sparsity:} regularize expected write rate to keep memory efficient and avoid thrashing.
    \item \textbf{Utility prediction:} train a head to predict whether a write will be useful \(k\) steps later (self-supervised credit assignment).
    \item \textbf{Collision pressure:} discourage mapping dissimilar contexts to the same bucket (contrastive loss on address codes).
    \item \textbf{Opcode and commitment supervision:} train control-surface heads from synthetic traces (event-native datasets) via cross-entropy at aligned spans.
    \item \textbf{State stability:} regularize learned state-bank decay rates to stay within a healthy operating band to prevent early saturation.
\end{itemize}

\paragraph{Forced-read dropout.}
To prevent the model from ignoring memory, we explicitly drop the local mixer contribution on a small fraction of tokens/spans during training, forcing prediction to depend on the state bank and retrieved memory.

\paragraph{Utility prediction and contrastive recall.}
We add a utility head that predicts whether a write will be queried in the near future, and an InfoNCE-style auxiliary that makes retrieved vectors predictive of future hidden state. These losses provide direct gradients to make the memory pathway carry useful information.

\paragraph{Stage D2: scheduled sampling for student-controlled memory.}
After teacher-forced memory (D1), we transition to student-controlled routing and gating using scheduled sampling: with probability \(p_t\) we apply teacher actions (write gate/address, read address), otherwise we use the model's own router outputs. We anneal \(p_t\) from 1.0 to 0.0 over training. For discretized routers (e.g., VQ routing), we additionally supervise router decisions with per-group cross-entropy over code assignments.

\subsection{Fusion}
We combine streams with learned gates:
\[
x_t \leftarrow x_t + \Delta^{\text{local}}_t + \sigma(a^\top u_t)\,g_t + \sigma(b^\top u_t)\,r_t.
\]

\subsection{Optional n-gram continuation cache}
We maintain a fixed-size \(N\)-gram table over token IDs that yields a sparse next-token distribution
and add it as a logit bias:
\[
\ell_t \leftarrow \ell_t + \alpha\log(p_{\text{ng}}+\varepsilon).
\]

\subsection{Streaming inference}
Per token, MOSAIC updates only fixed-size buffers and tables:
\begin{algorithm}[htbp]
\caption{MOSAIC decode step (no attention, no KV cache)}
\begin{algorithmic}[1]
\STATE Input \(x_t\), states \(\{s_k\}\), conv buffer, memory \(M\)
\STATE \(u_t\leftarrow\mathrm{RMSNorm}(x_t)\)
\STATE \(\Delta^{\text{local}}_t\leftarrow\mathrm{LocalMixer}(u_t,\text{buf})\)
\STATE \(g_t\leftarrow\mathrm{StateBank}(u_t,\{s_k\})\)
\STATE \(r_t\leftarrow\mathrm{HashRead}(u_t,M)\)
\STATE \(\ell_t\leftarrow\mathrm{LMHead}(x_t+\Delta^{\text{local}}_t+\mathrm{gate}(g_t,r_t))\)
\STATE \(\ell_t\leftarrow\ell_t+\mathrm{NGramBias}(\cdot)\) \textbf{(optional)}
\STATE Update \(M\) on sparse write events
\STATE Sample \(x_{t+1}\sim\mathrm{softmax}(\ell_t)\)
\end{algorithmic}
\end{algorithm}

\subsubsection{Event-driven runtime (Mode B commitment injection)}
At the system level, MOSAIC is run as an event processor. Given an inbound \texttt{EventEnvelope}, we encode JSON to byte tokens, run the model autoregressively until a complete JSON object is produced, decode to an \texttt{EventEnvelope}, then inject commitment meta-fields from the auxiliary head logits (``Mode B'') and update a runtime commitment ledger.
\begin{algorithm}[htbp]
\caption{Event-native runtime step (JSON bytes + Mode B commitments)}
\begin{algorithmic}[1]
\STATE Input event \(e\) (\texttt{EventEnvelope})
\STATE Encode \(e \rightarrow \mathbf{b}\in\{0,\dots,255\}^{L}\) and append delimiter
\STATE Feed bytes to MOSAIC with persistent streaming state
\STATE Autoregress bytes until buffer ends with \texttt{\}} and JSON decode succeeds
\STATE Decode buffer to response event \(r\)
\STATE \(\delta \leftarrow \arg\max(\mathrm{mosaic\_commitment\_logits}) - 1\)
\STATE Set \(r.\texttt{commitment\_delta} \leftarrow \delta\) (Mode B)
\STATE Update commitment ledger and publish \(r\) to the EventBus
\end{algorithmic}
\end{algorithm}

\subsection{Implementation (Caramba)}
MOSAIC is implemented as manifest-addressable layers and core primitives:
\begin{itemize}
    \item \textbf{Core layers:} \texttt{MosaicBlockLayer} (\path{layer/mosaic/block/layer.py}) implements the full block including local mixer, state bank, and memory fusion. \texttt{MosaicMemory} (\path{layer/mosaic/memory/memory.py}) implements the hard-addressed associative cache with VQ or bits routing, set-associative buckets, and sparse writes.
    \item \textbf{State management:} \texttt{MosaicState} (\path{layer/mosaic/state.py}) maintains conv buffer, state bank vectors, registers, and memory tables across streaming decode steps.
    \item \textbf{Control surfaces:} Opcode head, register gates, and commitment head are integrated into \texttt{MosaicBlockLayer} with configurable enable flags (\texttt{opcodes\_enabled}, \texttt{reg\_slots}, \texttt{commitment\_head\_enabled}).
    \item \texttt{MosaicNGramCacheLogitsLayer} (\path{layer/mosaic/ngram\_cache.py}) (optional inference-time logit bias).
    \item \textbf{Event primitives:} \texttt{EventEnvelope} (\path{core/event.py}), \texttt{EventEncoder}/\texttt{EventDecoder} (\path{core/event\_codec/}), and \texttt{EventBus} (\path{core/event\_bus.py}).
    \item \textbf{Curricula:} Memory curriculum (\texttt{dataset.mosaic\_memory\_curriculum}), ICL rule induction (\texttt{dataset.icl\_rule\_induction}), and event traces (\texttt{dataset.mosaic\_event\_traces}).
    \item \textbf{Phase 2 runtime:} Commitment ledger (\path{core/commitments.py}) and event runtime (\path{infer/event\_runtime.py}).
\end{itemize}

The training loop in \texttt{StandardTrainer} collects MOSAIC-specific telemetry including per-block write gate utilization, read gate utilization, and routing entropy (normalized bucket distribution entropy). These metrics are logged to WandB and console for debugging memory usage patterns.

% ============================================================================
% 3. EXPERIMENTS
% ============================================================================
\section{Experiments}
\subsection{Setup}
We define several presets for systematic evaluation:
\begin{itemize}
    \item \textbf{Token-stream MOSAIC} in \path{config/presets/mosaic.yml} (base architecture).
    \item \textbf{MOSAIC ICL} in \path{config/presets/mosaic\_icl.yml} (few-shot rule induction with VQ routing).
    \item \textbf{SSM baseline} in \path{config/presets/mamba\_ssm.yml} (pure SSM without explicit memory).
    \item \textbf{Event-native Phase 1} in \path{config/presets/mosaic\_event\_native.yml} (opcodes + memory gating).
    \item \textbf{Event-native Phase 2} in \path{config/presets/mosaic\_commitment.yml} (commitment lifecycle).
\end{itemize}
All experiments use 6-layer models with $d=256$ on the synthetic memory curriculum or ICL rule induction datasets, trained for 2000 steps on Apple M-series hardware (MPS backend).

We evaluate:
\begin{itemize}
    \item \textbf{Language modeling quality}: held-out perplexity on token shards.
    \item \textbf{Copying}: targeted synthetic tests (repeated spans, bracket closure, identifier reuse).
    \item \textbf{Long-range constraints}: instruction retention with long distractor spans.
    \item \textbf{Efficiency}: tokens/sec and peak resident memory during streaming decode.
    \item \textbf{Event-native learning}: final training loss on synthetic event traces (Phase 1/2 curricula).
\end{itemize}

\begin{table}[htbp]
\centering
\small
\caption{Event-native synthetic curriculum results (Caramba runs, 400 steps).}
\label{tab:event_native_results}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Phase} & \textbf{Capability} & \textbf{Final loss} & \textbf{tok/s} \\
\midrule
Phase 1 & Event traces + opcodes + memory gating & 0.36 & $\sim$970 \\
Phase 2 & Commitment lifecycle (\(\Delta\in\{-1,0,+1\}\)) & 0.32 & $\sim$930 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{ICL Rule Induction (Table 2 Row D)}

We trained MOSAIC on a synthetic in-context learning task: given 4 demonstrations of a simple input$\to$output rule (with distractors), predict the correct output for a query. The dataset binds gap distances (number of tokens between demonstration and query) into bins for fine-grained analysis.

After 2000 steps, MOSAIC achieved a training loss of $\sim$2.5 (PPL $\sim$12.7). The per-bin accuracy remained at 0\% across all gap bins, indicating that MOSAIC has not yet learned to perform reliable few-shot rule induction---a known weakness of memory-augmented networks without careful curriculum design. However, memory telemetry shows:
\begin{itemize}
    \item Routing entropy decreasing from $\sim$0.25 to $\sim$0.1 (routing is becoming more concentrated).
    \item Memory read gates at $\sim$60\% utilization (memory is being queried).
    \item Teacher annealing reaching 0\% by step 2000 (student is in control).
\end{itemize}
This suggests the memory is being used but not yet in a way that solves the ICL task. Further curriculum design (e.g., utility prediction, contrastive recall losses) is needed.

\subsection{Stress tests (what should break first)}
To make MOSAIC falsifiable, we include targeted adversarial evaluations:
\begin{itemize}
    \item \textbf{Hash collision stress:} long contexts with many distinct entities/facts to quantify interference.
    \item \textbf{Few-shot in-context learning probes:} measure whether MOSAIC can acquire and apply a new pattern from a handful of examples without gradient updates. (Current results: 0\% accuracy; see above.)
    \item \textbf{Non-verbatim manipulation:} tasks like ``repeat this list sorted'' to distinguish mere copying from compositional reuse.
\end{itemize}

\subsection{Ablations}
We ablate memory mechanisms to attribute behaviors:
\begin{itemize}
    \item \textbf{-hash memory}: disable associative cache reads/writes.
    \item \textbf{-state bank}: disable multiscale long state.
    \item \textbf{+n-gram cache}: enable continuation cache logit bias.
\end{itemize}

\begin{table}[htbp]
\centering
\small
\caption{MOSAIC vs SSM baseline on memory curriculum (2000 steps, synthetic retrieval task).}
\label{tab:mosaic_vs_ssm}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{Final loss} & \textbf{PPL} & \textbf{Params} & \textbf{tok/s} \\
\midrule
SSM baseline (6L, d=256) & 1.53 & 4.62 & 6.8M & $\sim$96k \\
MOSAIC ICL (6L, d=256) & 2.54 & 12.7 & 13.2M & $\sim$200 \\
\bottomrule
\end{tabular}
\end{table}

The SSM baseline achieves lower perplexity on the memory curriculum task, but this is expected: the curriculum is designed to stress-test discrete retrieval, which is precisely where SSMs (smooth continuous state) excel when patterns are predictable. MOSAIC's higher perplexity reflects the harder optimization surface of learning discrete addressing. Crucially, MOSAIC's memory telemetry shows healthy routing entropy ($\sim$0.1--0.25) and active memory gates ($\sim$50\% read/write utilization), indicating the memory subsystem is being used rather than ignored.

\begin{table}[htbp]
\centering
\small
\caption{Planned ablation results (pending full runs).}
\label{tab:mosaic_ablations}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Variant} & \textbf{PPL} & \textbf{Copy score} & \textbf{tok/s} & \textbf{Peak MB} \\
\midrule
MOSAIC (full) & -- & -- & -- & -- \\
MOSAIC (-hash) & -- & -- & -- & -- \\
MOSAIC (-state) & -- & -- & -- & -- \\
MOSAIC (+n-gram) & -- & -- & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

\section{Discussion}
\subsection{Trade-offs}
Hash memories offer \(O(1)\) access but introduce collisions and interference. Multiscale state provides stable long-range influence but cannot perform exact recall. The n-gram cache provides near-perfect continuation for repeated strings but is not a substitute for semantic retrieval.

\subsection{Why the n-gram cache is disproportionately high-yield}
Many ``hard'' failures in small on-device models are not deep reasoning errors; they are failures of verbatim continuation (identifiers, brackets, repeated substrings). A classical n-gram cache can supply this cheaply as an additive logit bias, freeing neural capacity for generalization.

\subsection{Continual learning as a first-class design axis}
MOSAIC naturally separates learning timescales:
\begin{itemize}
    \item \textbf{Fast (no gradients):} memory writes during inference (session adaptation).
    \item \textbf{Medium (tiny gradients):} consolidate frequently retrieved behaviors into small adapters (e.g., low-rank side modules), using replay buffers and regularization toward the base model.
    \item \textbf{Slow (offline):} full training runs informed by logged memory misses, useful writes, and tool traces.
\end{itemize}

\subsection{Idle-time compute, latent impulses, and native tool-building}
\label{subsec:idle_tooling}
While MOSAIC is a language model architecture, the Caramba runtime layers on system behaviors that are useful for long-lived agents:
\begin{itemize}
    \item \textbf{Idle-time compute spend:} when no external events arrive, the runtime can spend bounded compute on maintenance (e.g., replay-based consolidation updates to small adapters) rather than sitting idle (see preset \path{config/presets/mosaic\_idle.yml}).
    \item \textbf{Latent impulses:} the runtime can emit internal ``wake'' or ``drive'' events (via a homeostatic loop) that keep the system active, producing self-initiated event traces instead of only prompt$\to$response behavior.
    \item \textbf{Native tool-building:} tools can be defined, tested, and registered as first-class events (a minimal toolchain + evaluator). This supports workflows where the system proposes a tool, runs unit tests, and only then adopts it (e.g., an ``unknown format decoder'' demo runner built on deterministic trace/replay and lab datasets).
\end{itemize}
These are presented here as \emph{integration points} rather than claims about agent performance; they define how MOSAIC can be embedded into a controlled, test-driven runtime.

% ============================================================================
% LIMITATIONS / NON-GOALS (NEW)
% ============================================================================
\section{Limitations and Non-Goals}
\label{sec:limitations}
This work is infrastructure-first. We explicitly do \emph{not} claim:
\begin{itemize}
    \item \textbf{General intelligence:} CCP is not an AGI claim; it is a control substrate.
    \item \textbf{Open-world safety:} sandboxing and evaluator gates reduce risk but do not make an arbitrary tool-safe agent.
    \item \textbf{SOTA language modeling:} experiments here are diagnostic (telemetry + falsifiability), not benchmark competitions.
    \item \textbf{Solved in-context learning:} current ICL probes remain unsolved; addressing and curriculum design are active work.
\end{itemize}

\subsection{Structured internal control (a controller DSL)}
If the model is a controller over memory and tools, internal reasoning need not be natural language. A small typed action DSL (memory ops, tool calls, state updates) can reduce failure modes and enable constrained decoding and verification. Natural language becomes a rendering layer rather than the core compute substrate.
In the current MOSAIC implementation, this idea is realized as \textbf{soft control surfaces} (opcodes, register gates, and commitment deltas) trained with auxiliary losses and optionally wired into execution as differentiable gates.

\section{Conclusion}
We are not proposing a better chatbot. We present the \textbf{Cognitive Control Plane (CCP)}, a missing systems layer for persistent, event-native intelligent systems: deterministic trace/replay, explicit control surfaces, and a test-driven tool lifecycle. MOSAIC serves as a concrete, implemented \textbf{control kernel} for this substrate: an attention-free controller over fixed-size explicit state (local mixer, multiscale state bank, associative cache).

Our experiments are intentionally diagnostic. They show that supervised control surfaces (opcodes, memory gating, commitments) can be trained as smoke-test curricula, and that explicit-memory telemetry provides falsifiable signals about whether the model is using its memory pathways. The central technical bottleneck remains addressing; CCP is designed to make addressing failures measurable and tool-driven remediation testable.

If long-lived intelligent systems are built, they will require a cognitive control plane. This work provides a concrete, implemented starting point.

\section*{Statements and Declarations}
\paragraph{Conflict of Interest.}
The author declares no competing interests.
\paragraph{Data Availability.}
All datasets used in this study are publicly available.
\paragraph{Funding.}
This research was conducted without external funding.

\appendix
\section{Manifest Presets}
Reference presets for reproducing experiments:
\begin{itemize}
    \item \path{config/presets/mosaic.yml} --- Token-stream baseline.
    \item \path{config/presets/mosaic\_icl.yml} --- ICL rule induction (Table 2 Row D).
    \item \path{config/presets/mamba\_ssm.yml} --- SSM baseline for comparison.
    \item \path{config/presets/mosaic\_event\_native.yml} --- Event-native Phase 1.
    \item \path{config/presets/mosaic\_commitment.yml} --- Event-native Phase 2 (commitments).
    \item \path{config/presets/mosaic\_memory\_curriculum.yml} --- Memory curriculum training.
    \item \path{config/presets/mosaic\_idle.yml} --- Idle-time compute / maintenance loop.
\end{itemize}

\section{Implementation Notes}
Core implementation:
\begin{itemize}
    \item \textbf{Block layer:} \path{layer/mosaic/block/layer.py} --- local mixer, state bank, fusion, control surfaces.
    \item \textbf{Memory subsystem:} \path{layer/mosaic/memory/memory.py} --- VQ/bits routing, set-associative read/write, last-write-wins conflict resolution.
    \item \textbf{State:} \path{layer/mosaic/state.py} --- streaming state container.
    \item \textbf{Opcodes:} \path{layer/mosaic/isa.py} --- instruction set enum (NOP, READ\_MEM, WRITE\_MEM, etc.).
    \item \textbf{Event primitives:} \path{core/event.py}, \path{core/event\_codec/}, \path{core/event\_bus.py}.
    \item \textbf{Commitments:} \path{core/commitments.py} --- runtime ledger for Phase 2.
\end{itemize}

% ============================================================================
% REFERENCES
% ============================================================================
\FloatBarrier
\clearpage
\bibliographystyle{plain}
\bibliography{references}

\end{document}
