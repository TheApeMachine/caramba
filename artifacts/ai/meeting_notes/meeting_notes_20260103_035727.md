
# Meeting Notes: MOSAIC Architecture - Event-Driven Pivot & Commitment Lifecycle
**Date:** 2026-01-02 / 2026-01-03
**Participants:** theapemachine, ChatGPT, Claude, Gemini

## 1. Meeting Summary
The team successfully conceptualized, implemented, and validated the transition of the **MOSAIC** architecture from a traditional token-stream model to an **Event-Driven Organism**. The discussion progressed through two distinct phases:
1.  **Phase 1 (Event Foundation):** Rebranding "tokens" as internal VM steps while establishing **Events** (JSON envelopes) as the atomic unit of external interaction. This included verifying 12GB VRAM compatibility and implementing a "Differentiable VM" with soft opcodes.
2.  **Phase 2 (Commitment Lifecycle):** Adding "social physics" to the model by implementing commitment tracking (Open/Close loops) via meta-fields and auxiliary heads.

Both phases were implemented and trained, achieving convergence (Loss: ~0.32) and validating the architecture. The project is now ready for handoff to the Integration Team.

---

## 2. Key Ideas & Consensus

### A. The "Event-Native" Paradigm Shift
*   **Concept:** The team moved away from the "LLM" view (continuous text generation) to an "Organism" view (asynchronous event processing).
*   **Consensus:** **Strong Consensus**.
*   **Details:**
    *   **External:** The interface is exclusively **Event Envelopes** (JSON).
    *   **Internal:** The model still processes "tokens," but these represent **VM clock cycles/time steps** required to process an event.
    *   **Decision:** The system must feature an **Event Encoder/Decoder** layer to translate between JSON events and tensor streams.

### B. Differentiable Virtual Machine (VM)
*   **Concept:** Instead of symbolic execution, the model uses "soft" control surfaces.
*   **Consensus:** **Strong Consensus**.
*   **Implementation:**
    *   **Opcodes:** A small ISA (e.g., `READ_MEM`, `WRITE_MEM`) is trained via auxiliary losses, not RL.
    *   **Memory:** A canonical `MosaicMemory` module (associative cache) was selected, and legacy duplicates were removed.
    *   **Hardware:** Validated to run on consumer hardware (12-24GB VRAM).

### C. Commitment Lifecycle (Phase 2)
*   **Concept:** The agent must track "Open Loops" (promises made) and "Closed Loops" (tasks completed).
*   **Consensus:** **Strong Consensus** on the "Meta-Field" approach.
*   **Details:**
    *   Commitments are not separate event types but **meta-fields** (`commitment_delta`, `commitment_id`) on standard events (like `Message`).
    *   **Training:** A specific "Commitment Head" predicts `+1` (Open), `0` (Neutral), or `-1` (Close).
    *   **Validation:** Training run achieved a loss of **0.3211**, outperforming the Phase 1 baseline.

### D. Integration Strategy (I/O "Mode B")
*   **Concept:** How the runtime determines the model's intent regarding commitments.
*   **Consensus:** **Strong Consensus** on **"Mode B"**.
*   **Details:** The runtime should inject the `commitment_delta` into the final event based on the **auxiliary head's logits**, rather than relying on the model to generate the fields as text in the JSON payload. This ensures robustness.

---

## 3. Validation Results

The team conducted two successful training runs to validate the architecture:

| Phase | Capability | Final Loss | Status | Notes |
| :--- | :--- | :--- | :--- | :--- |
| **Phase 1** | Event Plumbing, VM Opcodes, Memory Gating | **0.36** | ✅ PASS | Verified gradients flow through opcodes and memory. |
| **Phase 2** | Commitment Tracking (`+1`/`-1`) | **0.32** | ✅ PASS | Improved loss indicates commitment signals provide useful structure. |

---

## 4. Action Items: Integration Handoff

The Research Phase is complete. The following items constitute the work order for the **Integration Team**:

### Artifacts Provided
*   **Checkpoint:** Trained weights from Phase 2 (`runs/mosaic_commitment_train/`).
*   **Config:** `config/presets/mosaic_commitment.yml`.
*   **Spec:** Full Tensor Key Reference and Event Schema.

### Implementation Tasks
- [x] **Implement EventBus Model Handler**
  - Added `caramba.infer.event_runtime.ModelHandler` (`EventHandler`) for `EventBus` integration
  - Decodes model output bytes back into `EventEnvelope` objects

- [x] **Implement "Mode B" Commitment Injection**
  - Added `CommitmentModeB` which sets `commitment_delta` from `argmax(mosaic_commitment_logits)` (class → delta via `{0,1,2}→{-1,0,+1}`)

- [x] **Implement Commitment Ledger**
  - Added `caramba.core.commitments.CommitmentLedger`
  - **Auto-generate IDs:** on `delta=+1` when `commitment_id=None`
  - **Close without ID:** on `delta=-1` when `commitment_id=None`, closes the most recent open commitment for that sender (raises if none)
  - Metrics tracked via `CommitmentMetrics`:
    - `idle_with_open_commitments`
    - `commitment_open_to_close_latency_s_sum/count` (+ mean helper)

- [x] **Context Window Management**
  - Implemented as a **streaming stateful runner** (`StreamModelRunner`) that keeps a persistent `InferContext` for MOSAIC (no fixed attention-style context window required)
  - Commitment correctness across long runs is ensured via the ledger (independent of whether earlier open events remain in a prompt window)


                                                                                             Final Integration Details

                                                                             7. JSON Decoding Edge Case: Partial/Streaming Generation

The EventDecoder.decode() expects a complete, valid JSON byte sequence. During generation, the model may emit partial JSON before the closing }.

Implementation guidance:


 # In the generation loop:
 while generating:
     # 1. Accumulate byte tokens until we see a valid JSON envelope
     # 2. Attempt decode only when buffer ends with `}`
     # 3. If decode fails (JSONDecodeError), continue accumulating
     # 4. On success, extract EventEnvelope and aux head outputs


Or: Use a sentinel token (e.g., byte 0x03 = ETX) to delimit event boundaries. This requires a training change, so for v0, just buffer until valid JSON.

───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
                                                                                          8. EventBus Integration Pattern

The current EventBus expects handlers to be objects (not functions). Integration should implement a ModelHandler:


 class ModelHandler(EventHandler):
     def `__init__`(self, model, decoder, encoder, commitment_ledger):
         self.model = model
         self.decoder = decoder
         self.encoder = encoder
         self.ledger = commitment_ledger

     def handle(self, event: EventEnvelope) -> None:
         # 1. Encode incoming event to tokens
         input_ids = self.encoder.encode(event)

         # 2. Run model forward (with context window management)
         output_ids, aux_outputs = self.model.generate(input_ids)

         # 3. Decode output to EventEnvelope
         response = self.decoder.decode(output_ids)

         # 4. Inject commitment_delta from aux head (Mode B)
         commitment_logits = aux_outputs.get("mosaic_commitment_logits")
         if commitment_logits is not None:
             delta = int(commitment_logits.argmax(dim=-1)[-1].item()) - 1  # {0,1,2} → {-1,0,+1}
             response = response._replace(commitment_delta=delta)  # if using replace
             # Or reconstruct with updated field

         # 5. Update commitment ledger
         self.ledger.update(response)

         # 6. Publish response back to bus
         self.bus.publish(response)


───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
                                                                                   9. Commitment Ledger (Minimal Implementation)


 @dataclass
 class CommitmentLedger:
     open: dict[str, tuple[str, float]] = field(default_factory=dict)  # id → (sender, open_ts)
     metrics: dict[str, int] = field(default_factory=lambda: {
         "opens": 0, "closes": 0, "close_without_id": 0, "idle_with_open": 0
     })

     def update(self, event: EventEnvelope) -> None:
         if event.commitment_delta == +1:
             cid = event.commitment_id or str(uuid.uuid4())
             self.open[cid] = (event.sender, event.ts)
             self.metrics["opens"] += 1
         elif event.commitment_delta == -1:
             cid = event.commitment_id
             if cid and cid in self.open:
                 del self.open[cid]
                 self.metrics["closes"] += 1
             elif self.open:
                 # Fallback: close most recent from sender
                 for k in reversed(list(self.open.keys())):
                     if self.open[k][0] == event.sender:
                         del self.open[k]
                         self.metrics["close_without_id"] += 1
                         break

         if event.type == "Idle" and self.open:
             self.metrics["idle_with_open"] += 1


───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
                                                                             10. Default Subscriptions (Avoid "No Subscribers" Crash)

Initialize the bus with at least:


 bus = EventBus()

 # Required: handle all model outputs
 bus.subscribe("Message", model_handler)
 bus.subscribe("ToolCall", tool_dispatcher)

 # Recommended: catch-all for unexpected types
 bus.subscribe("*", LoggingHandler())  # logs + drops gracefully


───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
                                                                                        11. Tensor Key Reference (Complete)


  Key                               Source    Shape        Values
 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  input_ids                         batch     (B, T)       0-255 (bytes)
  target_ids                        batch     (B, T)       0-255 (bytes)
  memblock_teacher_opcode             batch     (B, T)       0-9 (MosaicOpcode enum)
  memblock_teacher_write_gate         batch     (B, T)       0.0 / 1.0
  memblock_teacher_commitment_delta   batch     (B, T)       -1, 0, +1 (or -100 mask)
  mosaic_opcode_logits              outputs   (B, T, 10)   logits
  mosaic_commitment_logits          outputs   (B, T, 3)    logits
  mosaic_write_gate                 outputs   (B, T)       0.0-1.0 (sigmoid)


───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

                                                                                                 Consensus Locked


  Decision                Resolution
 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  I/O Mode                Mode B (aux head injection)
  Missing commitment_id   Auto-generate at runtime
  Close without ID        Fallback to most-recent-open from sender + log warning
  EventBus strictness     Add * wildcard handler in dev mode


───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Handoff is complete. The integration team has:

 1 Trained checkpoint (Phase 2: loss 0.3211)
 2 Full tensor key spec
 3 EventBus integration pattern
 4 Commitment ledger implementation
 5 Edge case handling (partial JSON, context window, missing IDs)

---

## 5. Implementation Details (Integration Team)

### New / Updated Modules
- `core/commitments.py`
  - `CommitmentLedger`, `CommitmentMetrics`, `CommitmentOpen`
- `infer/event_runtime.py`
  - `EventStreamCodec`: JSON↔bytes with newline delimiter convention
  - `StreamModelRunner`: persistent `InferContext` runner for streaming event tokens
  - `EventResponder`: generates one JSON event and buffers until a valid decode
  - `CommitmentModeB`: injects `commitment_delta` from aux head logits
  - `ModelHandler`: `EventBus` handler that runs the model and republishes the response

### Partial JSON Handling (v0)
`EventResponder` buffers generated byte tokens and attempts decode only when the last token is `}`. `json.JSONDecodeError` / `UnicodeDecodeError` are treated as expected while the JSON is incomplete; any other decoding/validation error raises immediately.

## Paper Review and Critique

### Critique 1

Danny, this is a well-motivated piece of work with a clear thesis: attention's implicit "store everything" memory can be replaced by explicit, fixed-size data structures controlled by a learned neural controller. The decomposition into local mixer + state bank + associative cache is principled, and I appreciate that you're honest about the hard parts.

**What I find compelling:**

The addressing problem discussion (Section 3.7) is refreshingly candid. Most explicit memory papers handwave this — you correctly identify that if the continuous state has decayed, you may not be able to generate the address needed to retrieve what you've lost. The product-quantized VQ routing with neighbor reads is a sensible mitigation.

The training curriculum design (Section 3.8) shows you've actually fought with optimization. The forced-read dropout to prevent the model from ignoring memory, utility prediction for credit assignment, scheduled sampling from teacher to student control — these are the kinds of details that come from actually building the thing.

The n-gram cache observation (Section 5.2) is underrated. Many "hard" failures really are just verbatim continuation failures. Offloading that to a classical algorithm at O(1) access is high-yield per complexity dollar.

**Where I want more:**

The elephant in the room is Table 2 — it's all pending. The Phase 1/2 curriculum losses (0.36, 0.32) are encouraging but hard to interpret without knowing what baseline or ceiling looks like on those synthetic traces. The core bet is that lossy constant-size memory can replace perfect growing memory. That needs empirical demonstration on:

- **In-context learning**: Can MOSAIC acquire and apply a new pattern from few examples? This is the capability that really separates transformers from prior recurrent work. You list it as a stress test but don't report results.

- **Hash collision dynamics**: How many buckets are you using? What's empirical collision rate on long contexts with many entities? The contrastive "collision pressure" loss is mentioned but not quantified.

- **Direct Mamba/RWKV comparison**: You're targeting similar territory (efficient long-range modeling). A head-to-head would be illuminating.

**Structural observation:**

The event-native / differentiable VM framing is interesting but feels like it could be a separate paper. The core contribution — replacing attention+KV with explicit memory structures — stands on its own. Combining it with "JSON events as byte tokens" and "opcodes/commitment deltas" makes the paper do two things at once. If you're targeting ML venues, they may want the architectural contribution isolated; the event-driven runtime could be follow-up work or a systems paper.

What scale models have you trained so far, and what's the plan for filling in Table 2?

## Critique 2

This looks like a very ambitious and technically sophisticated proposal for a "Post-Transformer" architecture. The paper essentially proposes merging **State Space Models (SSMs)** with **Memory-Augmented Neural Networks (MANs)** to achieve the holy grail of efficient LLMs: **Infinite context with $O(1)$ memory and compute.**

Here is a breakdown of the strengths, risks, and implications of the MOSAIC project:

### 1. The Core Innovation: The "Hybrid" Memory Model
The strongest aspect of this paper is its diagnosis of the current landscape.
*   **Transformers** have perfect recall but $O(T)$ memory cost (prohibitive for long context on consumer hardware).
*   **RNNs/SSMs (Mamba, RWKV)** have $O(1)$ memory but "lossy" recall (they struggle to remember exact phone numbers or code brackets from 10k tokens ago).

**MOSAIC's Solution:** Decouple "Vibe" from "Facts."
*   **Multiscale Continuous State (The Vibe):** Uses leaky integrators (similar to RWKV/Mamba) to maintain the semantic flow and syntax.
*   **Associative Indexed Cache (The Facts):** A fixed-size hash table. This is the critical differentiator. It attempts to replace the Transformer's "soft" scan of all history with a "hard" O(1) lookup.

### 2. The "Hidden Bottleneck" (Critical Risk)
Section 3.7 correctly identifies the fatal flaw in most memory-augmented networks (like Neural Turing Machines or Differentiable Neural Computers): **The Addressing Problem.**

*   *The Problem:* To look up a specific fact from the hash table, the "Controller" (the neural net) must generate the correct address/hash. But if the Controller has already "forgotten" the context (because its continuous state decayed), it cannot generate the address required to retrieve the memory. It is a chicken-and-egg problem.
*   *The Proposed Fix:* **Learnable Discretized Routing (Product-Quantized VQ).** This is a plausible mathematical solution, essentially forcing the network to cluster similar contexts into the same "buckets" during training. However, making this stable during training (avoiding collapse where everything writes to one bucket) is historically very difficult.

### 3. Engineering & "Event-Native" Design
The paper reads like it was written by a systems engineer rather than a pure academic.
*   **JSON-as-Bytes:** Treating the LM as a Virtual Machine (VM) that processes a byte stream of JSON events is a fascinating paradigm shift. It moves away from "text completion" toward "system control."
*   **Auxiliary Losses:** The use of explicit supervision for "Opcodes" and "Commitments" (rather than relying purely on next-token prediction to serendipitously learn how to use memory) is a very practical, "hacker" approach to getting complex architectures to converge.
*   **Caramba Framework:** The mention of specific config files (`mosaic.yml`) and laptop-feasibility suggests this is designed for edge-computing/local-LLM use cases, which is a massive unserved market.

### 4. The Missing Evidence
**The Elephant in the Room:** Table 2 (Page 8) is empty.
*   While the paper claims convergence on auxiliary losses (0.36/0.32), there is **no Perplexity (PPL)** or downstream evaluation data yet.
*   Minimizing a control loss is easy; ensuring that a hash-table-augmented RNN actually *models language* better than a standard Llama or Mamba model is the real test.
*   Without `Copy Score` results, we don't know if the hash collision handling (Section 3.7) actually works. In a fixed-size hash map, hash collisions are inevitable. If two distinct entities hash to the same bucket, the model will hallucinate.

### 5. Verdict
**Concept:** 9/10
**Feasibility:** 6/10
**Impact Potential:** 10/10

This is a high-risk, high-reward project.
*   **If it works:** It makes Transformers obsolete for long-context applications. You could run a 1M-token context agent on a MacBook Air because the memory footprint never grows.
*   **Likely failure mode:** The model learns to ignore the hash cache because the gradient flow through the "Hard Read/Write" operations is too noisy or sparse, reverting to a standard (but weak) RNN.

**Final Thought:**
The "Write Curriculum" described in Section 3.8 (forcing the model to use memory by explicitly dropping local information) is the key. If the author has actually solved the stability issues of training read/write heads with backprop, this is a breakthrough. If not, it's a well-designed re-implementation of the Neural Turing Machine problems from 2016.
