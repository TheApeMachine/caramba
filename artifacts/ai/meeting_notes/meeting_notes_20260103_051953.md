# Meeting Notes: MOSAIC Research Strategy & Table 2 Definition
**Date:** 2026-01-03
**Participants:** theapemachine, Gemini, Claude, ChatGPT

## 1. Meeting Summary
Following the successful technical implementation of the MOSAIC Event-Driven architecture (Phase 1 & 2), the team pivoted to **Research Validation**. Reviewing early paper critiques, the team identified a critical gap: the lack of empirical evidence in "Table 2" regarding memory capability, collision dynamics, and baseline comparisons.

The team established a consensus to adopt an **"ML-Venue Framing"** for the paper (positioning MOSAIC primarily as a memory architecture replacing Attention/KV). Crucially, this framing will be treated as a **"Production Readiness Checklist"**—the experiments required to satisfy reviewers (e.g., collision sweeps) are identical to the engineering acceptance tests required to trust the system in production. A specific execution contract was defined for the implementation team.

---

## 2. Key Ideas & Consensus

### A. Paper Positioning: ML-Venue Framing
*   **Concept:** To address reviewer skepticism, the paper will focus on "Fixed-size explicit memory as a substitute for Attention/KV," rather than the "Event-Driven Organism" narrative (which moves to an application/appendix role).
*   **Consensus:** ✅ **Strong Consensus**
*   **Rationale:** Reviewers want to see if the hash-based memory actually works compared to strong $O(1)$ baselines (like Mamba).
*   **Constraint:** This framing must not result in "paper-only" optimization. Every experiment must map to a real-world failure mode (e.g., forgetting, corruption, ignoring memory).

### B. "Table 2" Experiment Schema
*   **Concept:** The exact set of experiments required to fill the empty results table in the paper.
*   **Consensus:** ✅ **Strong Consensus**
*   **Details:** The team agreed on four specific rows:
    1.  **Row A:** Long-range Key/Value binding accuracy vs. Distance.
    2.  **Row B:** Collision pressure sweep (varying `mem_buckets` and `mem_hashes`).
    3.  **Row C:** Head-to-head baseline vs. **Mamba SSM** (same budget).
    4.  **Row D:** ICL-like few-shot mapping task (demonstrating generalization).

### C. Mechanism Verification (Telemetry)
*   **Concept:** To rebut the critique that the model might "ignore the memory" and rely solely on local state (like a weak RNN).
*   **Consensus:** ✅ **Strong Consensus**
*   **Requirement:** All runs must log specific telemetry:
    *   Read/Write gate utilization (mean/variance).
    *   Routing entropy (to detect bucket collapse).
    *   Ablation comparisons (Curriculum ON vs. OFF).

### D. "North Star" Integration Regression
*   **Concept:** Ensuring the "Event-Native" capabilities do not regress while the team focuses on synthetic memory tasks.
*   **Consensus:** ✅ **Strong Consensus**
*   **Action:** Keep `mosaic_event_native.yml` and `mosaic_commitment.yml` as standing regression tests, verifying streamed JSON envelope parsing and commitment ledger consistency.

---

## 3. Action Items: Implementation Handoff

The following is the **Execution Contract** for the Implementation Team.

### 1. Run Table 2 Experiments (The Bundle)
Generate runs and W&B logs for the following 4 rows.
*   [ ] **Row A (Recall):** Use `config/presets/mosaic_memory_curriculum.yml`. Report accuracy binned by distance gap.
*   [ ] **Row B (Collisions):** Run the following sweep grid:
    *   `mem_buckets`: **{1024, 4096, 16384}**
    *   `mem_hashes`: **{1, 2, 4}**
    *   *(Optional)* `mem_assoc`: **{2, 4}**
*   [ ] **Row C (Baseline):** Compare MOSAIC vs. `config/presets/mamba_ssm.yml` (use the `memory_curriculum_table2` target).
    *   **Fairness Rule:** Must use **same tokenizer** (tiktoken) and match **total token budget** and **parameter count**.
*   [ ] **Row D (ICL):** Few-shot rule induction with distractors. Sweep the gap between demonstrations and query (use `config/presets/mosaic_icl.yml`).

### 2. Implement Required Telemetry
Update logging to include the following keys for every run:
*   [x] `acc/bin_0` ... `acc/bin_K` (Distance-binned accuracy)
*   [x] `acc/worst_bin` (Tail metric)
*   [x] `collision/wrong_item_read_rate` (Canonical collision proxy)
*   [x] `mem/read_gate` & `mem/write_gate` (Utilization stats)
*   [x] `mem/routing_entropy` (Collapse indicator)
*   [x] Writer-ready export JSON (`table2_summary_<run_id>.json`)

### 3. North Star Maintenance
*   [x] Ensure `mosaic_event_native.yml` continues to pass regression checks for:
    *   Streamed JSON parsing (partial buffering).
    *   Commitment Ledger state consistency.

---

## 4. Technical Specifications (Reference)

### Baseline Fairness Constraints
For **Row C (MOSAIC vs Mamba)**, the implementation must adhere to:
1.  **Same Data:** Identical dataset generator and seeds.
2.  **Same Tokenizer:** Do not mix Byte-level vs Tiktoken. Use `tiktoken` for the baseline comparison.
3.  **Same Budget:** Match `d_model`, `n_layers`, and total training tokens.

### Canonical Collision Proxy
For **Row B**, the primary metric for corruption is:
*   **`collision/wrong_item_read_rate`**: The fraction of reads returning a non-target item when a target exists.

### Writer-Ready Exports
Each run must export a summary JSON containing:
```json
{
  "config": {
    "mem_buckets": int,
    "mem_hashes": int,
    "model_size": str
  },
  "metrics": {
    "acc_per_bin": [float],
    "acc_worst_bin": float,
    "collision_proxy": float
  }
}
```

---

## 5. Implementation Notes (Codebase)

### Telemetry & Exports
- `trainer/mosaic_table2.py`: `Table2Telemetry` (computes `acc/bin_*`, `acc/worst_bin`, `collision/wrong_item_read_rate`) and `Table2SummaryWriter`.
- `trainer/standard.py`: logs Table 2 telemetry every `telemetry_interval` step and writes `table2_summary_<run_id>.json` into the run directory.
- `layer/mosaic/block.py`: emits MOSAIC scalar stats into `ctx.mosaic_mem_stats` on telemetry steps, including:
  - `.../write_gate_p_mean`
  - `.../fuse_gate_mem_mean`
  - `.../write_bucket_entropy_norm` (routing entropy)

### Row D (ICL) Dataset
- `data/icl_rule.py`: ICL-like rule induction dataset emitting `table2_bin` for gap-binned accuracy.
- `runtime/engine/torch_engine.py`: registers `dataset.icl_rule_induction`.
- `config/presets/mosaic_icl.yml`: runnable preset for Row D.

### Row C (Baseline) Preset
- `config/presets/mamba_ssm.yml`: adds `memory_curriculum_table2` target for baseline comparisons on the same synthetic generator.

### Regression Tests (North Star)
- `core/commitments_test.py`: commitment ledger consistency.
- `infer/event_runtime_test.py`: streamed JSON buffering and Mode B commitment injection plumbing.