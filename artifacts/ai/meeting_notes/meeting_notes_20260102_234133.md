# Meeting Notes: MOSAIC Event-Driven Architecture & VM Implementation

**Date:** 2026-01-02
**Participants:** theapemachine, ChatGPT, Claude, Gemini

## 1. Meeting Summary
The discussion centered on solidifying the technical architecture for **MOSAIC** within the `caramba` framework. The team reached a critical consensus to shift the model's fundamental paradigm from a standard token-stream generator to an **Event-Driven Organism**. While the internal compute substrate will still operate on discrete steps (effectively "tokens" acting as VM clock cycles), the external interface will be exclusively event-based (JSON envelopes).

Technical feasibility was validated for consumer hardware (12–24GB VRAM), confirming that a "Differentiable VM" approach—using soft control surfaces and auxiliary losses rather than hard symbolic execution—is the correct path. The team defined specific implementation steps to unify memory modules, establish a minimal Instruction Set Architecture (ISA), and build the necessary Event Encoder/Decoder infrastructure.

---

## 2. Architecture Positioning: Caramba Substrate vs. MOSAIC Module

**Important:** To avoid confusion about package layout, here are the clear boundaries:

### Caramba Substrate (`caramba.core`)
- **Purpose:** Generic, architecture-agnostic primitives usable by any neural architecture or agent system
- **Contents:**
  - `EventEnvelope`: JSON-serializable event contract
  - `EventEncoder`/`EventDecoder`: Transducers for event ↔ tensor conversion
  - `EventBus`/`EventHandler`: In-memory event routing primitives
  - `HomeostaticLoop`/`IntrinsicDrive`: Generic homeostasis primitives
- **Design principle:** No MOSAIC-specific logic; these components could be used by Transformer, Mamba, or any other architecture

### MOSAIC Module (`caramba.layer.mosaic`)
- **Purpose:** MOSAIC-specific implementation (the "no-attention" streaming architecture)
- **Contents:**
  - `MosaicBlockLayer`: Core layer with local mixer, state bank, memory
  - `MosaicMemory`: Associative indexed cache
  - `MosaicState`: Multiscale state bank
  - `MosaicOpcode`/`MosaicISAV0`: MOSAIC Instruction Set Architecture
- **Design principle:** All MOSAIC-specific control surfaces, opcodes, and VM features live here

### Note Generator (`caramba.ai`, `caramba.agent`)
- **Purpose:** Integrated AI research team that generates these meeting notes
- **Important:** `brainstorm` and related agent tools are **not** part of the MOSAIC architecture; they are separate utilities for documentation and research workflows

### Manifest-Driven Approach
- All components are registered in `runtime/engine/torch_engine.py` and referenced by stable semantic IDs (e.g., `objective.mosaic_event_prediction`, `dataset.mosaic_event_traces`)
- Configuration is defined in YAML manifests (`config/presets/*.yml`) that reference these IDs
- This ensures modularity, type safety, and clear separation of concerns

---

## 3. Key Ideas & Consensus

### A. The Paradigm Pivot: Events vs. Tokens
*   **Concept:** The team moved away from the "LLM" view (continuous text stream) to an "Organism" view (asynchronous events).
*   **Consensus:** **Strong Consensus**.
*   **Details:**
    *   **External:** The atomic unit of interaction is the **Event** (e.g., `Message`, `ToolResult`, `Wake`, `Idle`).
    *   **Internal:** "Tokens" are rebranded as **VM Time Steps** or "latent micro-streams." The model processes an event via a sequence of fixed-cost steps.
    *   **Decision:** Implement "Event-Native" training from Day 1, rather than bootstrapping with pure text.

### B. Differentiable VM & Control Surfaces
*   **Concept:** The model functions as a Virtual Machine. Instead of hard coding execution logic, it emits "soft" signals (logits) representing operations (Opcodes) and memory gating.
*   **Consensus:** **Strong Consensus**.
*   **Implementation:**
    *   **ISA:** A minimal set of 8–16 opcodes (e.g., `READ_MEM`, `WRITE_MEM`, `GATE_UP`, `IDLE`).
    *   **Training:** Use **Auxiliary Losses** to supervise these heads using synthetic traces, rather than Reinforcement Learning.
    *   **Execution:** Opcodes influence the computation path (gating) but remain differentiable.

### C. Hardware Constraints & Memory Sizing
*   **Concept:** Ensuring the architecture runs and trains on consumer GPUs (12–24GB VRAM).
*   **Consensus:** **Strong Consensus**.
*   **Verification:**
    *   **Claude & Gemini** verified the math. With `mem_buckets=4096` and `mem_assoc=2` (from `mosaic_idle.yml`), state memory is ~0.9GB (FP16/Batch=8), leaving ample room on a 12GB card.
    *   **Constraint:** Default configurations must adhere to these lower bucket counts to remain accessible.

### D. Canonical Codebase Structure
*   **Concept:** Resolving duplicate implementations of memory and state modules.
*   **Consensus:** **Strong Consensus**.
*   **Decision:**
    *   `layer/mosaic/memory.py` is **Canonical**.
    *   `layer/mosaic_memory.py` and `layer/mosaic_state.py` are deprecated/legacy and will be removed.

### E. Wake-Sleep & Homeostasis
*   **Concept:** Implementing an "Idle" loop where the model consolidates memory without external input.
*   **Consensus:** **Strong Consensus**.
*   **Mechanism:** "Sleep" involves locking external input, sampling from the internal `MosaicMemory` (Replay Buffer), and training the local mixer to predict masked content.

---

## 4. Action Items / Next Steps

### Architecture & Schemas
*   [x] **Define Event Schema:** Create `caramba.core.event.EventEnvelope` (JSON contract with fields for `type`, `payload`, `priority`, `sender`).
*   [x] **Draft ISA:** Finalize the v0 Opcode list (approx. 10 ops), including **Claude’s** suggestion for `GATE_UP`/`GATE_DOWN`.
*   [x] **Control Surface Contract:** Update output specs to include per-event signals (`response_events`, `opcode`, `mem_write`, `commitment_delta`).

### Implementation (Code)
*   [x] **Cleanup:** Delete `layer/mosaic_memory.py` and `layer/mosaic_state.py`.
*   [x] **Transducers:** Implement `EventEncoder` (JSON → Tensor) and `EventDecoder` (Tensor → JSON).
*   [x] **Update Objectives:** Modify `trainer/objectives.py`:
    *   Rename/Refactor objective to `MosaicEventPrediction`.
    *   Add `aux_opcode_weight` and corresponding loss logic to supervise opcode heads.
*   [x] **Config:** Ensure the default `mosaic.yml` or `mosaic_vm.yml` uses `mem_buckets: 4096` for 12GB compatibility.

### Research & Training
*   [x] **Synthetic Data:** Generate "Event Traces" (not just text) to train the initial opcode behaviors (e.g., explicitly training the model to emit `WRITE_MEM` after key information).
*   [x] **State Decay:** (Proposed by Claude) Add a regularizer to keep state-bank decay rates within a healthy band `[0.001, 0.999]` to prevent collapse during early training.

---

## 5. Implementation Summary

### Event Schema (`caramba.core.event.EventEnvelope`)
**Location:** `core/event.py`

Implemented a minimal JSON-serializable event envelope with:
- **Required fields:** `type` (event identifier), `payload` (JSON-serializable data), `sender` (stable identity)
- **Optional fields:** `priority` (int, default 0), `budget_ms` (compute budget), `id` (auto-generated UUID hex), `ts` (Unix timestamp)
- **Methods:** `to_json_dict()` and `from_json_dict()` with strict validation
- **Design rationale:** Supports peer-to-peer agent communication, event-driven workflows, and future "commitment/negotiation" protocols

### Event Transducers (`caramba.core.event_codec`)
**Location:** `core/event_codec.py`

**EventEncoder:**
- `encode(event: EventEnvelope) -> Tensor`: Converts event to UTF-8 bytes, then to int64 tensor with values in `[0, 255]`
- `encode_many(events: Sequence[EventEnvelope]) -> list[Tensor]`: Batch encoding
- `encode_padded(events, pad_id=0) -> (ids, mask)`: Returns `(B, L)` padded tensor and boolean mask for batching
- JSON serialization uses deterministic format: `separators=(",", ":")`, `sort_keys=True`

**EventDecoder:**
- `decode(ids: Tensor) -> EventEnvelope`: Reverses encoding process
- `decode_many(encoded: Iterable[Tensor]) -> list[EventEnvelope]`: Batch decoding
- `decode_padded(ids: Tensor, mask: Tensor) -> list[EventEnvelope]`: Decodes padded batches using attention mask
- Validates byte ranges and JSON structure strictly

**Use case:** Enables event-native training where external interface is JSON events but internal representation is tensor sequences (VM time-steps).

### Codebase Cleanup
**Deleted files:**
- `layer/mosaic_memory.py` (legacy, 481 lines)
- `layer/mosaic_state.py` (legacy, 55 lines)

**Canonical paths:**
- `layer/mosaic/memory.py` (MosaicMemory class)
- `layer/mosaic/state.py` (MosaicState dataclass, get_state/set_state helpers)

All imports now reference canonical modules. No breaking changes to existing code.

### Objective Refactoring (`trainer/objectives.py`)
**Changes:**

1. **Extended `MosaicNextTokenWithAuxObjective`:**
   - Added `aux_opcode_weight: float = 0.1` parameter
   - Added `aux_reg_gate_weight: float = 0.0` parameter (disabled by default)
   - Added `aux_reg_sel_weight: float = 0.0` parameter (disabled by default)
   - Opcode loss: Cross-entropy over `(B, T, opcode_vocab)` logits with `ignore_index=-1`
   - Register gate loss: Binary cross-entropy for write enable signals
   - Register selection loss: Cross-entropy over slot logits `(B, T, reg_slots)`
   - All losses are optional (only computed when teacher signals present and weights > 0)

2. **Created `MosaicEventPrediction` alias:**
   - Inherits from `MosaicNextTokenWithAuxObjective`
   - Registered in `runtime/engine/torch_engine.py` as `objective.mosaic_event_prediction`
   - Semantic name aligns with "event-driven organism" vision

3. **Metrics exposure:**
   - `aux_opcode_ce`: Raw opcode cross-entropy
   - `aux_opcode_weighted`: Weighted contribution to total loss
   - `aux_reg_gate_bce`, `aux_reg_gate_weighted`: Register gate metrics
   - `aux_reg_sel_ce`, `aux_reg_sel_weighted`: Register selection metrics

### VM Preset (`config/presets/mosaic_vm.yml`)
**Features:**
- **VM capabilities:** `reg_slots: 4`, `opcodes_enabled: true`, `opcode_vocab: 4`
- **12GB-friendly sizing:** `mem_buckets: 4096`, `mem_assoc: 2`, `mem_dim: 128`
- **Model size:** `d_model: 256`, `n_layers: 4` (fast iteration)
- **Objective:** Uses `objective.mosaic_event_prediction` with `aux_opcode_weight: 0.1`
- **Dataset:** Uses `dataset.mosaic_memory_curriculum` with opcode teacher signals
- **Training:** 200 steps, batch_size=8, CPU-friendly defaults

**Memory footprint estimate:**
- State memory: ~0.9GB (FP16, batch=8) as verified in meeting
- Leaves ample room on 12GB cards for gradients, optimizer states, activations

### Synthetic Curriculum with Opcode Traces (`data/mosaic_synth.py`)
**Changes:**

1. **Opcode constants:**
   - `OP_NOP = 0`: No operation (default for most tokens)
   - `OP_READ = 1`: Memory read operation
   - `OP_WRITE = 2`: Memory write operation
   - `OP_CLEAR = 3`: Memory clear operation (reserved for future use)

2. **Opcode assignment logic:**
   - `OP_WRITE` assigned to value tokens during "SET k IS v" phase (when `mosaic_teacher_write_gate=1`)
   - `OP_READ` assigned to tokens before value emission during "GET k ? IS v" phase (when memory read is expected)
   - `OP_NOP` for all other tokens (distractors, protocol tokens, padding)

3. **Output:**
   - Returns `mosaic_teacher_opcode: Tensor` with shape `(T,)` and dtype `int64`
   - Aligned with `input_ids` positions (same length)
   - Values in `{0, 1, 2, 3}` with `-1` reserved for ignore (not currently used)

**Training signal:** Model learns to emit opcode logits that match teacher assignments, creating a differentiable "control surface" for memory operations.

### Control Surface Outputs
**Location:** `layer/mosaic/block.py` → `ctx.mosaic_aux_out`

**Available signals:**
- `mosaic_opcode_logits`: `(B, T, opcode_vocab)` when `opcodes_enabled: true`
- `mosaic_reg_write_gate_logits`: `(B, T)` when `reg_slots > 0`
- `mosaic_reg_sel_logits`: `(B, T, reg_slots)` when `reg_slots > 0`
- `mosaic_write_gate_logits`: `(B, T)` (memory write gate)
- `mosaic_read_bit_logits`: `(B, T, H, BITS)` (routing bits for read)
- `mosaic_write_bit_logits`: `(B, T, H, BITS)` (routing bits for write)
- `mosaic_write_utility_logits`: `(B, T)` (utility prediction)

**Future extensions:** Can add `response_events`, `commitment_delta` as discussed in meeting.

### State Decay Regularizer (Completed)
**What was implemented:**
- Added `state_decay_reg_min` / `state_decay_reg_max` to `MosaicBlockLayerConfig` (`config/layer.py`)
- Each MOSAIC block now emits `mosaic_state_decay_reg_loss` (scalar) into `ctx.mosaic_aux_out` and stacked blocks **accumulate** this scalar loss
- Objective supports `aux_state_decay_weight` to apply this regularizer (`trainer/objectives.py`)

### v0 ISA Module (Completed)
**What was implemented:**
- Added `layer/mosaic/isa.py` with `MosaicOpcode` (v0) and `MosaicISAV0` helpers (MOSAIC-specific ISA; Caramba remains architecture-agnostic)
- Presets can now use larger opcode vocabularies (e.g. `opcode_vocab: 10`) while datasets may still supervise a smaller subset (NOP/READ/WRITE/CLEAR)

### Event-Native Training Dataset (Completed)
**What was implemented:**
- Added `data/event_trace.py` (`dataset.mosaic_event_traces`) generating deterministic EventEnvelope traces encoded as byte tokens
- Teacher alignment includes `mosaic_teacher_opcode`, routing buckets, and write gates at token spans within JSON literals
- Added a runnable preset `config/presets/mosaic_event_native.yml`

### Intrinsic Drives & Peer Event Bus (Completed)
**What was implemented:**
- Added `core/homeostasis.py` (`DriveBand`, `IntrinsicDrive`, `HomeostaticLoop`) to produce `"Impulse"` events from metric deviations
- Added `core/event_bus.py` (`EventBus`, `EventHandler`) as a strict in-memory dispatcher for `EventEnvelope` instances

### Completion Status
All “next steps” identified in this meeting are now implemented:

- [x] **Wake–Sleep consolidation loop**: implemented as synthetic sleep/replay segments in:
  - `data/mosaic_synth.py` via `sleep_replay_per_pair`
  - `data/event_trace.py` via `sleep_replay_per_pair`
  - enabled in presets (`config/presets/mosaic_vm.yml`, `config/presets/mosaic_event_native.yml`)

- [x] **Opcode-conditioned execution**: opcode logits now modulate compute paths (soft control surface):
  - `config/layer.py`: `opcodes_control_enabled`, `opcodes_control_temp`
  - `layer/mosaic/block.py`: uses opcode probabilities to scale READ/WRITE pathways and register writes
  - `layer/mosaic/memory.py`: `write_scale` scales write updates during training

- [x] **Register traces**: synthetic datasets now supervise register write/selection when `reg_slots > 0`:
  - `data/mosaic_synth.py`: `mosaic_teacher_reg_write_gate`, `mosaic_teacher_reg_sel`
  - `data/event_trace.py`: `mosaic_teacher_reg_write_gate`, `mosaic_teacher_reg_sel`
  - presets enable `aux_reg_gate_weight` and `aux_reg_sel_weight`

No remaining next steps from this meeting.