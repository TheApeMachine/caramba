version: 2
name: mosaic_memory_curriculum_full
notes: |
  Full-architecture addressing experiment: MOSAIC memory curriculum with
  - VSA in-bucket scoring + novelty write scaling
  - RMF (Resonant Memory Field) successor-biased routing prior

defaults:
  data:
    tokenizer: tiktoken
    val_frac: 0.1
  logging:
    instrument: rich
    wandb: true
    wandb_project: mosaic
    wandb_entity: ''
    wandb_mode: offline
    eval_iters: 50
  runtime:
    save_every: 100

vars:
  d_model: 256
  n_layers: 6
  vocab_size: 8192
  block_size: 512
  mem_buckets: 4096
  mem_hashes: 2

targets:
  - type: experiment
    name: mosaic_d1_memory_full
    description: "Train MOSAIC with teacher memory signals + VSA + RMF."
    backend: torch
    task: task.language_modeling

    data:
      ref: dataset.mosaic_memory_curriculum
      config:
        block_size: ${block_size}
        vocab_size: ${vocab_size}
        mem_buckets: ${mem_buckets}
        mem_hashes: ${mem_hashes}
        n_pairs: 1
        distractor_len: 64
        n_items: 200000
        seed: 1337

    system:
      ref: system.language_model
      config:
        model:
          type: TransformerModel
          embedder:
            type: token
            vocab_size: ${vocab_size}
            d_model: ${d_model}
          topology:
            type: StackedTopology
            layers:
              - type: NestedTopology
                repeat: ${n_layers}
                layers:
                  - type: MemoryBlockLayer
                    d_model: ${d_model}
                    conv_kernel: 7
                    mlp_mult: 2.0
                    dropout_p: 0.0
                    state_k: 8
                    state_decay_min: 0.90
                    state_decay_max: 0.999

                    # Memory index structure:
                    # - hash: set-associative hash table (default)
                    # - trie: flat radix trie overlay (longest-prefix fallback; stores internal prefix nodes)
                    mem_index: trie
                    mem_trie_eta_decay: 0.5
                    mem_trie_fallback_enabled: true
                    # Critical for MPS memory: cap ancestor writes (leaf + N parents).
                    # Zero-indexed: 2 means write levels 0, 1, and 2 (leaf + parent + grandparent; 3 total nodes).
                    mem_trie_max_levels: 2

                    mem_router: vq
                    mem_vq_groups: 2
                    mem_vq_codebook_size: 64    # 64^2=4096 buckets
                    mem_vq_group_dim: 16
                    mem_vq_beam: 2
                    mem_write_multi: true

                    mem_buckets: ${mem_buckets}
                    mem_dim: 128
                    mem_hashes: ${mem_hashes}
                    mem_assoc: 4
                    mem_key_dim: 32
                    mem_read_temp: 1.0
                    mem_match_threshold: 0.0
                    mem_write_threshold: 0.5
                    mem_write_eta: 0.2

                    mem_vsa_enabled: true
                    mem_vsa_dim: 32
                    mem_vsa_weight: 1.0
                    mem_vsa_tanh_scale: 1.0
                    mem_vsa_novelty_beta: 1.0
                    mem_vsa_novelty_threshold: 0.0

                    rmf_enabled: true
                    rmf_dim: 64
                    rmf_eta: 0.2
                    rmf_weight: 1.0

                    forced_read_dropout_p: 0.0
                    aux_contrastive_delta: 1
              - type: RMSNormLayer
                d_model: ${d_model}
              - type: LinearLayer
                d_in: ${d_model}
                d_out: ${vocab_size}

    objective:
      ref: objective.mosaic_next_token_aux
      config:
        logits_key: logits
        target_key: target_ids
        aux_gate_weight: 0.2
        aux_bits_weight: 0.2
        aux_utility_weight: 0.2
        aux_contrastive_weight: 0.2

    trainer: trainer.train
    runs:
      - id: train
        mode: train
        exp: mosaic_d1_memory_full
        seed: 1337
        steps: 2000
        expected: {}
        train:
          phase: standard
          batch_size: 16
          block_size: ${block_size}
          lr: 0.0003
          device: mps
          dtype: float16
          memblock_teacher_p_start: 1.0
          memblock_teacher_p_end: 0.0
          memblock_teacher_p_schedule: linear
          memblock_teacher_p_warmup_steps: 200
          memblock_teacher_p_cooldown_steps: 200

entrypoints:
  default: mosaic_d1_memory_full
