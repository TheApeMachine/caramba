version: 2
name: llama32_1b_dba_surgery_export
notes: >
  Export-only surgery manifest (PyTorch):
  - Load HF teacher: meta-llama/Llama-3.2-1B
  - Build baseline (standard attention) + DBA (decoupled) model
  - Apply weights via AdapterStateDictTransformer + chosen DBA init mode
  - Write raw state_dict checkpoints for downstream benchmark manifests

defaults:
  logging:
    instrument: rich
    wandb: false
    wandb_project: ''
    wandb_entity: ''
    wandb_mode: disabled
    eval_iters: 0
  runtime:
    # required by schema; this manifest does no training
    save_every: 1

vars:
  # ---- Teacher ----
  teacher_ckpt: hf://meta-llama/Llama-3.2-1B

  # ---- Dummy data (schema-required; trainer does not read data) ----
  # Use a real llama-tokenized dataset path if you have it; it won't be opened by export.
  dataset_path: artifacts/datasets/fineweb_llama/fineweb_llama_1b.npy
  block_size: 2048

  # ---- Output ----
  # Two files will be written:
  #   ${output_dir}/baseline.pt
  #   ${output_dir}/surgery.pt
  output_dir: artifacts/surgery/llama32_1b_fresh

  # ---- Init policy for DBA attention ----
  # Options: fresh | random | svd
  dba_init: fresh

  # ---- Runtime ----
  device: mps
  dtype: float16

  # ---- Llama 3.2 1B architecture ----
  d_model: 2048
  n_heads: 32
  n_kv_heads: 8
  n_layers: 16
  d_ff: 8192
  vocab_size: 128256
  rope_base: 500000.0
  # DBA dims (total dims, not per-head)
  sem_dim: 128
  geo_dim: 256

x-llama32-baseline-model: &baseline_model
  type: TransformerModel
  tied_embeddings: false
  embedder:
    type: token
    vocab_size: ${vocab_size}
    d_model: ${d_model}
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                n_kv_heads: ${n_kv_heads}
                mode: standard
                rope_enabled: true
                rope_base: ${rope_base}
                is_causal: true
                dropout_p: 0.0
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: false
      - type: RMSNormLayer
        d_model: ${d_model}
        eps: 1e-5
      - type: LinearLayer
        d_in: ${d_model}
        d_out: ${vocab_size}
        bias: false

x-llama32-surgery-model: &surgery_model
  type: TransformerModel
  tied_embeddings: false
  embedder:
    type: token
    vocab_size: ${vocab_size}
    d_model: ${d_model}
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: ${n_layers}
        layers:
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                n_kv_heads: ${n_kv_heads}
                mode: decoupled
                sem_dim: ${sem_dim}
                geo_dim: ${geo_dim}
                rope_enabled: true
                rope_base: ${rope_base}
                is_causal: true
                dropout_p: 0.0
                decoupled_gate: true
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: false
      - type: RMSNormLayer
        d_model: ${d_model}
        eps: 1e-5
      - type: LinearLayer
        d_in: ${d_model}
        d_out: ${vocab_size}
        bias: false

targets:
  - type: experiment
    name: export
    description: "Export baseline + DBA-surgery checkpoints (no benchmarks)."
    backend: torch
    task: task.language_modeling
    data:
      ref: dataset.tokens
      config:
        path: ${dataset_path}
        block_size: ${block_size}
    # System is unused by the trainer, but required by schema. Keep it aligned
    # with the surgery model for clarity.
    system:
      ref: system.language_model
      config:
        model: *surgery_model
    objective: objective.next_token_ce
    trainer:
      ref: trainer.surgery_export
      config:
        teacher_ckpt: ${teacher_ckpt}
        teacher_model: *baseline_model
        student_model: *surgery_model
        dba_init: ${dba_init}
        output_dir: ${output_dir}
        device: ${device}
        dtype: ${dtype}
    # Dummy run (schema requires runs, trainer does no training)
    runs:
      - id: export
        mode: train
        exp: export_only
        seed: 42
        steps: 1
        expected: {phase: standard}
        train:
          phase: standard
          batch_size: 1
          block_size: 1
          lr: 1.0e-9
          device: ${device}
          dtype: ${dtype}

entrypoints:
  default: "target:export"

