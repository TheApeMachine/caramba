version: 2
name: mosaic_hypothesis_suite
notes: |
  Manifest-driven "unit tests for ideas" (hypothesis micro-runs).

  Run all targets:
    uv run -m caramba.cli run config/presets/mosaic_hypothesis_suite.yml

  Each target is a controlled ablation:
  - teacher routing always vs never
  - RMF on vs off (with partial teacher forcing)

defaults:
  data: { tokenizer: tiktoken, val_frac: 0.0 }
  logging: { instrument: rich, wandb: false, wandb_project: "", wandb_entity: "", eval_iters: 0 }
  runtime: { save_every: 50, best_effort: false }

vars:
  d_model: 192
  n_layers: 4
  vocab_size: 8192
  block_size: 256
  mem_buckets: 1024
  mem_hashes: 2
  steps: 100
  batch_size: 4
  lr: 0.0003
  seed: 1337
  # Portability: default to CPU; override (e.g. mps/cuda) via CLI by setting vars.device.
  device: cpu

targets:
  # Shared dataset config
  - &hyp_data
    ref: dataset.mosaic_memory_curriculum
    config:
      block_size: ${block_size}
      vocab_size: ${vocab_size}
      mem_buckets: ${mem_buckets}
      mem_hashes: ${mem_hashes}
      n_pairs: 1
      distractor_len: 64
      sleep_replay_per_pair: 8
      n_items: 10000
      seed: ${seed}

  # Shared system configs: RMF ON/OFF variants (teacher routing ablations should use RMF OFF).
  - &hyp_system_rmf_off
    ref: system.language_model
    config:
      model:
        type: TransformerModel
        embedder:
          type: token
          vocab_size: ${vocab_size}
          d_model: ${d_model}
        topology:
          type: StackedTopology
          layers:
            - type: NestedTopology
              repeat: ${n_layers}
              layers:
                - type: MosaicBlockLayer
                  d_model: ${d_model}
                  conv_kernel: 7
                  mlp_mult: 2.0
                  dropout_p: 0.0
                  state_k: 8
                  state_decay_min: 0.90
                  state_decay_max: 0.999
                  mem_router: vq
                  mem_vq_groups: 2
                  mem_vq_codebook_size: 32
                  mem_vq_group_dim: 16
                  mem_vq_beam: 2
                  mem_write_multi: true
                  mem_buckets: ${mem_buckets}
                  mem_dim: 96
                  mem_hashes: ${mem_hashes}
                  mem_assoc: 4
                  mem_key_dim: 32
                  mem_read_temp: 1.0
                  mem_write_threshold: 0.5
                  mem_write_eta: 0.2
                  mem_vsa_enabled: true
                  mem_vsa_weight: 1.0
                  rmf_enabled: false
            - type: RMSNormLayer
              d_model: ${d_model}
            - type: LinearLayer
              d_in: ${d_model}
              d_out: ${vocab_size}

  - &hyp_system_rmf_on
    ref: system.language_model
    config:
      model:
        type: TransformerModel
        embedder:
          type: token
          vocab_size: ${vocab_size}
          d_model: ${d_model}
        topology:
          type: StackedTopology
          layers:
            - type: NestedTopology
              repeat: ${n_layers}
              layers:
                - type: MosaicBlockLayer
                  d_model: ${d_model}
                  conv_kernel: 7
                  mlp_mult: 2.0
                  dropout_p: 0.0
                  state_k: 8
                  state_decay_min: 0.90
                  state_decay_max: 0.999
                  mem_router: vq
                  mem_vq_groups: 2
                  mem_vq_codebook_size: 32
                  mem_vq_group_dim: 16
                  mem_vq_beam: 2
                  mem_write_multi: true
                  mem_buckets: ${mem_buckets}
                  mem_dim: 96
                  mem_hashes: ${mem_hashes}
                  mem_assoc: 4
                  mem_key_dim: 32
                  mem_read_temp: 1.0
                  mem_write_threshold: 0.5
                  mem_write_eta: 0.2
                  mem_vsa_enabled: true
                  mem_vsa_weight: 1.0
                  rmf_enabled: true
                  rmf_dim: 64
                  rmf_eta: 0.2
                  rmf_weight: 1.0
            - type: RMSNormLayer
              d_model: ${d_model}
            - type: LinearLayer
              d_in: ${d_model}
              d_out: ${vocab_size}

  # Shared objective config
  - &hyp_objective
    ref: objective.mosaic_next_token_aux
    config: { aux_gate_weight: 0.2, aux_bits_weight: 0.2, aux_utility_weight: 0.2, aux_contrastive_weight: 0.2 }

  # Shared run config (teacher p differs per target)
  - &hyp_train_base
    phase: standard
    batch_size: ${batch_size}
    block_size: ${block_size}
    lr: ${lr}
    device: ${device}
    dtype: float16
    telemetry_interval: 10
    offload_optimizer: true
    mosaic_teacher_p_schedule: constant
    mosaic_teacher_p_warmup_steps: 0
    mosaic_teacher_p_cooldown_steps: 0

  - type: experiment
    name: mosaic_hyp_teacher_always
    description: "Hypothesis run: teacher routing always (p=1.0)."
    backend: torch
    task: task.language_modeling
    data: *hyp_data
    system: *hyp_system_rmf_off
    objective: *hyp_objective
    trainer: trainer.standard
    runs:
      - id: train
        mode: train
        exp: mosaic_hyp_teacher_always
        seed: ${seed}
        steps: ${steps}
        expected: {}
        train:
          <<: *hyp_train_base
          mosaic_teacher_p_start: 1.0
          mosaic_teacher_p_end: 1.0

  - type: experiment
    name: mosaic_hyp_teacher_never
    description: "Hypothesis run: teacher routing never (p=0.0)."
    backend: torch
    task: task.language_modeling
    data: *hyp_data
    system: *hyp_system_rmf_off
    objective: *hyp_objective
    trainer: trainer.standard
    runs:
      - id: train
        mode: train
        exp: mosaic_hyp_teacher_never
        seed: ${seed}
        steps: ${steps}
        expected: {}
        train:
          <<: *hyp_train_base
          mosaic_teacher_p_start: 0.0
          mosaic_teacher_p_end: 0.0

  - type: experiment
    name: mosaic_hyp_rmf_on
    description: "Hypothesis run: RMF on (teacher_p=0.5 constant)."
    backend: torch
    task: task.language_modeling
    data: *hyp_data
    system: *hyp_system_rmf_on
    objective: *hyp_objective
    trainer: trainer.standard
    runs:
      - id: train
        mode: train
        exp: mosaic_hyp_rmf_on
        seed: ${seed}
        steps: ${steps}
        expected: {}
        train:
          <<: *hyp_train_base
          mosaic_teacher_p_start: 0.5
          mosaic_teacher_p_end: 0.5

  - type: experiment
    name: mosaic_hyp_rmf_off
    description: "Hypothesis run: RMF off (teacher_p=0.5 constant)."
    backend: torch
    task: task.language_modeling
    data: *hyp_data
    system: *hyp_system_rmf_off
    objective: *hyp_objective
    trainer: trainer.standard
    runs:
      - id: train
        mode: train
        exp: mosaic_hyp_rmf_off
        seed: ${seed}
        steps: ${steps}
        expected: {}
        train:
          <<: *hyp_train_base
          mosaic_teacher_p_start: 0.5
          mosaic_teacher_p_end: 0.5

entrypoints:
  default: mosaic_hyp_teacher_always

