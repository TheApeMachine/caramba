version: 2
name: llama32_1b_dba_retrofit_suite
notes: >
  Suite manifest for the "retrofit onto Llama" appendix:
  - Exp1: SVD init vs Random init (same architecture) to validate low-rank structure.
  - Exp2: Healing cost (tokens-to-recovery) is measured from the global loss/perplexity curve.
  - Exp3: Heterogeneous KV-cache quantization sensitivity (baseline vs hypothesis vs counter-hypothesis).

defaults:
  data:
    tokenizer: llama
    val_frac: 0.05
  logging:
    instrument: rich
    wandb: true
    wandb_project: dba-upcycle
    wandb_entity: ''
    wandb_mode: online
    eval_iters: 50
  runtime:
    save_every: 500

vars:
  # Llama 3.2 1B-ish topology (caramba baseline for retrofit experiments)
  d_model: 2048
  n_heads: 32
  n_kv_heads: 8
  head_dim: 64
  n_layers: 16
  d_ff: 8192
  vocab_size: 128256
  rope_theta: 500000.0
  # Llama 3 RoPE scaling (matches HF config `rope_scaling` for llama3).
  rope_scaling:
    rope_type: llama3
    factor: 32.0
    low_freq_factor: 1.0
    high_freq_factor: 4.0
    original_max_position_embeddings: 8192

  # DBA bottleneck knobs (tuned for efficiency)
  sem_dim: 64
  geo_dim: 128
  # NOTE: in this config schema, sem_dim/geo_dim are TOTAL projection dims (not per-head).
  # With n_heads=32, sem_head_dim=2 and geo_head_dim=4, so (sem+geo) per-head is 6 and total is 192.
  # Keep attn_dim aligned with that total to avoid shape mismatches in DBA blocks during upcycle.
  attn_dim: 192

  # Training knobs (stable small-batch on MPS)
  batch_size: 1
  block_size: 2048
  grad_accum: 16
  # DataLoader knobs (manifest-driven; tune per host)
  num_workers: 0
  pin_memory: false
  prefetch_factor: 2
  blockwise_steps: 500
  global_steps: 1000
  blockwise_lr: 0.0001
  global_lr: 5.0e-05

targets:
  # Experiment 1A/2/3: SVD init baseline + healing curve + quantization sweep
  - type: experiment
    name: exp_svd
    description: "SVD-initialized DBA retrofit (baseline)."
    backend: torch
    task: task.language_modeling
    data:
      ref: dataset.tokens
      config:
        path: fineweb_100m.npy
        block_size: ${block_size}
        prepare:
          type: fineweb
          # Token budget to (re)generate when missing or mismatched.
          tokens: 100M
          tokenizer: llama
          model_id: meta-llama/Llama-3.2-1B
          dataset: HuggingFaceFW/fineweb
          subset: null
          split: train
          text_field: text
          max_chars: 50000
          append_eos: true
          append_bos: false
          rebuild_on_failure: true
    system:
      ref: system.language_model
      config:
        model:
          type: TransformerModel
          tied_embeddings: false
          embedder:
            type: token
            vocab_size: ${vocab_size}
            d_model: ${d_model}
          topology:
            type: StackedTopology
            layers:
              - type: NestedTopology
                repeat: ${n_layers}
                layers:
                  - type: ResidualTopology
                    layers:
                      - type: RMSNormLayer
                        d_model: ${d_model}
                        eps: 1e-5
                      - type: AttentionLayer
                        d_model: ${d_model}
                        n_heads: ${n_heads}
                        n_kv_heads: ${n_kv_heads}
                        mode: decoupled
                        attn_dim: ${attn_dim}
                        sem_dim: ${sem_dim}
                        geo_dim: ${geo_dim}
                        rope_enabled: true
                        rope_base: ${rope_theta}
                        rope_scaling: ${rope_scaling}
                        is_causal: true
                        dropout_p: 0.0
                        decoupled_gate: true
                  - type: ResidualTopology
                    layers:
                      - type: RMSNormLayer
                        d_model: ${d_model}
                        eps: 1e-5
                      - type: SwiGLULayer
                        d_model: ${d_model}
                        d_ff: ${d_ff}
                        bias: false
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: LinearLayer
                d_in: ${d_model}
                d_out: ${vocab_size}
                bias: false
    objective: objective.next_token_ce
    trainer: trainer.upcycle
    runs:
      - id: blockwise
        mode: train
        exp: dba_blockwise
        seed: 42
        steps: ${blockwise_steps}
        expected: {phase: blockwise}
        train:
          phase: blockwise
          batch_size: ${batch_size}
          block_size: ${block_size}
          lr: ${blockwise_lr}
          device: mps
          dtype: auto
          num_workers: ${num_workers}
          pin_memory: ${pin_memory}
          prefetch_factor: ${prefetch_factor}
          teacher_ckpt: hf://meta-llama/Llama-3.2-1B
          teacher_sanity_reference: hf
          teacher_sanity_ref_batches: 1
          teacher_sanity_max_ppl: 20000.0
          teacher_sanity_max_ppl_ratio_vs_ref: 1.10
          teacher_sanity_max_nll_delta_vs_ref: 0.10
          dba_init: svd
          # Blockwise benefits from *more* optimizer updates per block.
          # (The distillation loop steps the optimizer once per `gradient_accumulation_steps`.)
          gradient_accumulation_steps: 4
          orchestrator_enabled: true
          orchestrator_max_loss_increase: 2.0
          optimizer: lion
          fused_optimizer: true
          use_amp: true
          amp_dtype: auto
          blockwise_autotune_enabled: true
          blockwise_autotune_mode: active
          blockwise_autotune_log_every: 50
          # CRITICAL: don't let early blocks starve later blocks.
          # Autotune can decay LR aggressively on block 1; reset each new block to `lr`.
          blockwise_reset_lr_each_block: true
          # Make autotune less eager to crush LR (keeps learning effective).
          blockwise_autotune_min_lr: 2.5e-5
          blockwise_autotune_lr_decay: 0.7
          blockwise_autotune_spike_std: 4.0
          blockwise_autotune_plateau_patience: 200
      - id: finetune
        mode: train
        exp: dba_finetune
        seed: 42
        steps: ${global_steps}
        expected: {phase: global}
        train:
          phase: global
          batch_size: ${batch_size}
          block_size: ${block_size}
          lr: ${global_lr}
          device: mps
          dtype: auto
          dba_init: svd
          num_workers: ${num_workers}
          pin_memory: ${pin_memory}
          prefetch_factor: ${prefetch_factor}
          gradient_accumulation_steps: ${grad_accum}
          # Speed: on MPS, the orchestrator's AdamW-based strategy portfolio can poison fp16 weights
          # (no GradScaler / no fp32 master weights), forcing us into bf16 and slowing things down.
          # Use the plain GlobalStepper here (Lion + fused) for fast, stable finetune.
          orchestrator_enabled: false
          # The student starts slightly worse than the teacher after blockwise distillation
          # (distilling intermediates, not logits). Allow a bit more headroom so we can heal.
          orchestrator_max_loss_increase: 2.5
          optimizer: lion
          fused_optimizer: true
          use_amp: true
          amp_dtype: auto
    benchmarks:
      - id: perplexity
        config:
          type: perplexity
          dataset: fineweb_100m.npy
          block_size: 2048
          batch_size: 1
          num_batches: 100
        models: [teacher, student]
        repeats: 1

      # Exp3: KV-cache quantization sensitivity sweep (latency as fast proxy).
      - id: latency_fp16_fp16
        config:
          type: latency
          prompt_lengths: [512, 2048]
          generation_lengths: [128]
          batch_sizes: [1]
          warmup_runs: 2
          timed_runs: 5
          use_cache: true
          cache_policy:
            type: DecoupledLayerKVCache
            k_sem: {kind: fp16, qblock: 32, residual_len: 0}
            k_geo: {kind: fp16, qblock: 32, residual_len: 0}
            v: {kind: fp16, qblock: 32, residual_len: 0}
        models: [student]
        repeats: 1

      - id: latency_hyp_q4q8q4
        config:
          type: latency
          prompt_lengths: [512, 2048]
          generation_lengths: [128]
          batch_sizes: [1]
          warmup_runs: 2
          timed_runs: 5
          use_cache: true
          cache_policy:
            type: DecoupledLayerKVCache
            k_sem: {kind: q4_0, qblock: 32, residual_len: 0}
            k_geo: {kind: q8_0, qblock: 32, residual_len: 0}
            v: {kind: q4_0, qblock: 32, residual_len: 0}
        models: [student]
        repeats: 1

      - id: latency_counter_q8q4q8
        config:
          type: latency
          prompt_lengths: [512, 2048]
          generation_lengths: [128]
          batch_sizes: [1]
          warmup_runs: 2
          timed_runs: 5
          use_cache: true
          cache_policy:
            type: DecoupledLayerKVCache
            k_sem: {kind: q8_0, qblock: 32, residual_len: 0}
            k_geo: {kind: q4_0, qblock: 32, residual_len: 0}
            v: {kind: q8_0, qblock: 32, residual_len: 0}
        models: [student]
        repeats: 1

  # Experiment 1B: Random init control (same architecture)
  - type: experiment
    name: exp_random
    description: "Random-initialized DBA retrofit (control for SVD hypothesis)."
    backend: torch
    task: task.language_modeling
    data:
      ref: dataset.tokens
      config:
        path: fineweb_100m.npy
        block_size: ${block_size}
        prepare:
          type: fineweb
          tokens: 100M
          tokenizer: llama
          model_id: meta-llama/Llama-3.2-1B
          dataset: HuggingFaceFW/fineweb
          subset: null
          split: train
          text_field: text
          max_chars: 50000
          append_eos: true
          append_bos: false
          rebuild_on_failure: true
    system:
      ref: system.language_model
      config:
        model:
          type: TransformerModel
          tied_embeddings: false
          embedder:
            type: token
            vocab_size: ${vocab_size}
            d_model: ${d_model}
          topology:
            type: StackedTopology
            layers:
              - type: NestedTopology
                repeat: ${n_layers}
                layers:
                  - type: ResidualTopology
                    layers:
                      - type: RMSNormLayer
                        d_model: ${d_model}
                        eps: 1e-5
                      - type: AttentionLayer
                        d_model: ${d_model}
                        n_heads: ${n_heads}
                        n_kv_heads: ${n_kv_heads}
                        mode: decoupled
                        attn_dim: ${attn_dim}
                        sem_dim: ${sem_dim}
                        geo_dim: ${geo_dim}
                        rope_enabled: true
                        rope_base: ${rope_theta}
                        rope_scaling: ${rope_scaling}
                        is_causal: true
                        dropout_p: 0.0
                        decoupled_gate: true
                  - type: ResidualTopology
                    layers:
                      - type: RMSNormLayer
                        d_model: ${d_model}
                        eps: 1e-5
                      - type: SwiGLULayer
                        d_model: ${d_model}
                        d_ff: ${d_ff}
                        bias: false
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: LinearLayer
                d_in: ${d_model}
                d_out: ${vocab_size}
                bias: false
    objective: objective.next_token_ce
    trainer: trainer.upcycle
    runs:
      - id: blockwise
        mode: train
        exp: dba_blockwise
        seed: 42
        steps: ${blockwise_steps}
        expected: {phase: blockwise}
        train:
          phase: blockwise
          batch_size: ${batch_size}
          block_size: ${block_size}
          lr: ${blockwise_lr}
          device: mps
          dtype: auto
          num_workers: ${num_workers}
          pin_memory: ${pin_memory}
          prefetch_factor: ${prefetch_factor}
          teacher_ckpt: hf://meta-llama/Llama-3.2-1B
          teacher_sanity_reference: hf
          teacher_sanity_ref_batches: 1
          teacher_sanity_max_ppl: 20000.0
          teacher_sanity_max_ppl_ratio_vs_ref: 1.10
          teacher_sanity_max_nll_delta_vs_ref: 0.10
          dba_init: random
          gradient_accumulation_steps: ${grad_accum}
          orchestrator_enabled: true
          orchestrator_max_loss_increase: 2.0
          optimizer: lion
          fused_optimizer: true
          use_amp: true
          amp_dtype: auto
          blockwise_autotune_enabled: true
          blockwise_autotune_mode: active
          blockwise_autotune_log_every: 50
      - id: finetune
        mode: train
        exp: dba_finetune
        seed: 42
        steps: ${global_steps}
        expected: {phase: global}
        train:
          phase: global
          batch_size: ${batch_size}
          block_size: ${block_size}
          lr: ${global_lr}
          device: mps
          dtype: auto
          dba_init: random
          num_workers: ${num_workers}
          pin_memory: ${pin_memory}
          prefetch_factor: ${prefetch_factor}
          gradient_accumulation_steps: ${grad_accum}
          orchestrator_enabled: true
          orchestrator_max_loss_increase: 2.0
          optimizer: lion
          fused_optimizer: true
          use_amp: true
          amp_dtype: auto
    benchmarks:
      - id: perplexity
        config:
          type: perplexity
          dataset: fineweb_100m.npy
          block_size: 2048
          batch_size: 1
          num_batches: 100
        models: [teacher, student]
        repeats: 1

  # Experiment 3 (standalone): reuse exp_svd checkpoint and run quantization sweeps only.
  - type: experiment
    name: exp_quant
    description: "Inference-only KV-cache quantization sweep (reuses exp_svd checkpoint)."
    backend: torch
    task: task.language_modeling
    data:
      ref: dataset.tokens
      config:
        path: fineweb_100m.npy
        block_size: ${block_size}
        prepare:
          type: fineweb
          tokens: 100M
          tokenizer: llama
          model_id: meta-llama/Llama-3.2-1B
          dataset: HuggingFaceFW/fineweb
          subset: null
          split: train
          text_field: text
          max_chars: 50000
          append_eos: true
          append_bos: false
          rebuild_on_failure: true
    system:
      ref: system.language_model
      config:
        model:
          type: TransformerModel
          tied_embeddings: false
          embedder:
            type: token
            vocab_size: ${vocab_size}
            d_model: ${d_model}
          topology:
            type: StackedTopology
            layers:
              - type: NestedTopology
                repeat: ${n_layers}
                layers:
                  - type: ResidualTopology
                    layers:
                      - type: RMSNormLayer
                        d_model: ${d_model}
                        eps: 1e-5
                      - type: AttentionLayer
                        d_model: ${d_model}
                        n_heads: ${n_heads}
                        n_kv_heads: ${n_kv_heads}
                        mode: decoupled
                        attn_dim: ${attn_dim}
                        sem_dim: ${sem_dim}
                        geo_dim: ${geo_dim}
                        rope_enabled: true
                        rope_base: ${rope_theta}
                        rope_scaling: ${rope_scaling}
                        is_causal: true
                        dropout_p: 0.0
                        decoupled_gate: true
                  - type: ResidualTopology
                    layers:
                      - type: RMSNormLayer
                        d_model: ${d_model}
                        eps: 1e-5
                      - type: SwiGLULayer
                        d_model: ${d_model}
                        d_ff: ${d_ff}
                        bias: false
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: LinearLayer
                d_in: ${d_model}
                d_out: ${vocab_size}
                bias: false
    objective: objective.next_token_ce
    trainer:
      ref: trainer.upcycle_eval
      config:
        teacher_ckpt: hf://meta-llama/Llama-3.2-1B
        # Produced by exp_svd's finetune run (orchestrated global)
        student_ckpt: runs/exp_svd/finetune_global_orchestrated_final.pt
        device: mps
        dtype: auto
    benchmarks:
      - id: perplexity
        config:
          type: perplexity
          dataset: fineweb_100m.npy
          block_size: 2048
          batch_size: 1
          num_batches: 100
        models: [teacher, student]
        repeats: 1

      - id: latency_fp16_fp16
        config:
          type: latency
          prompt_lengths: [512, 2048]
          generation_lengths: [128]
          batch_sizes: [1]
          warmup_runs: 2
          timed_runs: 5
          use_cache: true
          cache_policy:
            type: DecoupledLayerKVCache
            k_sem: {kind: fp16, qblock: 32, residual_len: 0}
            k_geo: {kind: fp16, qblock: 32, residual_len: 0}
            v: {kind: fp16, qblock: 32, residual_len: 0}
        models: [student]
        repeats: 1

      - id: latency_hyp_q4q8q4
        config:
          type: latency
          prompt_lengths: [512, 2048]
          generation_lengths: [128]
          batch_sizes: [1]
          warmup_runs: 2
          timed_runs: 5
          use_cache: true
          cache_policy:
            type: DecoupledLayerKVCache
            k_sem: {kind: q4_0, qblock: 32, residual_len: 0}
            k_geo: {kind: q8_0, qblock: 32, residual_len: 0}
            v: {kind: q4_0, qblock: 32, residual_len: 0}
        models: [student]
        repeats: 1

      - id: latency_counter_q8q4q8
        config:
          type: latency
          prompt_lengths: [512, 2048]
          generation_lengths: [128]
          batch_sizes: [1]
          warmup_runs: 2
          timed_runs: 5
          use_cache: true
          cache_policy:
            type: DecoupledLayerKVCache
            k_sem: {kind: q8_0, qblock: 32, residual_len: 0}
            k_geo: {kind: q4_0, qblock: 32, residual_len: 0}
            v: {kind: q8_0, qblock: 32, residual_len: 0}
        models: [student]
        repeats: 1
