version: 2
name: llama32_1b_dba_upcycle
notes: Llama 3.2 1B with Decoupled Bottleneck Attention via upcycle surgery.
defaults:
  data:
    tokenizer: llama
    val_frac: 0.05
  logging:
    instrument: rich
    wandb: true
    wandb_project: dba-upcycle
    wandb_entity: ''
    wandb_mode: online
    eval_iters: 50
  runtime:
    save_every: 500
targets:
- type: experiment
  name: paper
  description: Full DBA upcycle pipeline with benchmarks for paper publication.
  backend: torch
  task: task.language_modeling
  data:
    ref: dataset.tokens
    config:
      path: fineweb_100m.npy
      block_size: ${block_size}
  system:
    ref: system.language_model
    config:
      model: &id001
        type: TransformerModel
        tied_embeddings: false
        embedder:
          type: token
          vocab_size: ${vocab_size}
          d_model: ${d_model}
        topology:
          type: StackedTopology
          layers:
          - type: NestedTopology
            repeat: ${n_layers}
            layers:
            - type: ResidualTopology
              layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                n_kv_heads: ${n_kv_heads}
                mode: decoupled
                sem_dim: ${sem_dim}
                geo_dim: ${geo_dim}
                rope_enabled: true
                rope_base: ${rope_theta}
                is_causal: true
                dropout_p: 0.0
                decoupled_gate: true
                q_chunk: ${q_chunk}
            - type: ResidualTopology
              layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: false
          - type: RMSNormLayer
            d_model: ${d_model}
            eps: 1e-5
          - type: LinearLayer
            d_in: ${d_model}
            d_out: ${vocab_size}
            bias: false
  objective: objective.next_token_ce
  trainer: trainer.upcycle
  runs:
  - id: blockwise
    mode: train
    exp: dba_blockwise
    seed: 42
    steps: ${blockwise_steps}
    expected:
      phase: blockwise
    verify:
      type: compare
      batches: 5
      fail_fast: false
      attention:
        max_mean_l1: 0.05
        max_max_l1: 0.25
      logits:
        max_mean_l1: 0.05
        max_max_l1: 0.25
    train:
      phase: blockwise
      batch_size: ${batch_size}
      block_size: ${block_size}
      lr: ${blockwise_lr}
      device: mps
      dtype: ${dtype}
      teacher_ckpt: hf://meta-llama/Llama-3.2-1B
      convergence_target: ${convergence_target}
      convergence_patience: ${convergence_patience}
      convergence_max_steps: ${convergence_max_steps}
      cache_teacher_outputs: ${cache_teacher}
      use_amp: ${use_amp}
      amp_dtype: ${amp_dtype}
      optimizer: ${optimizer}
      weight_decay: ${weight_decay}
      fused_optimizer: ${fused_optimizer}
      gradient_accumulation_steps: ${grad_accum_steps}
      num_workers: ${num_workers}
      pin_memory: ${pin_memory}
      compile_model: ${compile_model}
  - id: finetune
    mode: train
    exp: dba_finetune
    seed: 42
    steps: ${global_steps}
    expected:
      phase: global
    verify:
      type: eval
      tokenizer:
        type: llama
      max_new_tokens: 32
      thresholds:
        min_student_accuracy: 0.7
        max_accuracy_drop: 0.1
      cases:
      - id: count_r
        prompt: 'How many times does the letter ''r'' appear in ''strawberry''? Answer
          with just the number:'
        answer: 3
        kind: int_greedy
      - id: capital_france
        prompt: 'What is the capital of France? Answer:'
        choices:
        - Paris
        - London
        - Berlin
        - Madrid
        answer: Paris
        kind: choice_logprob
    train:
      phase: global
      batch_size: ${batch_size}
      block_size: ${block_size}
      lr: ${global_lr}
      device: mps
      dtype: ${dtype}
      use_amp: ${use_amp}
      amp_dtype: ${amp_dtype}
      optimizer: ${optimizer}
      weight_decay: ${weight_decay}
      fused_optimizer: ${fused_optimizer}
  benchmarks:
  - id: perplexity
    config:
      type: perplexity
      dataset: fineweb_100m.npy
      block_size: 2048
      batch_size: 1
      num_batches: 100
    models:
    - teacher
    - student
    repeats: 1
  - id: latency
    config:
      type: latency
      seed: 42
      prompt_lengths:
      - 128
      - 512
      - 1024
      - 2048
      generation_lengths:
      - 64
      - 128
      - 256
      batch_sizes:
      - 1
      warmup_runs: 3
      timed_runs: 10
      use_cache: true
      cache_kind: fp16  # enables Metal fused DBA decode on MPS
    models:
    - teacher
    - student
    repeats: 1
  - id: memory
    config:
      type: memory
      seed: 42
      sequence_lengths:
      - 512
      - 1024
      - 2048
      - 4096
      batch_sizes:
      - 1
      measure_peak: true
      measure_kvcache: true
      quantization_modes:
      - fp16
      - q8
      - q4
    models:
    - teacher
    - student
    repeats: 1
- type: experiment
  name: quick
  description: Quick validation run with reduced steps.
  backend: torch
  task: task.language_modeling
  data:
    ref: dataset.tokens
    config:
      path: fineweb_100m.npy
      block_size: 512
  system:
    ref: system.language_model
    config:
      model: *id001
  objective: objective.next_token_ce
  trainer: trainer.upcycle
  runs:
  - id: blockwise_quick
    mode: train
    exp: dba_blockwise_quick
    seed: 42
    steps: 50
    expected:
      phase: blockwise
    train:
      phase: blockwise
      batch_size: 1
      block_size: 512
      lr: 0.0001
      device: mps
      dtype: ${dtype}
      teacher_ckpt: hf://meta-llama/Llama-3.2-1B
      cache_teacher_outputs: false
      use_amp: ${use_amp}
      amp_dtype: ${amp_dtype}
      optimizer: ${optimizer}
      weight_decay: ${weight_decay}
      fused_optimizer: ${fused_optimizer}
  - id: finetune_quick
    mode: train
    exp: dba_finetune_quick
    seed: 42
    steps: 100
    expected:
      phase: global
    train:
      phase: global
      batch_size: 1
      block_size: 512
      lr: 5.0e-05
      device: mps
      dtype: ${dtype}
      use_amp: ${use_amp}
      amp_dtype: ${amp_dtype}
      optimizer: ${optimizer}
      weight_decay: ${weight_decay}
      fused_optimizer: ${fused_optimizer}
  benchmarks:
  - id: perplexity_quick
    config:
      type: perplexity
      dataset: fineweb_100m.npy
      block_size: 512
      batch_size: 1
      num_batches: 10
    models:
    - teacher
    - student
    repeats: 1
vars:
  d_model: 2048
  n_heads: 32
  n_kv_heads: 8
  head_dim: 64
  n_layers: 16
  d_ff: 8192
  vocab_size: 128256
  rope_theta: 500000.0
  sem_dim: 128
  geo_dim: 256
  batch_size: 1
  block_size: 2048
  blockwise_steps: 500
  global_steps: 2000
  blockwise_lr: 0.0001
  global_lr: 5.0e-05
  convergence_target: 0.02
  convergence_patience: 100
  convergence_max_steps: 2000
  optimizer: lion
  fused_optimizer: true
  weight_decay: 0.0
  q_chunk: 1024
  dtype: auto
  cache_teacher: false
  use_amp: true
  amp_dtype: auto
  grad_accum_steps: 1
  num_workers: 0
  pin_memory: false
  compile_model: false
