version: 2
name: llama32_1b_dba_compare
notes: Llama 3.2 1B with DBA attention via upcycle + post-run compare verify.
defaults:
  data:
    tokenizer: llama
    val_frac: 0.05
  logging:
    instrument: rich
    wandb: true
    wandb_project: dba-upcycle
    wandb_entity: ''
    wandb_mode: online
    eval_iters: 50
  runtime:
    save_every: 500
targets:
- type: experiment
  name: upcycle
  description: DBA upcycle from Llama 3.2 1B, with post-run compare.
  backend: torch
  task: task.language_modeling
  data:
    ref: dataset.tokens
    config:
      path: fineweb_100m.npy
      block_size: 2048
  system:
    ref: system.language_model
    config:
      model:
        type: TransformerModel
        embedder:
          type: token
          vocab_size: ${vocab_size}
          d_model: ${d_model}
        topology:
          type: StackedTopology
          layers:
          - type: NestedTopology
            repeat: ${n_layers}
            layers:
            - type: ResidualTopology
              layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                n_kv_heads: ${n_kv_heads}
                mode: decoupled
                sem_dim: ${sem_dim}
                geo_dim: ${geo_dim}
                rope_enabled: true
                rope_base: 500000.0
                is_causal: true
                dropout_p: 0.0
                decoupled_gate: true
            - type: ResidualTopology
              layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: false
          - type: RMSNormLayer
            d_model: ${d_model}
            eps: 1e-5
          - type: LinearLayer
            d_in: ${d_model}
            d_out: ${vocab_size}
            bias: false
  objective: objective.next_token_ce
  trainer: trainer.train
  runs:
  - id: blockwise
    mode: train
    exp: dba_blockwise
    seed: 42
    steps: 500
    expected:
      phase: blockwise
    verify:
      type: compare
      batches: 2
      attention:
        max_mean_l1: 0.05
        max_max_l1: 0.25
      logits:
        max_mean_l1: 0.05
        max_max_l1: 0.25
    train:
      phase: blockwise
      batch_size: 1
      block_size: 2048
      lr: 0.0001
      device: mps
      dtype: float32
      teacher_ckpt: hf://meta-llama/Llama-3.2-1B
  - id: finetune
    mode: train
    exp: dba_finetune
    seed: 42
    steps: 2000
    expected:
      phase: global
    train:
      phase: global
      batch_size: 1
      block_size: 2048
      lr: 5.0e-05
      device: mps
      dtype: float32
  benchmarks:
  - id: riddles
    repeats: 10
  - id: adversarial
    repeats: 10
vars:
  d_model: 2048
  n_heads: 32
  n_kv_heads: 8
  head_dim: 64
  n_layers: 16
  d_ff: 8192
  vocab_size: 128256
  sem_dim: 128
  geo_dim: 256
