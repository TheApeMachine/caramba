version: 2
name: dba
notes: Decoupled Bottleneck Attention expressed via composable blocks.
defaults:
  data:
    tokenizer: tiktoken
    val_frac: 0.1
  logging:
    instrument: rich
    wandb: true
    wandb_project: ''
    wandb_entity: ''
    wandb_mode: online
    eval_iters: 20
  runtime:
    save_every: 0
targets:
- type: experiment
  name: default
  description: Default group
  backend: torch
  task: task.language_modeling
  data:
    ref: dataset.tokens
    config:
      path: ''
      block_size: 2048
  system:
    ref: system.language_model
    config:
      model:
        type: TransformerModel
        topology:
          type: NestedTopology
          repeat: ${n_blocks}
          layers:
          - type: ResidualTopology
            layers:
            - type: RMSNormLayer
              d_model: ${d_model}
              eps: 1e-5
            - type: AttentionLayer
              d_model: ${d_model}
              n_heads: ${n_heads}
              n_kv_heads: ${n_kv_heads}
              mode: decoupled
              sem_dim: ${sem_dim}
              geo_dim: ${geo_dim}
              rope_enabled: true
              is_causal: true
              dropout_p: 0.0
              decoupled_gate: true
          - type: ResidualTopology
            layers:
            - type: RMSNormLayer
              d_model: ${d_model}
              eps: 1e-5
            - type: SwiGLULayer
              d_model: ${d_model}
              d_ff: ${d_ff}
              bias: false
  objective: objective.next_token_ce
  trainer: trainer.standard
  runs:
  - id: default
    mode: train
    exp: dba
    seed: 1337
    steps: 1000
    expected:
      attn_mode: decoupled
vars:
  d_model: 128
  n_heads: 4
  n_kv_heads: 4
  sem_dim: 32
  geo_dim: 64
  d_ff: 256
  n_blocks: 2
