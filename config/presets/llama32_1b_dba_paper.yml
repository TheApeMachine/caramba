version: 2
name: llama32_1b_dba_paper
notes: Llama 3.2 1B with Decoupled Bottleneck Attention - with paper drafting.
defaults:
  data:
    tokenizer: llama
    val_frac: 0.05
  logging:
    instrument: rich
    wandb: true
    wandb_project: dba-upcycle
    wandb_entity: ''
    wandb_mode: online
    eval_iters: 50
  runtime:
    save_every: 500
targets:
- type: experiment
  name: paper
  description: Full DBA upcycle pipeline with paper drafting.
  backend: torch
  task: task.language_modeling
  data:
    ref: dataset.tokens
    config:
      path: fineweb_100m.npy
      block_size: ${block_size}
  system:
    ref: system.language_model
    config:
      model:
        type: TransformerModel
        tied_embeddings: false
        embedder:
          type: token
          vocab_size: ${vocab_size}
          d_model: ${d_model}
        topology:
          type: StackedTopology
          layers:
          - type: NestedTopology
            repeat: ${n_layers}
            layers:
            - type: ResidualTopology
              layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                n_kv_heads: ${n_kv_heads}
                mode: decoupled
                sem_dim: ${sem_dim}
                geo_dim: ${geo_dim}
                rope_enabled: true
                rope_base: ${rope_theta}
                is_causal: true
                dropout_p: 0.0
                decoupled_gate: true
            - type: ResidualTopology
              layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: false
          - type: RMSNormLayer
            d_model: ${d_model}
            eps: 1e-5
          - type: LinearLayer
            d_in: ${d_model}
            d_out: ${vocab_size}
            bias: false
  objective: objective.next_token_ce
  trainer: trainer.upcycle
  runs:
  - id: blockwise
    mode: train
    exp: dba_blockwise
    seed: 42
    steps: ${blockwise_steps}
    expected:
      phase: blockwise
    train:
      phase: blockwise
      batch_size: ${batch_size}
      block_size: ${block_size}
      lr: ${blockwise_lr}
      device: mps
      dtype: float32
      teacher_ckpt: hf://meta-llama/Llama-3.2-1B
      blockwise_autotune_enabled: true
      blockwise_autotune_mode: active
      blockwise_autotune_log_every: 50
  - id: finetune
    mode: train
    exp: dba_finetune
    seed: 42
    steps: ${global_steps}
    expected:
      phase: global
    train:
      phase: global
      batch_size: ${batch_size}
      block_size: ${block_size}
      lr: ${global_lr}
      device: mps
      dtype: float32
      orchestrator_enabled: true
      orchestrator_mode: active
      orchestrator_initial_strategy: spike_resistant
      orchestrator_max_loss_increase: 1.5
      orchestrator_fail_fast: true
  benchmarks:
  - id: perplexity
    config:
      type: perplexity
      dataset: fineweb_100m.npy
      block_size: 2048
      batch_size: 1
      num_batches: 100
    models:
    - teacher
    - student
    repeats: 1
  - id: latency
    config:
      type: latency
      prompt_lengths:
      - 128
      - 512
      - 1024
      - 2048
      generation_lengths:
      - 64
      - 128
      - 256
      batch_sizes:
      - 1
      warmup_runs: 3
      timed_runs: 10
      use_cache: true
      cache_kind: fp16  # enables Metal fused DBA decode on MPS
    models:
    - teacher
    - student
    repeats: 1
  - id: memory
    config:
      type: memory
      sequence_lengths:
      - 512
      - 1024
      - 2048
      - 4096
      batch_sizes:
      - 1
      measure_peak: true
      measure_kvcache: true
      quantization_modes:
      - fp16
      - q8
      - q4
    models:
    - teacher
    - student
    repeats: 1
- type: process
  name: code_graph_sync
  description: Index model topology into Graphiti memory for structural audits.
  team:
    leader: research_lead
  process:
    type: code_graph_sync
    name: code_graph_sync
    agent: leader
    index_namespace: main
- type: process
  name: paper_write
  description: Draft/update a LaTeX paper artifact (paper.tex).
  team:
    writer: writer
  process:
    type: paper_write
    name: paper_write
    writer: writer
    output_dir: paper
- type: process
  name: paper_review
  description: Review the LaTeX paper artifact and propose next steps.
  team:
    reviewer: reviewer
  process:
    type: paper_review
    name: paper_review
    reviewer: reviewer
    strictness: conference
    max_proposed_experiments: 3
    output_dir: paper
- type: process
  name: paper_loop
  description: Write → review → structural-audit loop (agent process).
  team:
    leader: research_lead
    writer: writer
    reviewer: reviewer
  process:
    type: research_loop
    name: paper_loop
    leader: leader
    writer: writer
    reviewer: reviewer
    max_iterations: 3
    output_dir: paper
vars:
  d_model: 2048
  n_heads: 32
  n_kv_heads: 8
  head_dim: 64
  n_layers: 16
  d_ff: 8192
  vocab_size: 128256
  rope_theta: 500000.0
  sem_dim: 128
  geo_dim: 256
  batch_size: 1
  block_size: 2048
  blockwise_steps: 500
  global_steps: 2000
  blockwise_lr: 0.0001
  global_lr: 5.0e-05
entrypoints:
  default: "target:paper"
  train: "target:paper"
  graph_sync: "process:code_graph_sync"
  write: "process:paper_write"
  review: "process:paper_review"
  loop: "process:paper_loop"
