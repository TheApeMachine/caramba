version: 2
name: herorun_unsloth
notes: >
  Unsloth-backed instruction fine-tune preset for the DBA paper models.
  This variant routes training through trainer.finetune_unsloth and expects
  Hugging Face model ids/paths for each target (via trainer.config.hf_model).
  The data/system blocks mirror the original herorun preset for parity, but
  the Unsloth trainer loads datasets directly from Hugging Face.

defaults:
  data:
    tokenizer: tiktoken:gpt2
    val_frac: 0.0
  logging:
    instrument: rich
    wandb: true
    wandb_project: dba-paper
    wandb_entity: ''
    wandb_mode: online
    eval_iters: 0
  runtime:
    # Checkpoints are written by the trainer into `runs/<target.name>/`.
    save_every: ${save_every}

vars:
  # ----- Dataset -----
  # Tokenized FineWeb-Edu (20B tokens) produced by your pipeline.
  # Keep as a plain path; generation is intentionally not attempted from manifest.
  # dataset: HuggingFaceFW/fineweb
  # tokens: 20b
  # Note: dataset_path is not used by trainer.finetune_unsloth (it loads datasets from HuggingFace),
  # but it's still required for manifest variable resolution in data sections
  dataset_path: dummy.npy  # Placeholder - not actually used by finetune_unsloth trainer

  model1: research/dba/100k_checkpoints/baseline/a100_fw20b_l22_baseline_s42_100k.pt
  model2: research/dba/100k_checkpoints/sem8geo32v40/a100_fw20b_l22_dba_s42_100k.pt
  model3: research/dba/100k_checkpoints/sem8geo32v40_gated/a100_fw20b_l22_dba_gated_s42_100k.pt

  block_size: 2048

  # ----- Model (Llama-1B-ish) -----
  d_model: 2048
  n_layers: 22
  n_heads: 32
  # Intermediate size: trades some throughput for stronger MLP capacity.
  d_ff: 5632
  # Dataset is GPT-2 encoded (fineweb_20b.npy); use 50257 rounded to 128.
  vocab_size: 50304
  rope_base: 10000.0

  # ----- DBA bottleneck dims (per-head: sem=8, geo=32, v=40) -----
  sem_dim: 256
  geo_dim: 1024
  attn_dim: 1280

  # ----- Training -----
  # Instruction finetune typically needs a small LR + enough steps to "learn EOS".
  steps: 5000
  seed: 42
  device: mps
  # Use explicit dtype to avoid "auto" differences across backends.
  # For throughput: prefer fp16 weights on MPS/CUDA (use bfloat16 on A100).
  dtype: float16
  # Prefer larger microbatches over high grad accumulation for better utilization.
  batch_size: 1
  grad_accum: 1
  # Feed the accelerator: parallelize mmap slicing + tensor materialization.
  num_workers: 0
  prefetch_factor: 1
  pin_memory: false
  # SFT-style LR (start small; these checkpoints are already trained).
  lr: 1.0e-5
  lr_decoupled: 1.0e-5
  weight_decay: 0.0
  optimizer: adamw
  fused_optimizer: true
  scheduler: cosine
  warmup_steps: 500
  save_every: 500
  compile_model: true
  compile_mode: default
  use_amp: false
  amp_dtype: float16
  # MPS-only speed knob for CE; can be disabled if unstable.
  mps_fast_ce: true
  mps_allow_fp16_autocast_without_gradscaler: false

targets:
  # ---------------------------------------------------------------------------
  # Baseline: standard attention (full KV dimension = d_model)
  # ---------------------------------------------------------------------------
  - type: experiment
    name: a100_fw20b_l22_baseline_s42_100k
    description: "Baseline finetune: standard attention (load pretrained, SFT on instructions)."
    backend: torch
    task: task.language_modeling
    data:
      ref: dataset.tokens
      config:
        path: ${dataset_path}
        block_size: ${block_size}
        # `instructions.npy` already has EOS appended per-example by `scripts/prepare_gpt2_instruct.py`.
        # Do NOT force EOS at the end of *every block* (that can inject extra EOS mid-stream).
    system:
      ref: system.language_model
      config:
        weight_init:
          type: GPT2Initializer
        model:
          type: TransformerModel
          tied_embeddings: false
          embedder:
            type: token
            vocab_size: ${vocab_size}
            d_model: ${d_model}
          topology:
            type: StackedTopology
            layers:
              - type: NestedTopology
                repeat: ${n_layers}
                layers:
                  - type: ResidualTopology
                    layers:
                      - type: RMSNormLayer
                        d_model: ${d_model}
                        eps: 1e-5
                      - type: AttentionLayer
                        d_model: ${d_model}
                        n_heads: ${n_heads}
                        # Full attention baseline: n_kv_heads defaults to n_heads when omitted.
                        mode: standard
                        rope_enabled: true
                        rope_base: ${rope_base}
                        is_causal: true
                        dropout_p: 0.0
                  - type: ResidualTopology
                    layers:
                      - type: RMSNormLayer
                        d_model: ${d_model}
                        eps: 1e-5
                      - type: SwiGLULayer
                        d_model: ${d_model}
                        d_ff: ${d_ff}
                        bias: false
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: LinearLayer
                d_in: ${d_model}
                d_out: ${vocab_size}
                bias: false
    objective: objective.next_token_ce
    trainer:
      ref: trainer.finetune_unsloth
      config:
        hf_model: ${model1}
        trust_remote_code: false
        full_finetuning: false
        load_in_4bit: false
        load_in_8bit: false
        load_in_16bit: false
        allow_non_cuda_quantization: false
        lora:
          enabled: true
          r: 8
          alpha: 16
          dropout: 0.0
          bias: none
          use_gradient_checkpointing: unsloth
    runs:
      - id: train
        mode: train
        exp: paper_baseline
        seed: ${seed}
        steps: ${steps}
        expected: {}
        train:
          phase: standard
          batch_size: ${batch_size}
          block_size: ${block_size}
          lr: ${lr}
          device: ${device}
          dtype: ${dtype}
          gradient_accumulation_steps: ${grad_accum}
          # Explicit finetune source checkpoint (HF model id/path for Unsloth).
          load_checkpoint: ${model1}
          # DataLoader performance knobs.
          num_workers: ${num_workers}
          prefetch_factor: ${prefetch_factor}
          pin_memory: ${pin_memory}
          # Memory optimization: recompute activations instead of storing them.
          # This reduces peak VRAM and can enable larger microbatches.
          activation_checkpointing: false
          activation_checkpoint_threshold_mb: 0.0
          # Throughput optimization: enable torch.compile (and AMP if you choose).
          use_amp: ${use_amp}
          amp_dtype: ${amp_dtype}
          mps_fast_ce: ${mps_fast_ce}
          mps_allow_fp16_autocast_without_gradscaler: ${mps_allow_fp16_autocast_without_gradscaler}
          compile_model: ${compile_model}
          compile_mode: ${compile_mode}
          # 'default' is the most stable mode under gradient accumulation and
          # tends to avoid long autotune/benchmark phases that don't reliably
          # improve cuBLAS GEMMs on A100.
          # compile_mode: default
          # Profiling disabled for production runs (avoids step-time spikes).
          profile_every: 0
          profile_record_shapes: false
          # Clip to prevent rare catastrophic steps (baseline can also hit these).
          grad_clip_norm: 1.0
          optimizer: ${optimizer}
          weight_decay: ${weight_decay}
          fused_optimizer: ${fused_optimizer}
          scheduler: ${scheduler}
          warmup_steps: ${warmup_steps}
          min_lr_ratio: 0.0
          # Strict / "paper" behavior: never silently resume or skip.
          auto_resume: false
          skip_if_final: false
          auto_batch_size: false

  # ---------------------------------------------------------------------------
  # DBA: decoupled bottleneck attention (KV dims = sem+geo for K, attn_dim for V)
  # ---------------------------------------------------------------------------
  - type: experiment
    name: a100_fw20b_l22_dba_s42_100k
    description: "DBA finetune: decoupled attention (load pretrained, SFT on instructions)."
    backend: torch
    task: task.language_modeling
    data:
      ref: dataset.tokens
      config:
        path: ${dataset_path}
        block_size: ${block_size}
        # `instructions.npy` already has EOS appended per-example by `scripts/prepare_gpt2_instruct.py`.
    system:
      ref: system.language_model
      config:
        weight_init:
          type: GPT2Initializer
        model:
          type: TransformerModel
          tied_embeddings: false
          embedder:
            type: token
            vocab_size: ${vocab_size}
            d_model: ${d_model}
          topology:
            type: StackedTopology
            layers:
              - type: NestedTopology
                repeat: ${n_layers}
                layers:
                  - type: ResidualTopology
                    layers:
                      - type: RMSNormLayer
                        d_model: ${d_model}
                        eps: 1e-5
                      - type: AttentionLayer
                        d_model: ${d_model}
                        n_heads: ${n_heads}
                        mode: decoupled
                        attn_dim: ${attn_dim}
                        sem_dim: ${sem_dim}
                        geo_dim: ${geo_dim}
                        rope_enabled: true
                        rope_base: ${rope_base}
                        rope_semantic: false
                        tie_qk: false
                        null_attn: false
                        decoupled_gate: false
                        is_causal: true
                        dropout_p: 0.0
                  - type: ResidualTopology
                    layers:
                      - type: RMSNormLayer
                        d_model: ${d_model}
                        eps: 1e-5
                      - type: SwiGLULayer
                        d_model: ${d_model}
                        d_ff: ${d_ff}
                        bias: false
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: LinearLayer
                d_in: ${d_model}
                d_out: ${vocab_size}
                bias: false
    objective: objective.next_token_ce
    trainer:
      ref: trainer.finetune_unsloth
      config:
        hf_model: ${model2}
        trust_remote_code: false
        full_finetuning: false
        load_in_4bit: false
        load_in_8bit: false
        load_in_16bit: false
        allow_non_cuda_quantization: false
        lora:
          enabled: true
          r: 8
          alpha: 16
          dropout: 0.0
          bias: none
          use_gradient_checkpointing: unsloth
    runs:
      - id: train
        mode: train
        exp: paper_decoupled_sem8geo32v40
        seed: ${seed}
        steps: ${steps}
        expected: {}
        train:
          phase: standard
          batch_size: ${batch_size}
          block_size: ${block_size}
          lr: ${lr_decoupled}
          device: ${device}
          dtype: ${dtype}
          gradient_accumulation_steps: ${grad_accum}
          load_checkpoint: ${model2}
          num_workers: ${num_workers}
          prefetch_factor: ${prefetch_factor}
          pin_memory: ${pin_memory}
          # Memory optimization: recompute activations instead of storing them.
          # This reduces peak VRAM and can enable larger microbatches.
          activation_checkpointing: false
          activation_checkpoint_threshold_mb: 0.0
          # Throughput optimization: enable torch.compile (and AMP if you choose).
          use_amp: ${use_amp}
          amp_dtype: ${amp_dtype}
          mps_fast_ce: ${mps_fast_ce}
          mps_allow_fp16_autocast_without_gradscaler: ${mps_allow_fp16_autocast_without_gradscaler}
          compile_model: ${compile_model}
          compile_mode: ${compile_mode}
          # See baseline run notes.
          # compile_mode: default
          # Profiling disabled for production runs (avoids step-time spikes).
          profile_every: 0
          profile_record_shapes: false
          grad_clip_norm: 1.0
          optimizer: ${optimizer}
          weight_decay: ${weight_decay}
          fused_optimizer: ${fused_optimizer}
          scheduler: ${scheduler}
          warmup_steps: ${warmup_steps}
          min_lr_ratio: 0.0
          auto_resume: false
          skip_if_final: false
          auto_batch_size: false
  # ---------------------------------------------------------------------------
  # DBA+gate: decoupled bottleneck attention with gating enabled
  # ---------------------------------------------------------------------------
  - type: experiment
    name: a100_fw20b_l22_dba_gated_s42_100k
    description: "DBA+gate finetune: decoupled attention with gate enabled (load pretrained, SFT on instructions)."
    backend: torch
    task: task.language_modeling
    data:
      ref: dataset.tokens
      config:
        path: ${dataset_path}
        block_size: ${block_size}
        # `instructions.npy` already has EOS appended per-example by `scripts/prepare_gpt2_instruct.py`.
    system:
      ref: system.language_model
      config:
        weight_init:
          type: GPT2Initializer
        model:
          type: TransformerModel
          tied_embeddings: false
          embedder:
            type: token
            vocab_size: ${vocab_size}
            d_model: ${d_model}
          topology:
            type: StackedTopology
            layers:
              - type: NestedTopology
                repeat: ${n_layers}
                layers:
                  - type: ResidualTopology
                    layers:
                      - type: RMSNormLayer
                        d_model: ${d_model}
                        eps: 1e-5
                      - type: AttentionLayer
                        d_model: ${d_model}
                        n_heads: ${n_heads}
                        mode: decoupled
                        attn_dim: ${attn_dim}
                        sem_dim: ${sem_dim}
                        geo_dim: ${geo_dim}
                        rope_enabled: true
                        rope_base: ${rope_base}
                        rope_semantic: false
                        tie_qk: false
                        null_attn: false
                        decoupled_gate: true
                        is_causal: true
                        dropout_p: 0.0
                  - type: ResidualTopology
                    layers:
                      - type: RMSNormLayer
                        d_model: ${d_model}
                        eps: 1e-5
                      - type: SwiGLULayer
                        d_model: ${d_model}
                        d_ff: ${d_ff}
                        bias: false
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: LinearLayer
                d_in: ${d_model}
                d_out: ${vocab_size}
                bias: false

    objective: objective.next_token_ce
    trainer:
      ref: trainer.finetune_unsloth
      config:
        hf_model: ${model3}
        trust_remote_code: false
        full_finetuning: false
        load_in_4bit: false
        load_in_8bit: false
        load_in_16bit: false
        allow_non_cuda_quantization: false
        lora:
          enabled: true
          r: 8
          alpha: 16
          dropout: 0.0
          bias: none
          use_gradient_checkpointing: unsloth
    runs:
      - id: train
        mode: train
        exp: paper_decoupled_sem8geo32v40_gated
        seed: ${seed}
        steps: ${steps}
        expected: {}
        train:
          phase: standard
          batch_size: ${batch_size}
          block_size: ${block_size}
          lr: ${lr_decoupled}
          device: ${device}
          dtype: ${dtype}
          gradient_accumulation_steps: ${grad_accum}
          load_checkpoint: ${model3}
          num_workers: ${num_workers}
          prefetch_factor: ${prefetch_factor}
          pin_memory: ${pin_memory}
          # Memory optimization: recompute activations instead of storing them.
          # This reduces peak VRAM and can enable larger microbatches.
          activation_checkpointing: false
          activation_checkpoint_threshold_mb: 0.0
          # Throughput optimization: enable torch.compile (and AMP if you choose).
          use_amp: ${use_amp}
          amp_dtype: ${amp_dtype}
          mps_fast_ce: ${mps_fast_ce}
          mps_allow_fp16_autocast_without_gradscaler: ${mps_allow_fp16_autocast_without_gradscaler}
          compile_model: ${compile_model}
          compile_mode: ${compile_mode}
          # See baseline run notes.
          # compile_mode: default
          # Profiling disabled for production runs (avoids step-time spikes).
          profile_every: 0
          profile_record_shapes: false
          grad_clip_norm: 1.0
          optimizer: ${optimizer}
          weight_decay: ${weight_decay}
          fused_optimizer: ${fused_optimizer}
          scheduler: ${scheduler}
          warmup_steps: ${warmup_steps}
          min_lr_ratio: 0.0
          auto_resume: false
          skip_if_final: false
          auto_batch_size: false

entrypoints:
  # Run both targets sequentially by omitting --target.
  baseline: "target:a100_fw1b_l22_baseline_s42_paper_strict"
  decoupled: "target:a100_fw1b_l22_decoupled_s42_paper_sem8geo32v40"
  gated: "target:a100_fw1b_l22_decoupled_gate_s42_paper_sem8geo32v40"
