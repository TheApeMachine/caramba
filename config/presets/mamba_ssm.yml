# ============================================================================
# Mamba-style State Space Model (SSM)
# ============================================================================
# This manifest defines a pure SSM model (no attention). SSMs scale linearly
# with sequence length and maintain a constant-size hidden state for inference.
#
# Selective SSMs like Mamba are effective for long-context modeling without
# the quadratic cost of self-attention.
# ============================================================================

version: 1
name: mamba_ssm
notes: "Selective SSM architecture for linear-time sequence modeling."

vars:
  d_model: 512
  n_layers: 12
  d_state: 16
  expand: 2
  vocab_size: 50257

defaults:
  tokenizer: tiktoken
  val_frac: 0.1
  instrument: rich
  wandb: false
  wandb_project: "ssm-research"
  save_every: 500

model:
  type: TransformerModel
  embedder:
    type: token
    vocab_size: "${vocab_size}"
    d_model: "${d_model}"
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: "${n_layers}"
        layers:
          # SSM block with residual
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: "${d_model}"
              - type: SSMLayer
                d_model: "${d_model}"
                d_state: "${d_state}"
                expand: "${expand}"
      # Output
      - type: RMSNormLayer
        d_model: "${d_model}"
      - type: LinearLayer
        d_in: "${d_model}"
        d_out: "${vocab_size}"

groups:
  - name: scratch_train
    description: "Training an SSM from scratch."
    data: "path/to/your/data.tokens"
    runs:
      - id: train_ssm
        mode: train
        exp: ssm_scratch
        seed: 42
        steps: 2000
        expected: {}
        train:
          phase: standard
          batch_size: 16
          block_size: 1024
          lr: 0.0001
          device: cpu
          dtype: float32
