version: 2
name: lora_finetune
notes: Efficient fine-tuning using LoRA adapters.
defaults:
  data:
    tokenizer: tiktoken
    val_frac: 0.1
  logging:
    instrument: rich
    wandb: false
    wandb_project: lora-research
    wandb_entity: ''
    wandb_mode: online
    eval_iters: 50
  runtime:
    save_every: 200
targets:
- type: experiment
  name: adapter_train
  description: Fine-tuning with LoRA.
  backend: torch
  task: task.language_modeling
  data:
    ref: dataset.tokens
    config:
      path: path/to/your/finetuning_data.tokens
      block_size: 512
  system:
    ref: system.language_model
    config:
      model:
        type: TransformerModel
        embedder:
          type: token
          vocab_size: ${vocab_size}
          d_model: ${d_model}
        topology:
          type: StackedTopology
          layers:
          - type: NestedTopology
            repeat: ${n_layers}
            layers:
            - type: ResidualTopology
              layers:
              - type: RMSNormLayer
                d_model: ${d_model}
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                mode: standard
            - type: ResidualTopology
              layers:
              - type: RMSNormLayer
                d_model: ${d_model}
              - type: LoRALinearLayer
                d_in: ${d_model}
                d_out: ${d_model}
                r: ${lora_r}
                alpha: ${lora_alpha}
          - type: RMSNormLayer
            d_model: ${d_model}
          - type: LinearLayer
            d_in: ${d_model}
            d_out: ${vocab_size}
  objective: objective.next_token_ce
  trainer: trainer.standard
  runs:
  - id: train_lora
    mode: train
    exp: lora_adapter
    seed: 1234
    steps: 1000
    expected: {}
    train:
      phase: standard
      batch_size: 16
      block_size: 512
      lr: 0.0005
      device: cpu
      dtype: float32
vars:
  d_model: 1024
  n_heads: 16
  n_layers: 8
  d_ff: 4096
  vocab_size: 50257
  lora_r: 8
  lora_alpha: 16
