# ============================================================================
# LoRA Efficient Fine-tuning
# ============================================================================
# This manifest demonstrates using Low-Rank Adaptation (LoRA) layers
# for parameter-efficient fine-tuning.
#
# Instead of fine-tuning full weight matrices, we inject small low-rank
# trainable matrices while keeping the base model frozen.
# ============================================================================

version: 1
name: lora_finetune
notes: "Efficient fine-tuning using LoRA adapters."

vars:
  d_model: 1024
  n_heads: 16
  n_layers: 8
  d_ff: 4096
  vocab_size: 50257
  lora_r: 8
  lora_alpha: 16

defaults:
  tokenizer: tiktoken
  val_frac: 0.1
  instrument: rich
  wandb: false
  wandb_project: "lora-research"
  save_every: 200

model:
  type: TransformerModel
  embedder:
    type: token
    vocab_size: "${vocab_size}"
    d_model: "${d_model}"
  topology:
    type: StackedTopology
    layers:
      - type: NestedTopology
        repeat: "${n_layers}"
        layers:
          # Attention block with LoRA
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: "${d_model}"
              - type: AttentionLayer
                d_model: "${d_model}"
                n_heads: "${n_heads}"
                mode: standard
          # MLP block with LoRA
          - type: ResidualTopology
            layers:
              - type: RMSNormLayer
                d_model: "${d_model}"
              # Here we use LoRALinear for efficient adaptation
              - type: LoRALinearLayer
                d_in: "${d_model}"
                d_out: "${d_model}"
                r: "${lora_r}"
                alpha: "${lora_alpha}"
      # Output
      - type: RMSNormLayer
        d_model: "${d_model}"
      - type: LinearLayer
        d_in: "${d_model}"
        d_out: "${vocab_size}"

groups:
  - name: adapter_train
    description: "Fine-tuning with LoRA."
    data: "path/to/your/finetuning_data.tokens"
    runs:
      - id: train_lora
        mode: train
        exp: lora_adapter
        seed: 1234
        steps: 1000
        expected: {}
        train:
          phase: standard
          batch_size: 16
          block_size: 512
          lr: 0.0005
          device: cpu
          dtype: float32
