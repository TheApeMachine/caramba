version: 2
name: llama32_1b_dba_paper_efficiency_aggressive_finetune_only
notes: "Finetune-only entrypoint for aggressive DBA. Resumes from the latest blockwise checkpoint (runs/paper/blockwise_blockwise_final.pt)."
defaults:
  data:
    tokenizer: llama
    val_frac: 0.05
  logging:
    instrument: rich
    wandb: true
    wandb_project: dba-upcycle
    wandb_entity: ""
    wandb_mode: online
    eval_iters: 50
  runtime:
    save_every: 500
targets:
  - type: experiment
    name: paper
    description: Aggressive DBA efficiency run (finetune only).
    backend: torch
    task: task.language_modeling
    data:
      ref: dataset.tokens
      config:
        path: artifacts/datasets/fineweb_llama/fineweb_1b.npy
        block_size: ${block_size}
        prepare:
          type: fineweb
          # Token budget to (re)generate when missing or mismatched.
          tokens: 1B
          tokenizer: llama
          model_id: meta-llama/Llama-3.2-1B
          dataset: fineweb_llama
          subset: sample-10BT
          split: train
          text_field: text
          max_chars: 50000
          append_eos: true
          append_bos: false
          rebuild_on_failure: true
    system:
      ref: system.language_model
      config:
        model:
          type: TransformerModel
          tied_embeddings: false
          embedder:
            type: token
            vocab_size: ${vocab_size}
            d_model: ${d_model}
          topology:
            type: StackedTopology
            layers:
              - type: NestedTopology
                repeat: ${n_layers}
                layers:
                  - type: ResidualTopology
                    layers:
                      - type: RMSNormLayer
                        d_model: ${d_model}
                        eps: 1e-5
                      - type: AttentionLayer
                        d_model: ${d_model}
                        n_heads: ${n_heads}
                        n_kv_heads: ${n_kv_heads}
                        mode: decoupled
                        attn_dim: ${attn_dim}
                        sem_dim: ${sem_dim}
                        geo_dim: ${geo_dim}
                        rope_enabled: true
                        rope_base: ${rope_theta}
                        is_causal: true
                        dropout_p: 0.0
                        decoupled_gate: true
                  - type: ResidualTopology
                    layers:
                      - type: RMSNormLayer
                        d_model: ${d_model}
                        eps: 1e-5
                      - type: SwiGLULayer
                        d_model: ${d_model}
                        d_ff: ${d_ff}
                        bias: false
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: LinearLayer
                d_in: ${d_model}
                d_out: ${vocab_size}
                bias: false
    objective: objective.next_token_ce
    trainer:
      ref: trainer.upcycle
      config:
        # Resume student weights from the blockwise phase (so we only run global finetune here).
        resume_from: runs/paper/blockwise_blockwise_final.pt
    runs:
      - id: finetune
        mode: train
        exp: dba_finetune
        seed: 42
        steps: ${global_steps}
        expected:
          phase: global
        train:
          phase: global
          batch_size: ${batch_size}
          block_size: ${block_size}
          lr: ${global_lr}
          device: mps
          dtype: auto
          teacher_ckpt: hf://meta-llama/Llama-3.2-1B
          dba_init: svd
          # Increase effective batch for stability (batch_size is 1 on MPS).
          gradient_accumulation_steps: 16
          # Train only the DBA routing path + gate (freeze embeddings/MLP/etc).
          # This relies on the upcycle steppers honoring TrainConfig.trainable_scope.
          trainable_scope:
            - ".*q_sem.*"
            - ".*k_sem.*"
            - ".*q_geo.*"
            - ".*k_geo.*"
            - ".*decoupled_gate_logit.*"
          # Deterministic finetune: disable the orchestrator (no optimizer switching).
          orchestrator_enabled: false
          # Small warmup helps when training only a small parameter subset.
          scheduler: cosine
          warmup_steps: 200
          grad_clip_norm: 1.0
          optimizer: lion
          fused_optimizer: true
          use_amp: true
          amp_dtype: auto
          # Speed: keep fp16 weights on MPS (Metal kernels). Set true only if you
          # hit fp16 instability/NaNs and are willing to trade speed for stability.
          mps_force_fp32_weights: false
          # Huge speed/memory win on MPS: avoid fp32 logits materialization for CE.
          mps_fast_ce: true
          # Speed: keep fp16 autocast even without GradScaler on MPS.
          mps_allow_fp16_autocast_without_gradscaler: true

    name: paper_loop
    description: Write → review → structural-audit loop (agent process).
    team:
      leader: research_lead
      writer: writer
      reviewer: reviewer
    process:
      type: research_loop
      name: paper_loop
      leader: leader
      writer: writer
      reviewer: reviewer
      max_iterations: 3
      output_dir: paper
vars:
  d_model: 2048
  n_heads: 32
  n_kv_heads: 8
  head_dim: 64
  n_layers: 16
  d_ff: 8192
  vocab_size: 128256
  rope_theta: 500000.0
  # DBA dims (sem8/geo32/v40):
  # - sem_dim: 256 (8 per head * 32 heads)
  # - geo_dim: 1024 (32 per head * 32 heads)
  # - attn_dim: 1280 (40 per head * 32 heads)
  sem_dim: 256
  geo_dim: 1024
  attn_dim: 1280
  batch_size: 1
  block_size: 2048
  global_steps: 2000
  # With trainable_scope active (DBA-only), we can usually use a higher fixed LR.
  global_lr: 1.0e-04
entrypoints:
  default: "target:paper"
  train: "target:paper"
