version: 2
name: llama32_1b_dba_paper_efficiency_aggressive
notes: 'Aggressive DBA: sem_dim=32, geo_dim=128, v_dim(attn_dim)=512. Cached-latency uses fp16 on MPS (Metal fused decode). For CUDA/Triton quantized decode, set latency.cache_kind: q4_0.'
defaults:
  data:
    tokenizer: llama
    val_frac: 0.05
  logging:
    instrument: rich
    wandb: true
    wandb_project: dba-upcycle
    wandb_entity: ''
    wandb_mode: online
    eval_iters: 50
  runtime:
    save_every: 500
targets:
- type: experiment
  name: paper
  description: Aggressive DBA efficiency run with paper drafting.
  backend: torch
  task: task.language_modeling
  data:
    ref: dataset.tokens
    config:
      path: fineweb_100m.npy
      block_size: ${block_size}
  system:
    ref: system.language_model
    config:
      model:
        type: TransformerModel
        tied_embeddings: false
        embedder:
          type: token
          vocab_size: ${vocab_size}
          d_model: ${d_model}
        topology:
          type: StackedTopology
          layers:
          - type: NestedTopology
            repeat: ${n_layers}
            layers:
            - type: ResidualTopology
              layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: AttentionLayer
                d_model: ${d_model}
                n_heads: ${n_heads}
                n_kv_heads: ${n_kv_heads}
                mode: decoupled
                attn_dim: ${attn_dim}
                sem_dim: ${sem_dim}
                geo_dim: ${geo_dim}
                rope_enabled: true
                rope_base: ${rope_theta}
                is_causal: true
                dropout_p: 0.0
                decoupled_gate: true
            - type: ResidualTopology
              layers:
              - type: RMSNormLayer
                d_model: ${d_model}
                eps: 1e-5
              - type: SwiGLULayer
                d_model: ${d_model}
                d_ff: ${d_ff}
                bias: false
          - type: RMSNormLayer
            d_model: ${d_model}
            eps: 1e-5
          - type: LinearLayer
            d_in: ${d_model}
            d_out: ${vocab_size}
            bias: false
  objective: objective.next_token_ce
  trainer: trainer.upcycle
  runs:
  - id: blockwise
    mode: train
    exp: dba_blockwise
    seed: 42
    steps: ${blockwise_steps}
    expected:
      phase: blockwise
    train:
      phase: blockwise
      batch_size: ${batch_size}
      block_size: ${block_size}
      lr: ${blockwise_lr}
      device: mps
      dtype: float32
      teacher_ckpt: hf://meta-llama/Llama-3.2-1B
  - id: finetune
    mode: train
    exp: dba_finetune
    seed: 42
    steps: ${global_steps}
    expected:
      phase: global
    train:
      phase: global
      batch_size: ${batch_size}
      block_size: ${block_size}
      lr: ${global_lr}
      device: mps
      dtype: float32
  benchmarks:
  - id: perplexity
    config:
      type: perplexity
      dataset: fineweb_100m.npy
      block_size: 2048
      batch_size: 1
      num_batches: 100
    models:
    - teacher
    - student
    repeats: 1
  - id: latency
    config:
      type: latency
      prompt_lengths:
      - 128
      - 512
      - 1024
      - 2048
      generation_lengths:
      - 64
      - 128
      - 256
      batch_sizes:
      - 1
      warmup_runs: 3
      timed_runs: 10
      use_cache: true
      cache_kind: fp16  # enables Metal fused DBA decode on MPS
    models:
    - teacher
    - student
    repeats: 1
  - id: memory
    config:
      type: memory
      sequence_lengths:
      - 512
      - 1024
      - 2048
      - 4096
      batch_sizes:
      - 1
      measure_peak: true
      measure_kvcache: true
      quantization_modes:
      - fp16
      - q8
      - q4
    models:
    - teacher
    - student
    repeats: 1
vars:
  d_model: 2048
  n_heads: 32
  n_kv_heads: 8
  head_dim: 64
  n_layers: 16
  d_ff: 8192
  vocab_size: 128256
  rope_theta: 500000.0
  sem_dim: 32
  geo_dim: 128
  attn_dim: 512
  batch_size: 1
  block_size: 2048
  blockwise_steps: 500
  global_steps: 2000
  blockwise_lr: 0.0001
  global_lr: 5.0e-05
paper:
  enabled: true
  title: 'Decoupled Bottleneck Attention: Efficient KV-Cache Compression via Attention
    Surgery'
  authors:
  - Daniel Owen van Dommelen
  paper_type: paper
  sections:
  - abstract
  - introduction
  - related_work
  - methodology
  - experiments
  - results
  - discussion
  - conclusion
  citations:
    enabled: true
    max_citations: 25
    sources:
    - arxiv
    - semantic_scholar
    prefer_recent: true
    recent_years: 3
  keywords:
  - attention mechanism
  - KV-cache compression
  - transformer efficiency
  - model distillation
  - grouped query attention
  - multi-query attention
  - long context modeling
  - memory efficient transformers
  model: gpt-5.2
  temperature: 0.7
  auto_version: true
  max_versions: 5
  custom_instructions: 'Focus on the following key contributions:

    1. The "attention surgery" approach for converting trained models

    2. The decoupled semantic/geometric bottleneck design

    3. The significant KV-cache memory reduction with minimal quality loss

    4. The distillation training procedure for recovering accuracy


    Use proper mathematical notation for attention mechanisms.

    Include comparisons to GQA, MQA, and standard multi-head attention.

    '
review:
  enabled: true
  strictness: conference
  check_methodology: true
  check_experiments: true
  check_results: true
  check_writing: true
  check_citations: true
  min_score_to_approve: 7.0
  auto_generate_experiments: true
  max_proposed_experiments: 3
  reviewer_persona: senior_researcher
  model: gpt-5.2
  temperature: 0.3
  custom_instructions: 'Pay special attention to:

    - Comparisons with GQA/MQA baselines

    - Memory vs quality trade-off analysis

    - Scalability to longer contexts

    - Ablation of semantic vs geometric bottleneck dimensions

    '
