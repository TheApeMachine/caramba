version: 2
name: mlp_classifier
notes: "Non-LM baseline: MLP classifier on (x.npy, y.npy)."

defaults:
  data:
    tokenizer: tiktoken
    val_frac: 0.1
  logging:
    instrument: rich
    wandb: false
    wandb_project: ""
    wandb_entity: ""
    eval_iters: 50
  runtime:
    save_every: 200

targets:
  - type: experiment
    name: mlp_demo
    description: "Train an MLP classifier from NumPy arrays."
    backend: torch
    task: task.supervised_classification

    data:
      ref: dataset.npy_supervised
      config:
        x_path: "path/to/x.npy"
        y_path: "path/to/y.npy"
        mmap: true

    system:
      ref: system.mlp_classifier
      config:
        d_in: 128
        d_hidden: 256
        n_layers: 2
        n_classes: 10
        dropout: 0.1

    objective:
      ref: objective.classification_ce
      config:
        logits_key: logits
        labels_key: targets

    trainer: trainer.standard

    runs:
      - id: train
        mode: train
        exp: mlp_baseline
        seed: 1337
        steps: 1000
        expected: {}
        train:
          phase: standard
          batch_size: 128
          block_size: 1
          lr: 0.001
          device: cpu
          dtype: float32

