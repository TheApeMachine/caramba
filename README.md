![caramba overview](assets/inside-caramba.png)

# caramba ğŸ§ª

**A substrate for architecture research and ML experimentation**

> Architectures are graphs. Graphs are manifests. Running experiments should require nothing more than a YAML file.

caramba provides a frictionless research environment with explicit building blocks, strict validation, and optimized execution. Define your model architecture in YAML, and caramba handles the rest, from compilation to training to publication-ready benchmarks.

---

## ğŸ“‹ Table of Contents

- [What is caramba?](#-what-is-caramba)
- [Key Features](#-key-features)
- [Quick Start](#-quick-start)
- [The Pipeline](#-the-pipeline)
- [Documentation](#-documentation)
- [Available Presets](#-available-presets)
- [Platform Support](#-platform-support)
- [Architecture Overview](#-architecture-overview)

---

## ğŸ¯ What is caramba?

caramba is a declarative ML experimentation platform that separates **intent** from **implementation**:

1. **You declare** what you want in a YAML manifest (architecture, training, benchmarks)
2. **caramba handles** the how (compilation, optimization, execution, artifacts)

This design enables:
- ğŸ”¬ **Rapid prototyping** â€” Test new architectures without writing training loops
- ğŸ“Š **Reproducible research** â€” Manifests are version-controllable experiment definitions
- âš¡ **Automatic optimization** â€” Runtime planning for batch sizes, precision, and compilation
- ğŸ“ **Publication-ready artifacts** â€” CSV, PNG, and LaTeX outputs from benchmarks

---

## âœ¨ Key Features

### ğŸ§± Generic Layer Library

Built-in support for modern neural network components:

| Layer Type         | Description                                         | Documentation                                 |
|--------------------|-----------------------------------------------------|-----------------------------------------------|
| **Attention**      | Standard, GQA, and DBA (Decoupled Bottleneck) modes | [â†’ Layers](docs/layers.md#attention)          |
| **MoE**            | Mixture of Experts with load balancing              | [â†’ Layers](docs/layers.md#mixture-of-experts) |
| **SSM**            | Selective State Space Models (Mamba-style)          | [â†’ Layers](docs/layers.md#state-space-models) |
| **GLU Variants**   | SwiGLU, GEGLU, and other gated linear units         | [â†’ Layers](docs/layers.md#feed-forward)       |
| **LoRA**           | Low-rank adaptation for efficient fine-tuning       | [â†’ Layers](docs/layers.md#lora)               |
| **Normalization**  | RMSNorm, LayerNorm                                  | [â†’ Layers](docs/layers.md#normalization)      |
| **RoPE**           | Rotary Position Embeddings                          | [â†’ Layers](docs/layers.md#attention)          |
| **Linear**         | Linear projections with optional bias               | [â†’ Layers](docs/layers.md#utility-layers)     |
| **Dropout**        | Dropout regularization                              | [â†’ Layers](docs/layers.md#utility-layers)     |
| **Diffusion Head** | Denoising head for diffusion models                 | [â†’ Layers](docs/layers.md#utility-layers)     |

### ğŸ”— Composable Topologies

Define complex model structures declaratively:

| Topology            | Use Case                      | Example              |
|---------------------|-------------------------------|----------------------|
| `StackedTopology`   | Sequential layer execution    | Transformer blocks   |
| `ResidualTopology`  | Skip connections (`x + f(x)`) | Pre-norm blocks      |
| `NestedTopology`    | Repeat layers N times         | N transformer layers |
| `ParallelTopology`  | Execute and stack outputs     | Multi-head attention |
| `BranchingTopology` | Execute and concatenate       | Feature fusion       |
| `CyclicTopology`    | Cyclic connections            | Graph networks       |
| `RecurrentTopology` | Recurrent with cache          | Sequence models      |

[â†’ Full Topology Guide](docs/topologies.md)

### ğŸ“ Multiple Training Modes

| Mode             | Description                         | When to Use                  |
|------------------|-------------------------------------|------------------------------|
| **Standard**     | End-to-end training from scratch    | Baseline experiments         |
| **Upcycle**      | Architecture surgery + distillation | Converting pretrained models |
| **Orchestrated** | Dynamic optimizer switching         | Adaptive training research   |

[â†’ Training Guide](docs/training.md)

### âš¡ Self-Optimization

caramba automatically optimizes your experiments:

- **Runtime planning** â€” Cached decisions for dtype, AMP, batch size, and `torch.compile`
- **KV-cache policy selection** â€” Budget-aware quantization with quality gates
- **Decode-plan bucketing** â€” Dynamic chunking for long-context inference
- **Adaptive speculative decoding** â€” Auto-adjusting draft lengths

[â†’ Optimization Details](docs/optimization.md)

### AI Research Collaborators

```bash
python3 -m caramba config/presets/multiplex_chat.yml --target brainstorm
```

The above command puts you in a chat session with ChatGPT 5.2, Claude Opus 4.1, and Gemini Pro 3, which all have the tools they need to inspect the code, perform research, and other relevant actions so you can collaborate on whatever research goals you have.

The agents are not just talking directly with you, but also have the ability to respond to each other, so it should really feel like a team structure.

### ğŸ¤– AI Research Automation

Optional AI-assisted workflows:

- **Paper drafting** â€” Generate LaTeX documents from experiment results
- **Automated review** â€” Get reviewer feedback and improvement suggestions
- **Research loop** â€” Write â†’ Review â†’ Experiment â†’ Repeat

[â†’ Agent Workflows](docs/agents.md)

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone and install
git clone https://github.com/theapemachine/caramba.git
cd caramba
pip install -r requirements.txt
```

### Run Your First Experiment

```bash
# Dry-run to validate a manifest (no execution)
python3 -m caramba config/presets/standard_transformer.yml --dry-run

# Run a full experiment with benchmarks
python3 -m caramba config/presets/llama32_1b_dba.yml --target paper

# Quick validation (reduced steps)
python3 -m caramba config/presets/llama32_1b_dba.yml --target quick
```

### Non-LM Architectures

```bash
# MLP classifier
python3 -m caramba config/presets/mlp_classifier.yml --dry-run

# Diffusion model
python3 -m caramba config/presets/diffusion_vector.yml --dry-run

# Graph neural network
python3 -m caramba config/presets/graph_node_classification.yml --dry-run
```

[â†’ Complete Getting Started Guide](docs/getting-started.md)

---

## ğŸ”„ The Pipeline

Every experiment flows through this chain:

```text
manifest â†’ parse â†’ lower â†’ validate â†’ build â†’ run â†’ verify â†’ benchmark â†’ artifacts
```

| Stage         | What Happens                              |
|---------------|-------------------------------------------|
| **parse**     | Load YAML/JSON, substitute `${variables}` |
| **lower**     | Normalize type names, resolve references  |
| **validate**  | Check schema, verify dimensions           |
| **build**     | Construct PyTorch modules from topology   |
| **run**       | Execute training runs with checkpointing  |
| **verify**    | Compare outputs against thresholds        |
| **benchmark** | Measure perplexity, latency, memory       |
| **artifacts** | Generate CSV, PNG, LaTeX outputs          |

---

## ğŸ“š Documentation

| Guide                                         | Description                                    |
|-----------------------------------------------|------------------------------------------------|
| [ğŸš€ Getting Started](docs/getting-started.md) | Installation, first experiment, basic concepts |
| [ğŸ“„ Manifest Reference](docs/manifests.md)    | Complete YAML schema with examples             |
| [ğŸ§± Layer Reference](docs/layers.md)          | All layer types and their configurations       |
| [ğŸ”— Topology Guide](docs/topologies.md)       | Composing complex architectures                |
| [ğŸ“ Training Guide](docs/training.md)         | Standard, upcycle, and orchestrated training   |
| [ğŸ”® Inference Guide](docs/inference.md)       | Generation, caching, speculative decoding      |
| [ğŸ“Š Benchmarking](docs/benchmarking.md)       | Running benchmarks and generating artifacts    |
| [ğŸ¤– Agent Workflows](docs/agents.md)          | AI-assisted paper drafting and review          |
| [âš¡ Optimization](docs/optimization.md)       | Metal/Triton kernels, runtime planning          |

---

## ğŸ“¦ Available Presets

Ready-to-use configurations in `config/presets/`:

| Preset | Architecture | Use Case |
|--------|--------------|----------|
| `llama32_1b_dba.yml` | Llama 3.2 1B â†’ DBA | KV-cache compression research |
| `standard_transformer.yml` | GPT-style transformer | Baseline language modeling |
| `moe_transformer.yml` | Transformer + MoE | Sparse scaling research |
| `mamba_ssm.yml` | Mamba-style SSM | Linear-time sequence modeling |
| `vit.yml` | Vision Transformer | Image classification |
| `lora_finetune.yml` | LoRA-enabled model | Efficient fine-tuning |
| `mlp_classifier.yml` | Simple MLP | Non-LM classification |
| `diffusion_vector.yml` | Diffusion denoiser | Generative modeling |
| `graph_node_classification.yml` | GCN | Graph learning |

[â†’ See all presets with full configurations](docs/manifests.md#presets)

---

## ğŸ–¥ï¸ Platform Support

### Apple Silicon (MPS)

caramba treats Apple Silicon as a first-class research target:

- âœ… **Works out of the box** â€” No special configuration needed
- âœ… **Unified memory** â€” Fit larger models than discrete GPU VRAM
- âœ… **Metal kernels** â€” Fused DBA decode for fp16 KV-caches
- âš ï¸ **Bandwidth limited** â€” Expect lower throughput than A100

### NVIDIA CUDA

For maximum throughput:

- âœ… **Triton kernels** â€” Fused attention decode with quantized caches
- âœ… **DDP/FSDP** â€” Multi-GPU training support
- âœ… **torch.compile** â€” Automatic graph optimization

### CPU

Fallback for development and testing:

- âœ… **Full functionality** â€” All features work
- âš ï¸ **Slow** â€” Not recommended for serious training

---

## ğŸ—ï¸ Architecture Overview

```text
caramba/
â”œâ”€â”€ config/          # Typed config models, presets, manifests
â”œâ”€â”€ compiler/        # Manifest â†’ executable plan
â”œâ”€â”€ topology/        # Graph nodes (stacked, residual, parallel, ...)
â”œâ”€â”€ layer/           # Thin PyTorch modules (attention, MoE, SSM, ...)
â”œâ”€â”€ model/           # Model building, embedders, trace utilities
â”œâ”€â”€ trainer/         # Training modes (standard, upcycle, orchestrated)
â”œâ”€â”€ infer/           # Generation loop with KV-cache management
â”œâ”€â”€ cache/           # KV-cache with quantization support
â”œâ”€â”€ benchmark/       # Perplexity, latency, memory measurement
â”œâ”€â”€ experiment/      # Unified pipeline orchestration
â”œâ”€â”€ orchestrator/    # Dynamic optimizer switching (SWATS, PIDAO, ...)
â”œâ”€â”€ optimizer/       # Triton (CUDA) + Metal (MPS) fused kernels
â”œâ”€â”€ agent/           # AI research automation (paper, review, loop)
â”œâ”€â”€ instrumentation/ # JSONL/HDF5/TensorBoard/W&B logging
â””â”€â”€ console/         # Rich-based logging and progress bars
```

---

## ğŸ§ª Testing

```bash
# Run all tests
python -m pytest -q

# Run with coverage
coverage run -m pytest && coverage report -m
```

---

## ğŸ“„ License

[MIT License](LICENSE.md)

---

<div align="center">

**[Getting Started](docs/getting-started.md)** Â· **[Manifests](docs/manifests.md)** Â· **[Layers](docs/layers.md)** Â· **[Training](docs/training.md)** Â· **[Inference](docs/inference.md)**

</div>
